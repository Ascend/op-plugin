all_version: [v1.11, v2.0, v2.1, v2.2, v2.3, v2.4, v2.5, v2.6, v2.7, v2.8]
official:
  - func: __ilshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version

  - func: __ilshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version

  - func: __ior__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: __ior__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: __irshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version

  - func: __irshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version

  - func: __lshift__.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version

  - func: __lshift__.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version

  - func: __rshift__.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version

  - func: __rshift__.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version

  - func: __xor__.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version

  - func: __xor__.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version

  - func: _adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: _adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: _adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnAdaptiveAvgPool2dBackward

  - func: _adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      out:
        size: adaptive_avg_pool3d_npu_output_size(self, output_size)
        dtype: self
      exec: aclnnAdaptiveAvgPool3d

  - func: _adaptive_avg_pool3d(Tensor self, int[3] output_size) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: adaptive_avg_pool3d_npu_output_size(self, output_size)
        dtype: self
      exec: aclnnAdaptiveAvgPool3d

  - func: _adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnAdaptiveAvgPool3dBackward

  - func: _add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnAddRelu

  - func: _add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnAddRelu

  - func: _add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
        exec: aclnnInplaceAddRelu

  - func: _aminmax(Tensor self) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out0:
        size: reduce_ops_npu_output_size(self, get_dimlist_for_tensor(self), false)
        dtype: self
      out1:
        size: reduce_ops_npu_output_size(self, get_dimlist_for_tensor(self), false)
        dtype: self
      exec: aclnnAminmaxAll

  - func: _aminmax.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out0:
        size: reduce_ops_npu_output_size(self, {dim}, keepdim)
        dtype: self
      out1:
        size: reduce_ops_npu_output_size(self, {dim}, keepdim)
        dtype: self
      exec: aclnnAminmaxDim

  - func: _amp_foreach_non_finite_check_and_unscale_(Tensor(a!)[] self, Tensor(b!) found_inf, Tensor inv_scale) -> ()
    acl_op: all_version
    op_api: all_version

  - func: _amp_update_scale_(Tensor(a!) self, Tensor(b!) growth_tracker, Tensor found_inf, float growth_factor, float backoff_factor, int growth_interval) -> Tensor(a!)
    op_api: all_version

  - func: _batch_norm_impl_index(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> (Tensor, Tensor, Tensor, Tensor, int)
    acl_op: all_version

  - func: _batch_norm_impl_index_backward(int impl_index, Tensor input, Tensor grad_output, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var_transform, bool train, float eps, bool[3] output_mask, Tensor reservedSpace) -> (Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: _cat(Tensor[] tensors, int dim=0) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: _cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: _cdist_backward(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist) -> Tensor
    acl_op: all_version

  - func: _cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor
    acl_op: all_version

  - func: _convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: _convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: _ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: _ctc_loss_backward(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: log_probs
        dtype: grad
      exec: aclnnCtcLossBackward

  - func: _cummax_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
    acl_op: all_version
    op_api: all_version

  - func: _cummin_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()
    acl_op: all_version
    op_api: all_version

  - func: _conv_depthwise2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, SymInt[2] padding, int[2] dilation) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: _conv_depthwise2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: _conv_depthwise2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, SymInt[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, v2.6]
    op_api: [v2.1, v2.6]
    use_const_ref_for_mutable_tensors: true

  - func: _conv_depthwise2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, SymInt[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.7, newest]
    op_api: [v2.7, newest]

  - func: _conv_depthwise2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    use_const_ref_for_mutable_tensors: true

  - func: _dim_arange(Tensor like, int dim) -> Tensor
    acl_op: all_version

  - func: _embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: _embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: _embedding_bag_per_sample_weights_backward(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode, int padding_idx=-1) -> Tensor
    acl_op: all_version

  - func: _fake_quantize_per_tensor_affine_cachemask_tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, Tensor fake_quant_enabled, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
    op_api: v2.1, v2.2

  - func: _fft_c2c(Tensor self, SymInt[] dim, int normalization, bool forward) -> Tensor
    op_api: [v2.1, newest]

  - func: _fft_c2c.out(Tensor self, SymInt[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)
    op_api: [v2.1, newest]

  - func: _fft_c2r(Tensor self, int[] dim, int normalization, SymInt last_dim_size) -> Tensor
    op_api: [v2.1, newest]

  - func: _fft_c2r.out(Tensor self, int[] dim, int normalization, SymInt last_dim_size, *, Tensor(a!) out) -> Tensor(a!)
    op_api: [v2.1, newest]

  - func: _fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor
    op_api: [v2.1, newest]

  - func: _fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)
    op_api: [v2.1, newest]

  - func: _foreach_norm.Scalar(Tensor[] tensors, Scalar ord=2) -> Tensor[]
    op_api: v1.11, v2.1, v2.2, v2.3

  - func: _foreach_norm.Scalar(Tensor[] tensors, Scalar ord=2, ScalarType? dtype=None) -> Tensor[]
    op_api: [v2.4, newest]

  - func: _foreach_abs_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_abs(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_add.List(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]
    op_api: all_version

  - func: _foreach_add.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
    op_api: all_version

  - func: _foreach_add.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
    op_api: all_version

  - func: _foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
    op_api: all_version

  - func: _foreach_add_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
    op_api: all_version

  - func: _foreach_add_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
    op_api: all_version

  - func: _foreach_addcdiv.Scalar(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
    op_api: all_version

  - func: _foreach_addcdiv.ScalarList(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
    op_api: all_version

  - func: _foreach_addcdiv.Tensor(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_addcdiv_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
    op_api: all_version

  - func: _foreach_addcdiv_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
    op_api: all_version

  - func: _foreach_addcdiv_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_addcmul.Scalar(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]
    op_api: all_version

  - func: _foreach_addcmul.ScalarList(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]
    op_api: all_version

  - func: _foreach_addcmul.Tensor(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()
    op_api: all_version

  - func: _foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()
    op_api: all_version

  - func: _foreach_addcmul_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_ceil(Tensor(a!)[] self) -> Tensor[]
    op_api: all_version

  - func: _foreach_ceil_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_cos(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_cos_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_copy_(Tensor(a!)[] self, Tensor[] src, bool non_blocking=False) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_sign(Tensor[] self) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_sign_(Tensor(a!)[] self) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_div.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
    op_api: all_version

  - func: _foreach_div.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
    op_api: all_version

  - func: _foreach_div.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
    op_api: all_version

  - func: _foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -> ()
    op_api: all_version

  - func: _foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
    op_api: all_version

  - func: _foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
    op_api: all_version

  - func: _foreach_exp(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_exp_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_floor(Tensor(a!)[] self) -> Tensor[]
    op_api: all_version

  - func: _foreach_floor_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_frac(Tensor(a!)[] self) -> Tensor[]
    op_api: all_version

  - func: _foreach_frac_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_maximum.List(Tensor[] self, Tensor[] other) -> Tensor[]
    op_api: all_version

  - func: _foreach_maximum.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_maximum.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_maximum_.List(Tensor(a!)[] self, Tensor[] other) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_maximum_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_maximum_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_minimum.List(Tensor[] self, Tensor[] other) -> Tensor[]
    op_api: all_version

  - func: _foreach_minimum.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_minimum.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_minimum_.List(Tensor(a!)[] self, Tensor[] other) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_minimum_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_minimum_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_mul.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]
    op_api: all_version

  - func: _foreach_mul.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
    op_api: all_version

  - func: _foreach_mul.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
    op_api: all_version

  - func: _foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -> ()
    op_api: all_version

  - func: _foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
    op_api: all_version

  - func: _foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
    op_api: all_version

  - func: _foreach_neg(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_neg_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_pow.List(Tensor[] self, Tensor[] exponent) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_pow.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_pow.ScalarList(Tensor[] self, Scalar[] exponent) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_pow_.List(Tensor(a!)[] self, Tensor[] exponent) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_pow_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_pow_.ScalarList(Tensor(a!)[] self, Scalar[] exponent) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_pow.ScalarAndTensor(Scalar self, Tensor[] exponent) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_reciprocal_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_reciprocal(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_round(Tensor(a!)[] self) -> Tensor[]
    op_api: all_version

  - func: _foreach_round_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_sigmoid(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_sigmoid_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_sqrt(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_sqrt_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_sub.List(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]
    op_api: all_version

  - func: _foreach_sub.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
    op_api: all_version

  - func: _foreach_sub.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]
    op_api: all_version

  - func: _foreach_sub_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()
    op_api: all_version

  - func: _foreach_sub_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
    op_api: all_version

  - func: _foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
    op_api: all_version

  - func: _foreach_trunc(Tensor(a!)[] self) -> Tensor[]
    op_api: all_version

  - func: _foreach_trunc_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_expm1_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_expm1(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_erf(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_erf_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_erfc(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_erfc_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_asin(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_asin_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_sin_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_sin(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_sinh_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_sinh(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_acos_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_acos(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_cosh_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_cosh(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_atan_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_atan(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_tan_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_tan(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_tanh(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_tanh_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_clamp_max.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_clamp_max_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_clamp_max.List(Tensor[] self, Tensor[] other) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_clamp_max_.List(Tensor(a!)[] self, Tensor[] other) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_clamp_max.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_clamp_max_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_clamp_min.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_clamp_min_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_clamp_min.List(Tensor[] self, Tensor[] other) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_clamp_min_.List(Tensor(a!)[] self, Tensor[] other) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_clamp_min.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_clamp_min_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_lerp_.Scalar(Tensor(a!)[] self, Tensor[] tensors1, Scalar weight) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_lerp.Scalar(Tensor[] self, Tensor[] tensors1, Scalar weight) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_lerp_.List(Tensor(a!)[] self, Tensor[] tensors1, Tensor[] weights) -> ()
    op_api: [v2.1, newest]

  - func: _foreach_lerp.List(Tensor[] self, Tensor[] tensors1, Tensor[] weights) -> Tensor[]
    op_api: [v2.1, newest]

  - func: _foreach_log1p_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_log1p(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_log_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_log(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_log10_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_log10(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_log2_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _foreach_log2(Tensor[] tensors) -> Tensor[]
    op_api: all_version

  - func: _foreach_zero_(Tensor(a!)[] self) -> ()
    op_api: all_version

  - func: _fused_adamw_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()
    op_api: all_version

  - func: _fused_moving_avg_obs_fq_helper(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> (Tensor output, Tensor mask)
    op_api: v2.1, v2.2

  - func: _index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      exec: aclnnInplaceIndexCopy

  - func: _index_put_impl_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: _linalg_svd(Tensor A, bool full_matrices=False, bool compute_uv=True) -> (Tensor U, Tensor S, Tensor Vh)
    acl_op: v1.11

  - func: _linalg_svd(Tensor A, bool full_matrices=False, bool compute_uv=True, *, str? driver=None) -> (Tensor U, Tensor S, Tensor Vh)
    acl_op: [v2.0, newest]

  - func: _linalg_svd.U(Tensor A, bool full_matrices=False, bool compute_uv=True, *, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)
    acl_op: v1.11

  - func: _linalg_svd.U(Tensor A, bool full_matrices=False, bool compute_uv=True, *, str? driver=None, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)
    acl_op: [v2.0, newest]

  - func: _log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'half_to_float ? at::kFloat : self.scalar_type()'
      exec: aclnnLogSoftmax, self, dim, out

  - func: _log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: all_version
    gen_opapi:
      exec: aclnnLogSoftmax, self, dim, out

  - func: _log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: grad_output
        dtype: grad_output
      exec: aclnnLogSoftmaxBackward, grad_output, output, dim, out

  - func: _log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: grad_output
        dtype: grad_output
      exec: aclnnLogSoftmaxBackward, grad_output, output, dim, out

  - func: _native_batch_norm_legit(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
    acl_op: [v2.0, newest]
    op_api: all_version

  - func: _nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, int[2] stride=1) -> Tensor
    acl_op: [v2.0, newest]

  - func: _nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, int[2] padding, int[2] stride=1) -> Tensor
    acl_op: v1.11

  - func: _pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)
    acl_op: all_version
    device_check: NoCheck

  - func: _pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)
    acl_op: all_version
    device_check: NoCheck

  - func: _pdist_forward(Tensor self, float p=2) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: _s_where(Tensor condition, Tensor self, Tensor other) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: 'broadcast_ops_npu_output_size(condition.sizes(), broadcast_ops_npu_output_size(self, other))'
        dtype: self
      exec: aclnnSWhere

  - func: _softmax(Tensor self, int dim, bool half_to_float) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'half_to_float ? at::ScalarType::Float : self.scalar_type()'
      exec: aclnnSoftmax, self, dim, out

  - func: _softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnSoftmax, self, dim, out

  - func: _softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: output
        dtype: output
      exec: aclnnSoftmaxBackward, grad_output, output, dim, grad_input

  - func: _softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: grad_output
        dtype: grad_output
      exec: aclnnSoftmaxBackward, grad_output, output, dim, grad_input

  - func: _symeig_helper(Tensor self, bool eigenvectors, bool upper) -> (Tensor, Tensor)
    acl_op: v1.11, v2.0

  - func: _prelu_kernel(Tensor self, Tensor weight) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnPrelu

  - func: _prelu_kernel_backward(Tensor grad_output, Tensor self, Tensor weight) -> (Tensor, Tensor)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: _slow_conv2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
    acl_op: all_version
    op_api: all_version

  - func: _slow_conv2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: _unique(Tensor self, bool sorted=True, bool return_inverse=False) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: _unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: _unsafe_index.Tensor(Tensor self, Tensor?[] indices) -> Tensor
    acl_op: v2.1

  - func: _weight_norm(Tensor v, Tensor g, int dim=0) -> Tensor
    acl_op: [v2.0, newest]

  - func: abs(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: abs.out

  - func: abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
        name: self
      exec: aclnnAbs, self, out

  - func: abs_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: abs.out

  - func: acos(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::kFloat : self.scalar_type()'
        name: self
      exec: aclnnAcos

  - func: acos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        name: self
      exec: aclnnAcos

  - func: acos_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceAcos

  - func: acosh(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::kFloat : self.scalar_type()'
      exec: aclnnAcosh

  - func: acosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: out
      exec: aclnnAcosh

  - func: acosh_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceAcosh

  - func: adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor
    acl_op: all_version

  - func: adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: adaptive_avg_pool2d(Tensor self, int[2] output_size) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: adaptive_avg_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      exec: aclnnAdaptiveAvgPool3d


  - func: adaptive_avg_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      exec: aclnnAdaptiveAvgPool3d

  - func: adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnAdaptiveAvgPool3dBackward

  - func: adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: adaptive_max_pool2d.out

  - func: adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: max_pool2d_out_size(self, output_size)
        dtype: self
      indices:
        size: max_pool2d_out_size(self, output_size)
        dtype: at::kLong
      exec: aclnnAdaptiveMaxPool2d

  - func: adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
    acl_op: all_version

  - func: adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version

  - func: adaptive_max_pool3d.out(Tensor self, int[3] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
    op_api: all_version

  - func: adaptive_max_pool3d(Tensor self, int[3] output_size) -> (Tensor, Tensor)
    op_api: all_version

  - func: adaptive_max_pool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: all_version

  - func: adaptive_max_pool3d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor
    op_api: all_version

  - func: add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
    sparse: [v2.1, newest]
    acl_op: all_version
    op_api: all_version

  - func: add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: addr_npu_output_size(self, vec1, vec2)
        dtype: at::native::result_type({self, vec1, vec2})
      exec: aclnnAddr

  - func: addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnAddr

  - func: addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceAddr

  - func: affine_grid_generator(Tensor theta, SymInt[] size, bool align_corners) -> Tensor
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: affine_grid_generator(Tensor theta, int[] size, bool align_corners) -> Tensor
    acl_op: v1.11, v2.0
    op_api: v1.11

  - func: affine_grid_generator_backward(Tensor grad, SymInt[] size, bool align_corners) -> Tensor
    acl_op: [v2.1, newest]

  - func: affine_grid_generator_backward(Tensor grad, int[] size, bool align_corners) -> Tensor
    acl_op: v1.11, v2.0

  - func: all(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: all.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: reduce_ops_npu_output_size(self, dim, keepdim)
        dtype: self
      exec: aclnnAmax

  - func: amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: reduce_ops_npu_output_size(self, dim, keepdim)
      exec: aclnnAmax

  - func: amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: reduce_ops_npu_output_size(self, dim, keepdim)
        dtype: self
      exec: aclnnAmin

  - func: amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: reduce_ops_npu_output_size(self, dim, keepdim)
      exec: aclnnAmin

  - func: aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -> (Tensor min, Tensor max)
    op_api: all_version

  - func: aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -> (Tensor(a!) min, Tensor(b!) max)
    acl_op: all_version
    op_api: all_version

  - func: angle(Tensor self) -> Tensor
    op_api: all_version
    gen_opapi:
      structured_inherit: angle.out

  - func: angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: angle_out_dtype(self)
      exec: aclnnAngleV2

  - func: any(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: argsort.stable(Tensor self, bool stable=False, int dim=-1, bool descending=False) -> Tensor
    op_api: [v2.0, newest]

  - func: asin(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::kFloat : self.scalar_type()'
        name: self
      exec: aclnnAsin

  - func: asin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        name: self
      exec: aclnnAsin

  - func: asin_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceAsin

  - func: asinh(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::kFloat : self.scalar_type()'
      exec: aclnnAsinh

  - func: asinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnAsinh

  - func: asinh_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceAsinh

  - func: atan(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::kFloat : self.scalar_type()'
        name: self
      exec: aclnnAtan

  - func: atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        name: self
      exec: aclnnAtan

  - func: atan2(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: atan2_(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: atan_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceAtan

  - func: atanh(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::kFloat : self.scalar_type()'
      exec: aclnnAtanh

  - func: atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnAtanh

  - func: atanh_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceAtanh

  - func: avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: avg_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> Tensor
    acl_op: all_version

  - func: batch_norm_backward_elemt(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor mean_dy, Tensor mean_dy_xmu, Tensor count) -> Tensor
    acl_op: v2.0

  - func: batch_norm_backward_elemt(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor sum_dy, Tensor sum_dy_xmu, Tensor count) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: input
        dtype: input
      exec: aclnnBatchNormElemtBackward

  - func: batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: batch_norm_elemt(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: batch_norm_elemt.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out0:
        size: '{input.size(1)}'
        dtype: 'input.scalar_type() == mean.scalar_type() && input.scalar_type() == at::kHalf ? at::kHalf : at::kFloat'
      out1:
        size: '{input.size(1)}'
        dtype: 'input.scalar_type() == mean.scalar_type() && input.scalar_type() == at::kHalf ? at::kHalf : at::kFloat'
      exec: aclnnBatchNormGatherStatsWithCounts

  - func: batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out0:
        size: '{input.size(1)}'
        dtype: at::kFloat
      out1:
        size: '{input.size(1)}'
        dtype: at::kFloat
      exec: aclnnBatchNormStats

  - func: bernoulli(Tensor self, *, Generator? generator=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: bernoulli.p(Tensor self, float p, *, Generator? generator=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: 'reduction == at::Reduction::None ? self.sizes() : at::ArrayRef<int64_t>()'
        dtype: self
      exec: aclnnBinaryCrossEntropy

  - func: binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnBinaryCrossEntropy

  - func: binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnBinaryCrossEntropyBackward

  - func: binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnBinaryCrossEntropyBackward

  - func: binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: '(reduction == at::Reduction::None) ? input_same_output_size(target) : at::ArrayRef<int64_t>()'
        dtype: target
      exec: aclnnBinaryCrossEntropyWithLogits

  - func: binary_cross_entropy_with_logits_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
    acl_op: v1.11

  - func: bincount(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: bitwise_not(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: bitwise_not.out

  - func: bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
        name: self
      exec: aclnnBitwiseNot

  - func: bitwise_not_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: bitwise_not.out

  - func: bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: bmm(Tensor self, Tensor mat2) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
    op_api: all_version

  - func: bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor
    op_api: all_version

  - func: bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)
    op_api: all_version

  - func: cat(Tensor[] tensors, int dim=0) -> Tensor
    structured_delegate: cat.out
    acl_op: all_version
    op_api: all_version

  - func: cat.names(Tensor[] tensors, Dimname dim) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
    structured: true
    acl_op: all_version
    op_api: all_version

  - func: cdist(Tensor x1, Tensor x2, float p=2, int? compute_mode=None) -> Tensor
    acl_op: all_version

  - func: ceil(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: ceil.out

  - func: ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnCeil

  - func: ceil_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceCeil

  - func: _coalesce(Tensor self) -> Tensor
    sparse: [v2.1, newest]

  - func: celu(Tensor self, Scalar alpha=1.0) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnCelu

  - func: celu_(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceCelu

  - func: channel_shuffle(Tensor self, int groups) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnChannelShuffle

  - func: clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: clamp.out

  - func: clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: clamp_npu_output_size(self, min, max)
        dtype: self
      exec: aclnnClampTensor

  - func: clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: clamp_npu_output_size(self, min, max)
      exec: aclnnClampTensor

  - func: clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnClamp

  - func: clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: clamp.out

  - func: clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: clamp.Tensor_out

  - func: clamp_max(Tensor self, Scalar max) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: clamp_max.out

  - func: clamp_max.Tensor(Tensor self, Tensor max) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: op_infer::broadcast_ops_npu_output_size(self, max)
        dtype: self
      exec: aclnnClampMaxTensor

  - func: clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: op_infer::broadcast_ops_npu_output_size(self, max)
      exec: aclnnClampMaxTensor

  - func: clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnClampMax

  - func: clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceClampMax

  - func: clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceClampMaxTensor

  - func: clamp_min(Tensor self, Scalar min) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: clamp_min.out

  - func: clamp_min.Tensor(Tensor self, Tensor min) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: clamp_min.Tensor_out

  - func: clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnClampMinTensor

  - func: clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnClampMin

  - func: clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: clamp_min.out

  - func: clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceClampMinTensor

  - func: complex(Tensor real, Tensor imag) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: constant_pad_nd(Tensor self, SymInt[] pad, Scalar value=0) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.0, newest]

  - func: constant_pad_nd(Tensor self, int[] pad, Scalar value=0) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: '{self.size(0) + 2 * pad - weight.size(0) + 1, self.size(1), weight.size(2)}'
        dtype: self
      new_params:
        cube_math_type: 'npu_preparation::get_cube_math_type(at_npu::native::env::IsAllowConvHF32())'
      exec: aclnnConvTbc, self, weight, bias, pad, out, cube_math_type

  - func: conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor
    acl_op: [v2.1, newest]

  - func: conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor
    acl_op: v1.11, v2.0

  - func: conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int groups=1, int[3] dilation=1) -> Tensor
    acl_op: [v2.1, newest]

  - func: conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, int[3] padding=0, int[3] output_padding=0, int groups=1, int[3] dilation=1) -> Tensor
    acl_op: v1.11, v2.0

  - func: convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: convolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: convolution_backward(Tensor grad_output, Tensor input, Tensor weight, int[]? bias_sizes, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    acl_op: v1.11
    op_api: v1.11

  - func: convolution_backward_overrideable(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
    acl_op: all_version
    op_api: all_version

  - func: convolution_overrideable(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: cos(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      resutl:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::kFloat : self.scalar_type()'
      exec: aclnnCos

  - func: cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnCos

  - func: cos_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceCos

  - func: cosh(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::kFloat : self.scalar_type()'
      exec: aclnnCosh

  - func: cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnCosh

  - func: cosh_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceCosh

  - func: count_nonzero(Tensor self, int? dim=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: count_nonzero.dim_IntList(Tensor self, int[] dim) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.0, newest]
    gen_opapi:
      out:
        size: im2col_backward_npu_output_size(self, output_size, kernel_size)
        dtype: self
      exec: aclnnIm2colBackward

  - func: col2im(Tensor self, int[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: im2col_backward_npu_output_size(self, output_size, kernel_size)
        dtype: self
      exec: aclnnIm2colBackward

  - func: col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.0, newest]
    gen_opapi:
      out:
        size: im2col_backward_npu_output_size(self, output_size, kernel_size)
      exec: aclnnIm2colBackward

  - func: col2im.out(Tensor self, int[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: im2col_backward_npu_output_size(self, output_size, kernel_size)
      exec: aclnnIm2colBackward

  - func: col2im_backward(Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
    acl_op: v1.11

  - func: col2im_backward.grad_input(Tensor grad_output, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11

  - func: ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor
    acl_op: all_version
    op_api: all_version
    device_check: NoCheck

  - func: cumprod(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
    acl_op: v2.0

  - func: cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version

  - func: cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'dtype.has_value() ? dtype.value() : out.scalar_type()'
      new_params:
        dim_scalar: 'at::Scalar(dim)'
        dst_type: 'dtype.has_value() ? dtype.value() : out.scalar_type()'
      exec: aclnnCumprod, self, dim_scalar, dst_type, out

  - func: cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)
    acl_op: all_version

  - func: cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: cumsum.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: cumsum_(Tensor(a!) self, int dim, *, ScalarType? dtype=None) -> Tensor(a!)
    acl_op: v2.0

  - func: dequantize.self(Tensor self) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: diag(Tensor self, int diagonal=0) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      structured_inherit: diag.out

  - func: diag.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: diag_output_size(self, diagonal)
        dtype: self
      exec: aclnnDiag

  - func: div.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: div.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: dot(Tensor self, Tensor tensor) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: 'c10::SmallVector<int64_t, op_infer::SIZE>{}'
        dtype: self
      exec: aclnnDot

  - func: dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: 'c10::SmallVector<int64_t, op_infer::SIZE>{}'
      exec: aclnnDot

  - func: dropout(Tensor input, float p, bool train) -> Tensor
    acl_op: all_version
    op_api: all_version
    internal_format_opapi: all_version

  - func: dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)
    op_api: all_version
    internal_format_opapi: all_version

  - func: elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnElu

  - func: elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnElu

  - func: elu_(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceElu

  - func: elu_backward(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: grad_output
        dtype: grad_output
      exec: aclnnEluBackward

  - func: elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: grad_output
      exec: aclnnEluBackward

  - func: embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: embedding(Tensor weight, Tensor indices, int padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: embedding_dense_backward(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      out:
        size: '{num_weights, grad_output.size(-1)}'
        dtype: grad_output
      exec: aclnnEmbeddingDenseBackward

  - func: embedding_dense_backward(Tensor grad_output, Tensor indices, int num_weights, int padding_idx, bool scale_grad_by_freq) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: '{num_weights, grad_output.size(-1)}'
        dtype: grad_output
      exec: aclnnEmbeddingDenseBackward

  - func: embedding_renorm_(Tensor(a!) self, Tensor indices, float max_norm, float norm_type) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: eq.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: eq.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: eq_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: eq_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: equal(Tensor self, Tensor other) -> bool
    acl_op: all_version
    op_api: all_version

  - func: erf(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: '(self.scalar_type() == at::ScalarType::Bool || self.scalar_type() == at::ScalarType::Long || self.scalar_type() == at::ScalarType::Int) ? at::kFloat : self.scalar_type()'
      exec: aclnnErf

  - func: erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnErf

  - func: erf_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceErf

  - func: erfc(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: '(self.scalar_type() == at::ScalarType::Bool || self.scalar_type() == at::ScalarType::Long || self.scalar_type() == at::ScalarType::Int) ? at::kFloat : self.scalar_type()'
      exec: aclnnErfc

  - func: erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnErfc

  - func: erfc_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceErfc

  - func: erfinv(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::kFloat : self.scalar_type()'
      exec: aclnnErfinv

  - func: erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnErfinv

  - func: erfinv_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceErfinv

  - func: exp(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: '(self.scalar_type() == at::ScalarType::Bool || self.scalar_type() == at::ScalarType::Long) ? at::kFloat : self.scalar_type()'
      exec: aclnnExp

  - func: exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnExp

  - func: exp2(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::ScalarType::Float : self.scalar_type()'
      exec: aclnnExp2

  - func: exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnExp2

  - func: exp2_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceExp2

  - func: exp_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceExp

  - func: expm1(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isFloatingType(self.scalar_type()) ? self.scalar_type() : at::kFloat'
      exec: aclnnExpm1

  - func: expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnExpm1

  - func: expm1_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceExpm1

  - func: exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -> Tensor(a!)
    op_api: all_version

  - func: eye(SymInt n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: eye(int n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: v1.11, v2.0
    op_api: v1.11

  - func: eye.m(SymInt n, SymInt m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: eye.m(int n, int m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: v1.11, v2.0
    op_api: v1.11

  - func: eye.m_out(SymInt n, SymInt m, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: eye.m_out(int n, int m, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11, v2.0
    op_api: v1.11

  - func: eye.out(SymInt n, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: eye.out(int n, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11, v2.0
    op_api: v1.11

  - func: fake_quantize_per_channel_affine_cachemask(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> (Tensor output, Tensor mask)
    op_api: v2.1, v2.2

  - func: fft_fftshift(Tensor self, int[1]? dim=None) -> Tensor
    op_api: [v2.1, newest]

  - func: fft_ifftshift(Tensor self, int[1]? dim=None) -> Tensor
    op_api: [v2.1, newest]

  - func: fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: fill_.Tensor(Tensor(a!) self, Tensor value) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: fill_diagonal_(Tensor(a!) self, Scalar fill_value, bool wrap=False) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceFillDiagonal

  - func: flip(Tensor self, int[] dims) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnFlip

  - func: floor(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnFloor

  - func: floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnFloor

  - func: floor_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceFloor

  - func: floor_divide(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: floor_divide.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: floor_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: floor_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: fmod.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnFmodScalar

  - func: fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnFmodScalar

  - func: fmod.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, other)
        dtype: at::native::result_type(self, other)
      exec: aclnnFmodTensor

  - func: fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, other)
      exec: aclnnFmodTensor

  - func: fmod_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceFmodScalar

  - func: fmod_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceFmodTensor

  - func: frac(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: frac.out

  - func: frac.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnFrac

  - func: frac_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceFrac

  - func: gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, other)
      exec: aclnnGcd

  - func: ge.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: ge.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: gelu(Tensor self) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      structured_inherit: gelu.out

  - func: gelu(Tensor self, *, str approximate='none') -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnGelu, self, out

  - func: gelu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnGelu

  - func: gelu_backward(Tensor grad_output, Tensor self) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: 'broadcast_ops_npu_output_size(grad_output, self)'
        dtype: 'at::native::result_type(grad_output, self)'
      exec: aclnnGeluBackward

  - func: gelu_backward(Tensor grad_output, Tensor self, *, str approximate='none') -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      grad_input:
        size: 'broadcast_ops_npu_output_size(grad_output, self)'
        dtype: 'at::native::result_type(grad_output, self)'
      exec: aclnnGeluBackward, grad_output, self, grad_input

  - func: glu(Tensor self, int dim=-1) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: glu_npu_output_size(self, dim)
        dtype: self
      exec: aclnnGlu

  - func: glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: glu_npu_output_size(self, dim)
      exec: aclnnGlu

  - func: glu_backward(Tensor grad_output, Tensor self, int dim) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnGluBackward

  - func: glu_backward.grad_input(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: self
        dtype: grad_output
      exec: aclnnGluBackward

  - func: grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out0:
        size: '{input.size(0), input.size(1), grid.size(1), grid.size(2)}'
        dtype: input
      exec: aclnnGridSampler2D

  - func: grid_sampler_2d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out0:
        size: input
        dtype: input
      out1:
        size: grid
        dtype: grid
      exec: aclnnGridSampler2DBackward

  - func: grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out0:
        size: '{input.size(0), input.size(1), grid.size(1), grid.size(2), grid.size(3)}'
        dtype: input
      exec: aclnnGridSampler3D

  - func: grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor, Tensor)
    acl_op: v1.11
    op_api: v1.11

  - func: grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -> (Tensor, Tensor)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      dinput:
        size: input
        dtype: input
      dgrid:
        size: grid
        dtype: grid
      exec: aclnnGridSampler3DBackward

  - func: gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)
    acl_op: all_version

  - func: gt.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: gt.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: gt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: gt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnHardshrink

  - func: hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnHardshrink

  - func: hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: broadcast_ops_npu_output_size(grad_out, self)
        dtype: at::native::result_type(grad_out, self)
      exec: aclnnHardshrinkBackward

  - func: hardshrink_backward.grad_input(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: broadcast_ops_npu_output_size(grad_out, self)
      exec: aclnnHardshrinkBackward

  - func: hardsigmoid(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: hardsigmoid.out

  - func: hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnHardsigmoid

  - func: hardsigmoid_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceHardsigmoid

  - func: hardsigmoid_backward(Tensor grad_output, Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: at::native::result_type(grad_output, self)
      exec: aclnnHardsigmoidBackward

  - func: hardswish(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: hardswish.out

  - func: hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnHardswish

  - func: hardswish_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceHardswish

  - func: hardswish_backward(Tensor grad_output, Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnHardswishBackward

  - func: hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: hardtanh.out

  - func: hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnHardtanh

  - func: hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceHardtanh

  - func: hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: hardtanh_backward.grad_input

  - func: hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnHardtanhBackward

  - func: histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: histc.out

  - func: histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: '{bins}'
        dtype: self
      exec: aclnnHistc

  - func: index.Tensor(Tensor self, Tensor?[] indices) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: index_add(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor
    acl_op: all_version

  - func: index_add.out(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: index_add_(Tensor(a!) self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor(a!)
    acl_op: v2.0

  - func: index_copy(Tensor self, int dim, Tensor index, Tensor source) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnIndexCopy

  - func: index_copy.out(Tensor self, int dim, Tensor index, Tensor source, *, Tensor(a!) out) -> Tensor(a!)
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnIndexCopy

  - func: index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.0, newest]
    gen_opapi:
      exec: aclnnInplaceIndexCopy

  - func: index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: index_select(Tensor self, int dim, Tensor index) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: index_select.dimname(Tensor self, Dimname dim, Tensor index) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: index_select.dimname_out(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: inverse(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnInverse

  - func: inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnInverse

  - func: isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: 'at::isFloatingType(self.scalar_type()) && equal_nan ? self.sizes() : broadcast_ops_npu_output_size(self, other)'
        dtype: at::kBool
      exec: aclnnIsClose

  - func: isfinite(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: at::kBool
      exec: aclnnIsFinite

  - func: isin.Scalar_Tensor_out(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: 'c10::SmallVector<int64_t, op_infer::SIZE>{}'
        dtype: at::kBool
      exec: aclnnIsInScalarTensor

  - func: isin.Tensor_Scalar(Tensor element, Scalar test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: isin.Tensor_Scalar_out

  - func: isin.Tensor_Scalar_out(Tensor element, Scalar test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: element
        dtype: at::kBool
      exec: aclnnIsInTensorScalar

  - func: isneginf(Tensor self) -> Tensor
    acl_op: v2.0

  - func: isneginf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnIsNegInf

  - func: isposinf(Tensor self) -> Tensor
    acl_op: v2.0

  - func: isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnIsPosInf

  - func: im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: im2col_backward(Tensor grad_output, int[2] input_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: 'im2col_backward_npu_output_size(grad_output, input_size, kernel_size)'
        dtype: grad_output
      exec: aclnnIm2colBackward

  - func: im2col_backward.grad_input(Tensor grad_output, int[2] input_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: 'im2col_backward_npu_output_size(grad_output, input_size, kernel_size)'
        dtype: grad_input
      exec: aclnnIm2colBackward

  - func: kl_div(Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: 'reduction == at::Reduction::None
              ? broadcast_ops_npu_output_size(self.sizes(), target.sizes())
              : at::ArrayRef<int64_t>()'
        dtype: at::native::result_type(self, target)
      exec: aclnnKlDiv

  - func: kl_div_backward(Tensor grad_output, Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnKlDivBackward

  - func: kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)
    acl_op: all_version
    op_api: all_version

  - func: kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    acl_op: all_version
    op_api: all_version

  - func: kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
    acl_op: all_version
    op_api: all_version

  - func: kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
    acl_op: all_version
    op_api: all_version

  - func: l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: '(reduction == at::Reduction::None) ? broadcast_ops_npu_output_size(self, target) : c10::SmallVector<int64_t, op_infer::SIZE>{}'
        dtype: 'at::native::result_type(target, self)'
      exec: aclnnL1Loss

  - func: l1_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: 'broadcast_ops_npu_output_size(broadcast_ops_npu_output_size(self, target), grad_output.sizes())'
        dtype: 'promoteTypes(grad_output.scalar_type(), at::native::result_type(target, self))'
      exec: aclnnL1LossBackward

  - func: l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: 'broadcast_ops_npu_output_size(broadcast_ops_npu_output_size(self, target), grad_output.sizes())'
        dtype: grad_input
      exec: aclnnL1LossBackward

  - func: le.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: le.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: le_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: le_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: leaky_relu.out

  - func: leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnLeakyRelu

  - func: leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceLeakyRelu

  - func: leaky_relu_backward(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnLeakyReluBackward

  - func: leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: all_version
    gen_opapi:
      grad_input:
        size: self
      exec: aclnnLeakyReluBackward

  - func: lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, end)
        dtype: self
      exec: aclnnLerps

  - func: lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, end)
      exec: aclnnLerps

  - func: lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(broadcast_ops_npu_output_size(self, end), weight.sizes())
        dtype: self
      exec: aclnnLerp

  - func: lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(broadcast_ops_npu_output_size(self, end), weight.sizes())
      exec: aclnnLerp

  - func: lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceLerps

  - func: lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceLerp

  - func: linalg_cross(Tensor self, Tensor other, *, int dim=-1) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: linalg_cross.out

  - func: linalg_cross.out(Tensor self, Tensor other, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, other)
        dtype: self
      exec: aclnnLinalgCross

  - func: linalg_qr(Tensor self, str mode='reduced') -> (Tensor Q, Tensor R)
    acl_op: all_version
    op_api: all_version

  - func: linalg_qr.out(Tensor self, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)
    acl_op: all_version
    op_api: all_version

  - func: linalg_slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)
    acl_op: v1.11
    op_api: v1.11

  - func: linalg_slogdet.out(Tensor self, *, Tensor(a!) sign, Tensor(b!) logabsdet) -> (Tensor(a!) sign, Tensor(b!) logabsdet)
    acl_op: v1.11
    op_api: v1.11

  - func: linalg_solve_triangular(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False) -> Tensor
    op_api: all_version

  - func: linalg_solve_triangular.out(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False, Tensor(a!) out) -> Tensor(a!)
    op_api: all_version

  - func: linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: linspace(Scalar start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: linspace.out(Scalar start, Scalar end, int steps, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: log(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: log10(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::kFloat : self.scalar_type()'
      exec: aclnnLog10

  - func: log10.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnLog10

  - func: log10_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceLog10

  - func: log1p(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::kFloat : self.scalar_type()'
      exec: aclnnLog1p

  - func: log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnLog1p

  - func: log1p_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceLog1p

  - func: log2(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::kFloat : self.scalar_type()'
      exec: aclnnLog2

  - func: log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnLog2

  - func: log2_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceLog2

  - func: log_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: log_sigmoid(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: log_sigmoid_backward.grad_input

  - func: log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: grad_output
        dtype: grad_output
      exec: aclnnLogSigmoidBackward

  - func: log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)
    acl_op: all_version
    op_api: all_version

  - func: log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))
    acl_op: all_version
    op_api: all_version

  - func: log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
    acl_op: all_version

  - func: log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
    acl_op: all_version

  - func: logaddexp(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, other)
        dtype: at::native::result_type(self, other)
      exec: aclnnLogAddExp

  - func: logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, other)
      exec: aclnnLogAddExp

  - func: logaddexp2(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, other)
        dtype: at::native::result_type(self, other)
      exec: aclnnLogAddExp2

  - func: logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, other)
      exec: aclnnLogAddExp2

  - func: logdet(Tensor self) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: logical_and(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, other)
        dtype: at::kBool
      exec: aclnnLogicalAnd

  - func: logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, other)
      exec: aclnnLogicalAnd

  - func: logical_and_(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceLogicalAnd

  - func: logical_not(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: at::kBool
        name: self
      exec: aclnnLogicalNot

  - func: logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        name: self
      exec: aclnnLogicalNot

  - func: logical_not_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceLogicalNot

  - func: logical_or(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: logical_or_(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: logical_xor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, other)
        dtype: at::kBool
      exec: aclnnLogicalXor

  - func: logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, other)
      exec: aclnnLogicalXor

  - func: logspace(Scalar start, Scalar end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: all_version

  - func: logspace.out(Scalar start, Scalar end, int steps, float base=10.0, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version

  - func: logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: logsumexp.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)
    acl_op: all_version
    device_check: NoCheck

  - func: lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor)
    acl_op: all_version

  - func: lt.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: lt.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: lt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: lt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: masked_select(Tensor self, Tensor mask) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: _masked_softmax(Tensor self, Tensor mask, int? dim=None, int? mask_type=None) -> Tensor
    op_api: all_version

  - func: matmul(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: [v2.1, newest]

  - func: matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: [v2.1, newest]

  - func: matmul_backward(Tensor grad, Tensor self, Tensor other, bool[2] mask) -> (Tensor, Tensor)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: max(Tensor self) -> Tensor
    sparse: [v2.1, newest]
    acl_op: all_version
    op_api: all_version

  - func: max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    acl_op: all_version
    op_api: all_version

  - func: max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
    acl_op: all_version
    op_api: all_version

  - func: max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    acl_op: all_version
    op_api: all_version

  - func: max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)
    acl_op: all_version
    op_api: all_version

  - func: max.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    sparse: [v2.1, newest]
    acl_op: all_version
    op_api: all_version

  - func: max_unpool2d(Tensor self, Tensor indices, SymInt[2] output_size) -> Tensor
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      structured_inherit: max_unpool2d.out

  - func: max_unpool2d(Tensor self, Tensor indices, int[2] output_size) -> Tensor
    acl_op: v1.11, v2.0
    op_api: v1.11
    gen_opapi:
      structured_inherit: max_unpool2d.out

  - func: max_unpool2d.out(Tensor self, Tensor indices, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      out:
        size: max_pool2d_out_size(self, output_size)
        dtype: self
      exec: aclnnMaxUnpool2d

  - func: max_unpool2d.out(Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11, v2.0
    op_api: v1.11
    gen_opapi:
      out:
        size: max_pool2d_out_size(self, output_size)
        dtype: self
      exec: aclnnMaxUnpool2d

  - func: max_unpool2d_backward(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size) -> Tensor
    acl_op: v1.11, v2.0
    op_api: v1.11
    gen_opapi:
      structured_inherit: max_unpool2d_backward.grad_input

  - func: max_unpool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, int[2] output_size, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11, v2.0
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnMaxUnpool2dBackward

  - func: max_unpool3d(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding) -> Tensor
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      structured_inherit: max_unpool3d.out

  - func: max_unpool3d(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor
    acl_op: v1.11, v2.0
    op_api: v1.11
    gen_opapi:
      structured_inherit: max_unpool3d.out

  - func: max_unpool3d.out(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      out:
        size: max_pool3d_output_size(self, output_size)
        dtype: self
      exec: aclnnMaxUnpool3d

  - func: max_unpool3d.out(Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11, v2.0
    op_api: v1.11
    gen_opapi:
      out:
        size: max_pool3d_output_size(self, output_size)
        dtype: self
      exec: aclnnMaxUnpool3d

  - func: max_unpool3d_backward(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding) -> Tensor
    acl_op: v1.11, v2.0
    op_api: v1.11
    gen_opapi:
      structured_inherit: max_unpool3d_backward.grad_input

  - func: max_unpool3d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, int[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11, v2.0
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnMaxUnpool3dBackward

  - func: max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
    acl_op: all_version
    op_api: all_version

  - func: max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnMaxPool2dWithMaskBackward, grad_output, self, indices, kernel_size, stride, padding, dilation, ceil_mode, out

  - func: max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnMaxPool2dWithMaskBackward, grad_output, self, indices, kernel_size, stride, padding, dilation, ceil_mode, grad_input

  - func: max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))
    acl_op: all_version
    op_api: all_version

  - func: max_pool3d_with_indices_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: maximum(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: mean(Tensor self, *, ScalarType? dtype=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: mean.dim(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: mean.out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: median(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: reduce_ops_npu_output_size(self, get_dimlist_for_tensor(self), false)
        dtype: self
      exec: aclnnMedian

  - func: median.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      values:
        size: reduce_ops_npu_output_size(self, {dim}, keepdim)
        dtype: self
      indices:
        size: reduce_ops_npu_output_size(self, {dim}, keepdim)
        dtype: at::kLong
      exec: aclnnMedianDim

  - func: median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      values:
        size: reduce_ops_npu_output_size(self, {dim}, keepdim)
      indices:
        size: reduce_ops_npu_output_size(self, {dim}, keepdim)
      exec: aclnnMedianDim

  - func: min(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    acl_op: all_version
    op_api: all_version

  - func: min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
    acl_op: all_version
    op_api: all_version

  - func: min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    acl_op: all_version
    op_api: all_version

  - func: min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)
    acl_op: all_version
    op_api: all_version

  - func: min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: minimum(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: mish(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: mish_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: mish_backward(Tensor grad_output, Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(grad_output, self)
        dtype: at::native::result_type(grad_output, self)
      exec: aclnnMishBackward

  - func: mm(Tensor self, Tensor mat2) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(broadcast_ops_npu_output_size(grad_output, self), target.sizes())
        dtype: self
      exec: aclnnMseLossBackward

  - func: mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: broadcast_ops_npu_output_size(broadcast_ops_npu_output_size(grad_output, self), target.sizes())
      exec: aclnnMseLossBackward

  - func: mul.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: mul.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: multilabel_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: multilabel_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: multilabel_margin_loss_forward(Tensor self, Tensor target, int reduction) -> (Tensor output, Tensor is_target)
    acl_op: all_version
    op_api: all_version

  - func: multilabel_margin_loss_forward.output(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -> (Tensor(a!), Tensor(b!))
    acl_op: all_version
    op_api: all_version

  - func: multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: mv(Tensor self, Tensor vec) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: nanmedian(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: reduce_ops_npu_output_size(self, get_dimlist_for_tensor(self), false)
        dtype: self
      exec: aclnnNanMedian

  - func: nanmedian.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      output:
        size: reduce_ops_npu_output_size(self, dim, keepdim)
        dtype: self
      indices:
        size: reduce_ops_npu_output_size(self, dim, keepdim)
        dtype: at::kLong
      exec: aclnnNanMedianDim

  - func: nansum(Tensor self, *, ScalarType? dtype=None) -> Tensor
    op_api: v1.11

  - func: nansum(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
    op_api: [v2.0, newest]

  - func: nansum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
    op_api: v1.11

  - func: nansum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
    op_api: v1.11

  - func: nansum.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
    op_api: [v2.0, newest]

  - func: native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out0:
        size: input
        dtype: input
      out1:
        size: 'training ? c10::SmallVector<int64_t, op_infer::SIZE>{input.size(1)} : c10::SmallVector<int64_t, op_infer::SIZE>{0}'
        dtype: 'training ? at::kFloat : input.scalar_type()'
      out2:
        size: 'training ? c10::SmallVector<int64_t, op_infer::SIZE>{input.size(1)} : c10::SmallVector<int64_t, op_infer::SIZE>{0}'
        dtype: 'training ? at::kFloat : input.scalar_type()'
      exec: aclnnBatchNorm

  - func: native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnBatchNorm

  - func: native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: native_dropout(Tensor input, float p, bool? train) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: native_dropout_backward(Tensor grad_output, Tensor mask, float scale) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: native_group_norm(Tensor input, Tensor? weight, Tensor? bias, int N, int C, int HxW, int group, float eps) -> (Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out0:
        size: input
        dtype: input
      out1:
        size: '{N, group}'
        dtype: input
      out2:
        size: '{N, group}'
        dtype: input
      exec: aclnnGroupNorm

  - func: native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, int N, int C, int HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: native_layer_norm(Tensor input, int[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)
    acl_op: v1.11
    op_api: v1.11

  - func: native_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: native_layer_norm_backward(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    acl_op: v1.11
    op_api: v1.11

  - func: _native_multi_head_attention(Tensor query, Tensor key, Tensor value, int embed_dim, int num_head, Tensor qkv_weight, Tensor qkv_bias, Tensor proj_weight, Tensor proj_bias, Tensor? mask=None, bool need_weights=True, bool average_attn_weights=True, int? mask_type=None) -> (Tensor, Tensor)
    op_api: all_version

  - func: ne.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: ne.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: neg(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: neg.out

  - func: neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnNeg

  - func: neg_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: neg.out

  - func: nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
    acl_op: [v2.0, newest]

  - func: nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor
    acl_op: v1.11

  - func: nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]

  - func: nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11

  - func: nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor
    acl_op: [v2.0, newest]

  - func: nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100) -> Tensor
    acl_op: v1.11

  - func: nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]

  - func: nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, int ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11

  - func: nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor output, Tensor total_weight)
    acl_op: v1.11
    op_api: v1.11

  - func: nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
    acl_op: v1.11
    op_api: v1.11

  - func: nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index) -> (Tensor output, Tensor total_weight)
    acl_op: v1.11
    op_api: v1.11

  - func: nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, int ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))
    acl_op: v1.11
    op_api: v1.11

  - func: nonzero(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: norm.Scalar(Tensor self, Scalar p=2) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: normal.float_float(float mean, float std, int[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: normal.float_float_out(float mean, float std, int[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: one_hot(Tensor self, int num_classes=-1) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: ones(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: ones.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: pdist(Tensor self, float p=2) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: pow.Scalar(Scalar self, Tensor exponent) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: exponent
        dtype: at::result_type(self, exponent)
      exec: aclnnPowScalarTensor

  - func: pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: exponent
      exec: aclnnPowScalarTensor

  - func: pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: pow.Tensor_Scalar_out

  - func: pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: at::result_type(self, exponent)
      exec: aclnnPowTensorScalar

  - func: pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, exponent)
        dtype: at::result_type(self, exponent)
      exec: aclnnPowTensorTensor

  - func: pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, exponent)
      exec: aclnnPowTensorTensor

  - func: pow_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplacePowTensorScalar

  - func: pow_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplacePowTensorTensor

  - func: polar(Tensor abs, Tensor angle) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: polar.out


  - func: polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: op_infer::broadcast_ops_npu_output_size(abs, angle)
        dtype: polar_out_dtype(abs, angle)
      exec: aclnnPolar

  - func: prelu(Tensor self, Tensor weight) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnPrelu

  - func: prelu_backward(Tensor grad_output, Tensor self, Tensor weight) -> (Tensor, Tensor)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: grad_output
        dtype: grad_output
      grad_weight:
        size: 'prelu_backward_npu_grad_weight_output_size(weight)'
        dtype: weight
      exec: aclnnPreluBackward

  - func: prod(Tensor self, *, ScalarType? dtype=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor
    acl_op: all_version

  - func: quantize_per_tensor(Tensor self, float scale, int zero_point, ScalarType dtype) -> Tensor
    acl_op: all_version

  - func: random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: randperm(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: randperm(int n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: v1.11, v2.0
    op_api: v1.11

  - func: randperm.generator(SymInt n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: randperm.generator(int n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: v1.11, v2.0
    op_api: v1.11

  - func: randperm.generator_out(SymInt n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: randperm.generator_out(int n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11, v2.0
    op_api: v1.11

  - func: randperm.out(SymInt n, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: randperm.out(int n, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11, v2.0
    op_api: v1.11

  - func: range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: range.out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: reciprocal(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::kFloat : self.scalar_type()'
      exec: aclnnReciprocal

  - func: reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnReciprocal

  - func: reciprocal_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceReciprocal

  - func: reflection_pad1d(Tensor self, SymInt[2] padding) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      structured_inherit: reflection_pad1d.out

  - func: reflection_pad1d(Tensor self, int[2] padding) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      structured_inherit: reflection_pad1d.out

  - func: reflection_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      out:
        size: reflection_pad1d_npu_out_size(self, padding)
        dtype: self
      exec: aclnnReflectionPad1d

  - func: reflection_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: reflection_pad1d_npu_out_size(self, padding)
        dtype: self
      exec: aclnnReflectionPad1d

  - func: reflection_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      structured_inherit: reflection_pad1d_backward.grad_input

  - func: reflection_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      structured_inherit: reflection_pad1d_backward.grad_input

  - func: reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnReflectionPad1dBackward

  - func: reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnReflectionPad1dBackward

  - func: reflection_pad2d(Tensor self, SymInt[4] padding) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      structured_inherit: reflection_pad2d.out

  - func: reflection_pad2d(Tensor self, int[4] padding) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      structured_inherit: reflection_pad2d.out

  - func: reflection_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      out:
        size: reflection_pad2d_npu_out_size(self, padding)
        dtype: self
      exec: aclnnReflectionPad2d

  - func: reflection_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: reflection_pad2d_npu_out_size(self, padding)
        dtype: self
      exec: aclnnReflectionPad2d

  - func: reflection_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      structured_inherit: reflection_pad2d_backward.grad_input

  - func: reflection_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      structured_inherit: reflection_pad2d_backward.grad_input

  - func: reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnReflectionPad2dBackward

  - func: reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnReflectionPad2dBackward

  - func: reflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor
    op_api: [v2.0, newest]
    gen_opapi:
      structured_inherit: reflection_pad3d.out

  - func: reflection_pad3d(Tensor self, int[6] padding) -> Tensor
    op_api: v1.11
    gen_opapi:
      structured_inherit: reflection_pad3d.out

  - func: reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
    op_api: [v2.0, newest]
    gen_opapi:
      out:
        size: reflection_pad3d_npu_out_size(self, padding)
        dtype: self
      exec: aclnnReflectionPad3d

  - func: reflection_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)
    op_api: v1.11
    gen_opapi:
      out:
        size: reflection_pad3d_npu_out_size(self, padding)
        dtype: self
      exec: aclnnReflectionPad3d

  - func: reflection_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor
    op_api: [v2.0, newest]
    gen_opapi:
      structured_inherit: reflection_pad3d_backward.grad_input

  - func: reflection_pad3d_backward(Tensor grad_output, Tensor self, int[6] padding) -> Tensor
    op_api: v1.11
    gen_opapi:
      structured_inherit: reflection_pad3d_backward.grad_input

  - func: reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: [v2.0, newest]
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnReflectionPad3dBackward

  - func: reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnReflectionPad3dBackward

  - func: relu(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnRelu

  - func: relu_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceRelu

  - func: remainder.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: remainder.Scalar_Tensor(Scalar self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: remainder.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: remainder_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: remainder_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: renorm_(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: repeat(Tensor self, SymInt[] repeats) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      out:
        size: repeat_npu_output_size(self, repeats)
        dtype: self
      exec: aclnnRepeat

  - func: repeat(Tensor self, int[] repeats) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: repeat_npu_output_size(self, repeats)
        dtype: self
      exec: aclnnRepeat

  - func: repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, SymInt? output_size=None) -> Tensor
    acl_op: [v2.2, newest]
    op_api: [v2.2, newest]

  - func: repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, int? output_size=None) -> Tensor
    acl_op: v1.11, v2.0, v2.1
    op_api: v1.11, v2.1

  - func: repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, SymInt? output_size=None) -> Tensor
    acl_op: [v2.2, newest]
    op_api: [v2.2, newest]

  - func: repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, int? output_size=None) -> Tensor
    acl_op: v2.0, v2.1
    op_api: v2.1

  - func: repeat_interleave.self_int(Tensor self, int repeats, int? dim=None, *, int? output_size=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: replication_pad1d(Tensor self, SymInt[2] padding) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      structured_inherit: replication_pad1d.out

  - func: replication_pad1d(Tensor self, int[2] padding) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      structured_inherit: replication_pad1d.out

  - func: replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      out:
        size: replication_pad1d_npu_out_size(self, padding)
        dtype: self
      exec: aclnnReplicationPad1d

  - func: replication_pad1d.out(Tensor self, int[2] padding, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: replication_pad1d_npu_out_size(self, padding)
        dtype: self
      exec: aclnnReplicationPad1d

  - func: replication_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      structured_inherit: replication_pad1d_backward.grad_input

  - func: replication_pad1d_backward(Tensor grad_output, Tensor self, int[2] padding) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      structured_inherit: replication_pad1d_backward.grad_input

  - func: replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnReplicationPad1dBackward

  - func: replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, int[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnReplicationPad1dBackward

  - func: replication_pad2d(Tensor self, SymInt[4] padding) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      structured_inherit: replication_pad2d.out

  - func: replication_pad2d(Tensor self, int[4] padding) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      structured_inherit: replication_pad2d.out

  - func: replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      out:
        size: replication_pad2d_npu_out_size(self, padding)
        dtype: self
      exec: aclnnReplicationPad2d

  - func: replication_pad2d.out(Tensor self, int[4] padding, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: replication_pad2d_npu_out_size(self, padding)
        dtype: self
      exec: aclnnReplicationPad2d

  - func: replication_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      structured_inherit: replication_pad2d_backward.grad_input

  - func: replication_pad2d_backward(Tensor grad_output, Tensor self, int[4] padding) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      structured_inherit: replication_pad2d_backward.grad_input

  - func: replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnReplicationPad2dBackward

  - func: replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, int[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnReplicationPad2dBackward

  - func: replication_pad3d(Tensor self, SymInt[6] padding) -> Tensor
    op_api: [v2.0, newest]
    gen_opapi:
      structured_inherit: replication_pad3d.out

  - func: replication_pad3d(Tensor self, int[6] padding) -> Tensor
    op_api: v1.11
    gen_opapi:
      structured_inherit: replication_pad3d.out

  - func: replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)
    op_api: [v2.0, newest]
    gen_opapi:
      out:
        size: replication_pad3d_npu_out_size(self, padding)
        dtype: self
      exec: aclnnReplicationPad3d

  - func: replication_pad3d.out(Tensor self, int[6] padding, *, Tensor(a!) out) -> Tensor(a!)
    op_api: v1.11
    gen_opapi:
      out:
        size: replication_pad3d_npu_out_size(self, padding)
        dtype: self
      exec: aclnnReplicationPad3d

  - func: replication_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor
    op_api: [v2.0, newest]
    gen_opapi:
      structured_inherit: replication_pad3d_backward.grad_input

  - func: replication_pad3d_backward(Tensor grad_output, Tensor self, int[6] padding) -> Tensor
    op_api: v1.11
    gen_opapi:
      structured_inherit: replication_pad3d_backward.grad_input

  - func: replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: [v2.0, newest]
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnReplicationPad3dBackward

  - func: replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, int[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnReplicationPad3dBackward

  - func: roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -> Tensor
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnRoll

  - func: roll(Tensor self, int[1] shifts, int[1] dims=[]) -> Tensor
    acl_op: v1.11, v2.0
    op_api: v1.11
    gen_opapi:
      grad_input:
        size: self
        dtype: self
      exec: aclnnRoll

  - func: round(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: round.out

  - func: round.decimals(Tensor self, *, int decimals) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: round.decimals_out(Tensor self, *, int decimals, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnRound

  - func: round_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceRound

  - func: round_.decimals(Tensor(a!) self, *, int decimals) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: rrelu_with_noise(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: rrelu_with_noise.out(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: rrelu_with_noise_(Tensor(a!) self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: rsqrt(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'isIntegralType(self.scalar_type(), true) ? at::kFloat : self.scalar_type()'
      exec: aclnnRsqrt

  - func: rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
      exec: aclnnRsqrt

  - func: rsqrt_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceRsqrt

  - func: rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: at::native::result_type(self, other)
      exec: aclnnRsubs

  - func: rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: broadcast_ops_npu_output_size(self, other)
        dtype: at::native::result_type(self, other)
      exec: aclnnRsub

  - func: scaled_dot_product_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> Tensor
    op_api: v2.1, v2.2, v2.3, v2.4

  - func: scaled_dot_product_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None, bool enable_gqa=False) -> Tensor
    op_api: [v2.5, newest]

  - func: scatter.reduce_out(Tensor self, int dim, Tensor index, Tensor src, *, str reduce, Tensor(a!) out) -> Tensor(a!)
    op_api: v1.11

  - func: scatter.src(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
    acl_op: v2.0

  - func: scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: scatter.value(Tensor self, int dim, Tensor index, Scalar value) -> Tensor
    acl_op: v2.0

  - func: scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: scatter.value_reduce_out(Tensor self, int dim, Tensor index, Scalar value, *, str reduce, Tensor(a!) out) -> Tensor(a!)
    op_api: v1.11

  - func: scatter_.src(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: scatter_.value(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: searchsorted.Scalar(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: 'c10::SmallVector<int64_t, op_infer::SIZE>{}'
        dtype: 'out_int32 ? at::kInt : at::kLong'
      exec: aclnnSearchSorteds, sorted_sequence, self, out_int32, right, sorter, out

  - func: searchsorted.Tensor(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: searchsorted.Tensor_out

  - func: searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: 'out_int32 ? at::kInt : at::kLong'
      exec: aclnnSearchSorted, sorted_sequence, self, out_int32, right, sorter, out

  - func: sgn(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: sgn.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: sigmoid(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: '(isIntegralType(self.scalar_type(), true)) ? at::kFloat : self.scalar_type()'
      exec: aclnnSigmoid

  - func: sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: out
      exec: aclnnSigmoid

  - func: sigmoid_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceSigmoid

  - func: sigmoid_backward(Tensor grad_output, Tensor output) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: 'broadcast_ops_npu_output_size(grad_output, output)'
        dtype: 'at::native::result_type(grad_output, output)'
      exec: aclnnSigmoidBackward

  - func: sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: 'broadcast_ops_npu_output_size(grad_output, output)'
        dtype: grad_input
      exec: aclnnSigmoidBackward

  - func: sign(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: sign.out

  - func: sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnSign

  - func: sign_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: sign.out

  - func: signbit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: out
      exec: aclnnSignbit

  - func: silu(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: silu.out

  - func: silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnSilu

  - func: silu_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: silu.out

  - func: silu_backward(Tensor grad_output, Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: silu_backward.grad_input

  - func: silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: grad_output
        dtype: grad_output
      exec: aclnnSiluBackward

  - func: sin(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: '(isIntegralType(self.scalar_type(), true)) ? at::kFloat : self.scalar_type()'
      exec: aclnnSin

  - func: sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: out
      exec: aclnnSin

  - func: sin_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceSin

  - func: sinc(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: '(isIntegralType(self.scalar_type(), true)) ? at::kFloat : self.scalar_type()'
      exec: aclnnSinc

  - func: sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: out
      exec: aclnnSinc

  - func: sinc_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceSinc

  - func: sinh(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: '(isIntegralType(self.scalar_type(), true)) ? at::kFloat : self.scalar_type()'
      exec: aclnnSinh

  - func: sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: out
      exec: aclnnSinh

  - func: sinh_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceSinh

  - func: slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)
    acl_op: all_version
    op_api: all_version

  - func: slow_conv_transpose2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: slow_conv_transpose2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: _slow_conv2d_forward.output(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, *, Tensor(a!) output) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: slow_conv_dilated2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: slow_conv_dilated2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, int[2] padding=0, int[2] dilation=1) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: slow_conv3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0) -> Tensor
    acl_op: [v2.0, newest]

  - func: slow_conv3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0) -> Tensor
    acl_op: v1.11

  - func: slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]

  - func: slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, int[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11

  - func: slow_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding) -> Tensor
    acl_op: [v2.0, newest]

  - func: slow_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding) -> Tensor
    acl_op: v1.11

  - func: slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, *, Tensor(a!) output) -> Tensor(a!)
    acl_op: [v2.0, newest]

  - func: slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, int[3] padding, *, Tensor(a!) output) -> Tensor(a!)
    acl_op: v1.11

  - func: smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean, float beta=1.0) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: '(reduction == at::Reduction::None) ? broadcast_ops_npu_output_size(self, target) : c10::SmallVector<int64_t, op_infer::SIZE>{}'
        dtype: self
      exec: aclnnSoftMarginLoss

  - func: soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: '(reduction == at::Reduction::None) ? broadcast_ops_npu_output_size(self, target) : c10::SmallVector<int64_t, op_infer::SIZE>{}'
        dtype: out
      exec: aclnnSoftMarginLoss

  - func: soft_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: 'broadcast_ops_npu_output_size(broadcast_ops_npu_output_size(grad_output.sizes(), self.sizes()), target.sizes())'
        dtype: self
      exec: aclnnSoftMarginLossBackward

  - func: soft_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: 'broadcast_ops_npu_output_size(broadcast_ops_npu_output_size(grad_output.sizes(), self.sizes()), target.sizes())'
        dtype: grad_input
      exec: aclnnSoftMarginLossBackward

  - func: softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: softplus(Tensor self, Scalar beta=1, Scalar threshold=20) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: softplus.out

  - func: softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnSoftplus

  - func: softplus_backward(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold) -> Tensor
    acl_op: v2.0

  - func: softplus_backward.grad_input(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnSoftplusBackward

  - func: softshrink(Tensor self, Scalar lambd=0.5) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnSoftshrink

  - func: softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: out
      exec: aclnnSoftshrink

  - func: softshrink_backward(Tensor grad_output, Tensor self, Scalar lambd) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: 'broadcast_ops_npu_output_size(grad_output, self)'
        dtype: 'at::native::result_type(grad_output, self)'
      exec: aclnnSoftshrinkBackward

  - func: softshrink_backward.grad_input(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: 'broadcast_ops_npu_output_size(grad_output, self)'
        dtype: grad_input
      exec: aclnnSoftshrinkBackward

  - func: sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
    acl_op: [v2.1, newest]
    op_api: all_version

  - func: sort.dimname(Tensor self, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)
    acl_op: all_version
    op_api: all_version

  - func: sort.dimname_values(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
    acl_op: all_version
    op_api: all_version

  - func: sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)
    op_api: all_version

  - func: sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
    acl_op: all_version
    op_api: all_version

  - func: sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
    op_api: all_version

  - func: sqrt(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: '(isIntegralType(self.scalar_type(), true)) ? at::kFloat : self.scalar_type()'
      exec: aclnnSqrt

  - func: sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: out
      exec: aclnnSqrt

  - func: sqrt_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceSqrt

  - func: square.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: stack(Tensor[] tensors, int dim=0) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: stack_output_size(tensors, dim)
        dtype: at::native::result_type(tensors)
      exec: aclnnStack

  - func: stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: stack_output_size(tensors, dim)
        dtype: tensors[0]
      exec: aclnnStack

  - func: std.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: std.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: std.correction(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False) -> Tensor
    acl_op: v2.0

  - func: std.correction_out(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: std.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: std.correction_out(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
    acl_op: v2.0

  - func: std_mean.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
    acl_op: v1.11
    op_api: v1.11

  - func: std_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: std_mean.correction(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False) -> (Tensor, Tensor)
    acl_op: v2.0

  - func: stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
    op_api: [v2.1, v2.6]

  - func: stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None, bool? align_to_window=None) -> Tensor
    op_api: [v2.7, newest]

  - func: sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: sum(Tensor self, *, ScalarType? dtype=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: sum.DimnameList_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: sum.IntList_out(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: sum.dim_IntList(Tensor self, int[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: take(Tensor self, Tensor index) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: take.out

  - func: take.out(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: index
        dtype: self
      exec: aclnnTake

  - func: tan(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: '(isIntegralType(self.scalar_type(), true)) ? at::kFloat : self.scalar_type()'
      exec: aclnnTan

  - func: tan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: out
      exec: aclnnTan

  - func: tan_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceTan

  - func: tanh(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: tanh_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: tanh_backward(Tensor grad_output, Tensor output) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: tanh_backward.grad_input

  - func: tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      grad_input:
        size: 'broadcast_ops_npu_output_size(grad_output, output)'
        dtype: grad_output
      exec: aclnnTanhBackward

  - func: threshold(Tensor self, Scalar threshold, Scalar value) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnThresholdBackward

  - func: topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: topk(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)
    acl_op: v1.11, v2.0
    op_api: v1.11

  - func: topk.values(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: topk.values(Tensor self, int k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)
    acl_op: v1.11, v2.0
    op_api: v1.11

  - func: trace(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: 'c10::SmallVector<int64_t, op_infer::N>{}'
        dtype: '(isIntegralType(self.scalar_type(), true)) ? at::kLong : self.scalar_type()'
      exec: aclnnTrace

  - func: triangular_solve(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False) -> (Tensor solution, Tensor cloned_coefficient)
    acl_op: v2.0

  - func: triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)
    acl_op: all_version

  - func: tril(Tensor self, int diagonal=0) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: tril.out

  - func: tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnTril

  - func: tril_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceTril

  - func: triu(Tensor self, int diagonal=0) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: triu.out

  - func: triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnTriu

  - func: triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceTriu

  - func: true_divide.Scalar(Tensor self, Scalar other) -> Tensor
    acl_op: v2.0

  - func: true_divide.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: v2.0

  - func: true_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v2.0

  - func: true_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: v2.0

  - func: true_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: v2.0

  - func: trunc(Tensor self) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: trunc.out

  - func: trunc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnTrunc

  - func: trunc_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceTrunc

  - func: _transform_bias_rescale_qkv(Tensor qkv, Tensor qkv_bias, int num_heads) -> (Tensor, Tensor, Tensor)
    op_api: all_version

  - func: uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: upsample_bicubic2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.0, newest]

  - func: upsample_bicubic2d(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.0, newest]

  - func: upsample_bicubic2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_bicubic2d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: _upsample_bicubic2d_aa_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: [v2.1, newest]

  - func: _upsample_bicubic2d_aa_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: v1.11

  - func: _upsample_bicubic2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: [v2.1, newest]

  - func: _upsample_bicubic2d_aa_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: v1.11

  - func: _upsample_bicubic2d_aa(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: [v2.0, newest]

  - func: _upsample_bicubic2d_aa(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: v1.11

  - func: _upsample_bicubic2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    op_api: [v2.0, newest]

  - func: _upsample_bicubic2d_aa.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    op_api: v1.11

  - func: upsample_bicubic2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_bicubic2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_bicubic2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_bicubic2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_bilinear2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_bilinear2d(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_bilinear2d.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_bilinear2d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_bilinear2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_bilinear2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_bilinear2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_bilinear2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_bilinear2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: _upsample_bilinear2d_aa(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: [v2.1, newest]

  - func: _upsample_bilinear2d_aa(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: v1.11

  - func: _upsample_bilinear2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    op_api: [v2.1, newest]

  - func: _upsample_bilinear2d_aa.out(Tensor self, int[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    op_api: v1.11

  - func: _upsample_bilinear2d_aa_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: [v2.1, newest]

  - func: _upsample_bilinear2d_aa_backward(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: v1.11

  - func: _upsample_bilinear2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: [v2.1, newest]

  - func: _upsample_bilinear2d_aa_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: v1.11

  - func: upsample_linear1d(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_linear1d(Tensor self, int[1] output_size, bool align_corners, float? scales=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_linear1d.out(Tensor self, int[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_linear1d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_linear1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_linear1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, bool align_corners, float? scales=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_linear1d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_nearest1d(Tensor self, int[1] output_size, float? scales=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_nearest1d.out(Tensor self, int[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest1d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_nearest1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_nearest1d_backward.grad_input(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest1d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_nearest2d(Tensor self, int[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_nearest2d.out(Tensor self, int[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest2d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_nearest2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_nearest2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest2d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_nearest3d(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_nearest3d.out(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest3d.vec(Tensor input, int[]? output_size, float[]? scale_factors) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_nearest3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_nearest3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_nearest3d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, float[]? scale_factors) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: _upsample_nearest_exact1d.out(Tensor self, SymInt[3] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
    op_api: [v2.1, newest]

  - func: _upsample_nearest_exact1d.out(Tensor self, int[3] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)
    op_api: v1.11

  - func: _upsample_nearest_exact1d(Tensor self, SymInt[3] output_size, float? scales=None) -> Tensor
    op_api: [v2.1, newest]

  - func: _upsample_nearest_exact1d(Tensor self, int[3] output_size, float? scales=None) -> Tensor
    op_api: v1.11

  - func: _upsample_nearest_exact2d.out(Tensor self, SymInt[3] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    op_api: [v2.1, newest]

  - func: _upsample_nearest_exact2d.out(Tensor self, int[3] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    op_api: v1.11

  - func: _upsample_nearest_exact2d(Tensor self, SymInt[3] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: [v2.1, newest]

  - func: _upsample_nearest_exact2d(Tensor self, int[3] output_size, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: v1.11

  - func: _upsample_nearest_exact3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: [v2.1, newest]

  - func: _upsample_nearest_exact3d(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: v1.11

  - func: _upsample_nearest_exact3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    op_api: [v2.1, newest]

  - func: _upsample_nearest_exact3d.out(Tensor self, int[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    op_api: v1.11

  - func: _upsample_nearest_exact1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: [v2.1, newest]

  - func: _upsample_nearest_exact1d_backward.grad_input(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: v1.11

  - func: _upsample_nearest_exact1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -> Tensor
    op_api: [v2.1, newest]

  - func: _upsample_nearest_exact1d_backward(Tensor grad_output, int[1] output_size, int[3] input_size, float? scales=None) -> Tensor
    op_api: v1.11

  - func: _upsample_nearest_exact2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: [v2.1, newest]

  - func: _upsample_nearest_exact2d_backward.grad_input(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: v1.11

  - func: _upsample_nearest_exact2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: [v2.1, newest]

  - func: _upsample_nearest_exact2d_backward(Tensor grad_output, int[2] output_size, int[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: v1.11

  - func: upsample_trilinear3d(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_trilinear3d(Tensor self, int[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_trilinear3d.out(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_trilinear3d.out(Tensor self, int[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_trilinear3d.vec(Tensor input, int[]? output_size, bool align_corners, float[]? scale_factors) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_trilinear3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_trilinear3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: upsample_trilinear3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: upsample_trilinear3d_backward.vec(Tensor grad_output, int[]? output_size, int[] input_size, bool align_corners, float[]? scale_factors) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: _upsample_nearest_exact3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: [v2.1, newest]

  - func: _upsample_nearest_exact3d_backward(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor
    op_api: v1.11

  - func: _upsample_nearest_exact3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: [v2.1, newest]

  - func: _upsample_nearest_exact3d_backward.grad_input(Tensor grad_output, int[3] output_size, int[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)
    op_api: v1.11

  - func: var.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: var.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: var.correction(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False) -> Tensor
    acl_op: v2.0

  - func: var.correction_out(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: var.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: var.correction_out(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)
    acl_op: v2.0

  - func: var_mean.correction(Tensor self, int[1]? dim, *, int? correction, bool keepdim=False) -> (Tensor, Tensor)
    acl_op: v1.11
    op_api: v1.11

  - func: var_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)
    acl_op: [v2.1, newest]
    op_api: [v2.1, newest]

  - func: var_mean.correction(Tensor self, int[1]? dim=None, *, int? correction=None, bool keepdim=False) -> (Tensor, Tensor)
    acl_op: v2.0

  - func: vdot(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: 'c10::SmallVector<int64_t, op_infer::N>{}'
        dtype: self
      exec: aclnnDot

  - func: vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: 'c10::SmallVector<int64_t, op_infer::N>{}'
        dtype: out
      exec: aclnnDot

  - func: view_as_complex(Tensor(a) self) -> Tensor(a)
    acl_op: all_version

  - func: view_as_real(Tensor(a) self) -> Tensor(a)
    acl_op: all_version

  - func: where(Tensor condition) -> Tensor[]
    acl_op: all_version
    op_api: all_version

  - func: where.self(Tensor condition, Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: where.self_out(Tensor condition, Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: out
      exec: aclnnXLogYScalarOther

  - func: xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: other
        dtype: out
      exec: aclnnXLogYScalarSelf

  - func: xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: 'broadcast_ops_npu_output_size(self, other)'
        dtype: out
      exec: aclnnXLogYTensor

  - func: xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: '(isIntegralType(at::result_type(self, other), true)) ? at::kFloat : at::result_type(self, other)'
      exec: aclnnXLogYScalarOther

  - func: xlogy.Scalar_Self(Scalar self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: other
        dtype: '(isIntegralType(at::result_type(self, other), true)) ? at::kFloat : at::result_type(self, other)'
      exec: aclnnXLogYScalarSelf, self, other, out

  - func: xlogy.Tensor(Tensor self, Tensor other) -> Tensor
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out:
        size: 'broadcast_ops_npu_output_size(self, other)'
        dtype: '(isIntegralType(at::result_type(self, other), true)) ? at::kFloat : at::result_type(self, other)'
      exec: aclnnXLogYTensor

  - func: xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      exec: aclnnInplaceXLogYScalarOther

  - func: xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      structured_inherit: xlogy.OutTensor

  - func: zero_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: zeros(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: v1.11
    op_api: v1.11

  - func: zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: zeros.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: zeros.out(int[] size, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11
    op_api: v1.11

  - func: zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: isin.Tensor_Tensor(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor
    op_api: [v2.1, newest]

  - func: isin.Tensor_Tensor_out(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)
    op_api: [v2.1, newest]

custom:
  - func: npu_gather_backward(Tensor grad, SymInt[] self_size, int dim, Tensor index, bool sparse_grad) -> Tensor
    op_api: all_version

  - func: _amp_foreach_non_finite_check(Tensor[] scaled_grads) -> bool
    acl_op: all_version

  - func: npu_gelu(Tensor self, *, str approximate='none') -> Tensor
    op_api: [v2.1, newest]
    gen_opapi:
      out:
        size: self
        dtype: self
      new_params:
        approximate_mode: 'npu_gelu_approximate_mode(approximate)'
      exec: aclnnGeluV2, self, approximate_mode, out

  - func: npu_gelu_backward(Tensor grad_output, Tensor self, *, str approximate='none') -> Tensor
    op_api: [v2.1, newest]
    gen_opapi:
      out:
        size: 'broadcast_ops_npu_output_size(grad_output, self)'
        dtype: 'at::native::result_type(grad_output, self)'
      new_params:
        approximate_str: 'npu_gelu_approximate_str(approximate)'
        approximate_ptr: 'const_cast<char *>(approximate_str.c_str())'
      exec: aclnnGeluBackwardV2, grad_output, self, approximate_ptr, out

  - func: _conv_depthwise2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)
    acl_op: all_version

  - func: _dropout_with_byte_mask(Tensor self, float p) -> (Tensor, Tensor)
    acl_op: all_version
    tags: nondeterministic_seeded

  - func: _dropout_with_byte_mask_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
    acl_op: all_version

  - func: _npu_ciou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -> (Tensor, Tensor)
    acl_op: all_version

  - func: _npu_dropout(Tensor self, float p) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version
    tags: nondeterministic_seeded
    exposed: all_version

  - func: _npu_dropout_gen_mask.Tensor(Tensor self, int[] size, float p, int seed, int offset, *, bool? parallel=True, bool? sync=None) -> Tensor
    acl_op: all_version
    op_api: all_version
    tags: nondeterministic_seeded

  - func: _npu_silent_check(Tensor(a!) input_grad, Tensor val, Tensor(b!) pre_val, Tensor(c!) min_val, Tensor(d!) max_val, Tensor val_counter, int c_min_steps, float c_thresh_l1, float c_coeff_l1, float c_thresh_l2, float c_coeff_l2) -> Tensor
    acl_op: all_version

  - func: _npu_silent_check_v2(Tensor val, Tensor(a!) input_grad, Tensor(b!) sfda, Tensor(c!) step, int c_min_steps, float c_thresh_l1, float c_coeff_l1, float c_thresh_l2, float c_coeff_l2, int npu_asd_detect) -> Tensor
    op_api: all_version

  - func: _npu_silent_check_v3(Tensor val, Tensor(a!) input_grad, Tensor(b!) step, Tensor(c!) max, Tensor(d!) avg, float c_thresh_l1, float c_thresh_l2, float betal, int npu_asd_detect) -> Tensor
    op_api: all_version

  - func: batch_norm_gather_stats_update(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
    acl_op: all_version

  - func: batch_norm_reduce(Tensor input, float eps) -> (Tensor, Tensor)
    acl_op: all_version

  - func: crop_and_resize(Tensor self, float[]? boxes, int[] box_index, int[] crop_size, float extrapolation_value=0, str method="bilinear") -> Tensor
    acl_op: v1.11, v2.0, v2.1

  - func: decode_jpeg(Tensor self, int[] image_shape, int channels=3, bool try_recover_truncated=False) -> Tensor
    acl_op: v1.11, v2.0, v2.1

  - func: dropout_with_byte_mask(Tensor self, float p, bool train) -> Tensor
    acl_op: all_version
    tags: nondeterministic_seeded

  - func: fast_gelu(Tensor self) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: image_normalize(Tensor self, float[]? mean, float[]? variance, int dtype=0) -> Tensor
    acl_op: v1.11, v2.0, v2.1

  - func: image_normalize_(Tensor(a!) self, float[]? mean, float[]? variance, int dtype=0) -> Tensor(a!)
    acl_op: v1.11, v2.0, v2.1

  - func: img_to_tensor(Tensor self) -> Tensor
    acl_op: v1.11, v2.0, v2.1

  - func: kl_div_backward(Tensor grad_output, Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.0, newest]
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnKlDivBackward

  - func: l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      grad_input:
        size: 'broadcast_ops_npu_output_size(broadcast_ops_npu_output_size(self, target), grad_output.sizes())'
        dtype: 'promoteTypes(grad_output.scalar_type(), at::native::result_type(target, self))'
      exec: aclnnL1LossBackward

  - func: slow_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
    acl_op: all_version
    op_api: all_version

  - func: matmul_backward(Tensor grad, Tensor self, Tensor other, bool[2] mask) -> (Tensor, Tensor)
    acl_op: v1.11

  - func: matmul_double_backward(Tensor? grad_self, Tensor? grad_other, Tensor grad_out, Tensor self, Tensor other, bool[3] mask) -> (Tensor, Tensor, Tensor)
    op_api: [v2.1, newest]

  - func: npu_add_layer_norm(Tensor x1, Tensor x2, Tensor gamma, Tensor beta, float epsilon=1e-05, bool additional_output=False) -> (Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: npu_add_layer_norm_backward(Tensor? dy_opt, Tensor x1, Tensor x2, Tensor rstd, Tensor mean, Tensor gamma, Tensor? dsum_opt) -> (Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: npu_add_rms_norm(Tensor x1, Tensor x2, Tensor gamma, float epsilon=1e-06) -> (Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version
    gen_opapi:
      out0:
        size: rms_norm_npu_output_size(x1, gamma)[0]
        dtype: x1
      out1:
        size: rms_norm_npu_output_size(x1, gamma)[1]
        dtype: at::kFloat
      out2:
        size: rms_norm_npu_output_size(x1, gamma)[0]
        dtype: x1
      exec: aclnnAddRmsNorm

  - func: npu_add_rms_norm_cast(Tensor x1, Tensor x2, Tensor gamma, float epsilon=1e-06) -> (Tensor, Tensor, Tensor, Tensor)
    op_api: all_version
    gen_opapi:
      out0:
        size: rms_norm_npu_output_size(x1, gamma)[0]
        dtype: at::kFloat
      out1:
        size: rms_norm_npu_output_size(x1, gamma)[0]
        dtype: x1
      out2:
        size: rms_norm_npu_output_size(x1, gamma)[1]
        dtype: at::kFloat
      out3:
        size: rms_norm_npu_output_size(x1, gamma)[0]
        dtype: x1
      exec: aclnnAddRmsNormCast

  - func: npu_alltoallv_gmm(Tensor gmm_x, Tensor gmm_weight, str hcom, int ep_world_size, int[] send_counts, int[] recv_counts, *, Tensor? send_counts_tensor=None, Tensor? recv_counts_tensor=None, Tensor? mm_x=None, Tensor? mm_weight=None, bool trans_gmm_weight=False, bool trans_mm_weight=False, bool permute_out_flag=False) -> (Tensor, Tensor, Tensor)
    op_api: all_version
    exposed: all_version

  - func: npu_gmm_alltoallv(Tensor gmm_x, Tensor gmm_weight, str hcom, int ep_world_size, int[] send_counts, int[] recv_counts, *, Tensor? send_counts_tensor=None, Tensor? recv_counts_tensor=None, Tensor? mm_x=None, Tensor? mm_weight=None, bool trans_gmm_weight=False, bool trans_mm_weight=False) -> (Tensor, Tensor)
    op_api: all_version
    exposed: all_version

  - func: npu_all_gather_base_mm(Tensor self, Tensor x2, str hcom, int world_size, *, Tensor? bias=None, int gather_index=0, bool gather_output=True, int comm_turn=0) -> (Tensor, Tensor)
    op_api: all_version
    exposed: all_version

  - func: npu_alloc_float_status(Tensor self) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_anchor_response_flags(Tensor self, int[2] featmap_size, int[2] stride, int num_base_anchors) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_anti_quant(Tensor x, Tensor scale, *, Tensor? offset=None, ScalarType? dst_dtype=None, ScalarType? src_dtype=None) -> Tensor
    op_api: [v2.1, newest]
    exposed: [v2.1, newest]

  - func: npu_apply_adam(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov) -> (Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_apply_adam.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
    acl_op: all_version

  - func: npu_apply_adam_w(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize) -> (Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_apply_adam_w.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
    acl_op: all_version

  - func: npu_apply_rotary_pos_emb(Tensor query, Tensor key, Tensor cos, Tensor sin, str layout='BSH') -> (Tensor, Tensor)
    op_api: all_version

  - func: npu_kv_rmsnorm_rope_cache(Tensor kv, Tensor gamma, Tensor cos, Tensor sin, Tensor index, Tensor k_cache, Tensor ckv_cache, *, Tensor? k_rope_scale=None, Tensor? c_kv_scale=None, Tensor? k_rope_offset=None, Tensor? c_kv_offset=None, float epsilon=1e-5, str cache_mode='Norm', bool is_output_kv=False) -> (Tensor, Tensor, Tensor, Tensor)
    op_api: all_version

  - func: npu_batch_gather_matmul(Tensor self, Tensor x, Tensor weight_b, Tensor indices, Tensor? weight_a=None, int layer_idx=0, float scale=1e-3, int y_offset=0, int y_slice_size=-1) -> Tensor
    op_api: all_version

  - func: npu_batch_gather_matmul_(Tensor(a!) self, Tensor x, Tensor weight_b, Tensor indices, Tensor? weight_a=None, int layer_idx=0, float scale=1e-3, int y_offset=0, int y_slice_size=-1) -> Tensor(a!)
    op_api: all_version

  - func: npu_batch_nms(Tensor self, Tensor scores, float score_threshold, float iou_threshold, int max_size_per_class, int max_total_size, bool change_coordinate_frame=False, bool transpose_box=False) -> (Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version
    exposed: all_version

  - func: npu_bert_apply_adam(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0) -> (Tensor var, Tensor m, Tensor v)
    acl_op: all_version
    exposed: all_version

  - func: npu_bert_apply_adam.out(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
    acl_op: all_version
    exposed: all_version

  - func: npu_binary_cross_entropy_with_logits_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
    acl_op: v1.11
    op_api: v1.11
    gen_opapi:
      out:
        size: target
        dtype: target
      exec: aclnnBinaryCrossEntropyWithLogitsBackward

  - func: npu_binary_cross_entropy_with_logits_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight_opt, Tensor? pos_weight_opt, int reduction) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]
    gen_opapi:
      out:
        size: target
        dtype: target
      exec: aclnnBinaryCrossEntropyWithLogitsBackward

  - func: npu_bmmV2(Tensor self, Tensor mat2, int[] output_sizes) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_bmm_v2_mat1_backward(Tensor grad, Tensor mat1, Tensor mat2, SymInt[] size) -> Tensor
    acl_op: [v2.0, newest]

  - func: npu_bmm_v2_mat1_backward(Tensor grad, Tensor mat1, Tensor mat2, int[] size) -> Tensor
    acl_op: v1.11

  - func: npu_bmm_v2_mat2_backward(Tensor grad, Tensor mat1, Tensor mat2, SymInt[] size) -> Tensor
    acl_op: [v2.0, newest]

  - func: npu_bmm_v2_mat2_backward(Tensor grad, Tensor mat1, Tensor mat2, int[] size) -> Tensor
    acl_op: v1.11

  - func: npu_bounding_box_decode(Tensor rois, Tensor deltas, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3, int[1] max_shape, float wh_ratio_clip) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_bounding_box_encode(Tensor anchor_box, Tensor ground_truth_box, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_broadcast(Tensor self, int[] size) -> Tensor
    acl_op: all_version

  - func: npu_broadcast.out(Tensor self, int[] size, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    device_check: NoCheck

  - func: npu_ciou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_ciou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, Tensor? atan_sub, bool trans=False, bool is_cross=True, int mode=0) -> (Tensor, Tensor)
    acl_op: all_version

  - func: npu_clear_float_status(Tensor self, int mode=0) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_confusion_transpose(Tensor self, int[] perm, int[] shape, bool transpose_first) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_confusion_transpose_backward(Tensor grad, int[] perm, SymInt[] shape, bool transpose_first) -> Tensor
    acl_op: [v2.0, newest]

  - func: npu_confusion_transpose_backward(Tensor grad, int[] perm, int[] shape, bool transpose_first) -> Tensor
    acl_op: v1.11

  - func: npu_conv2d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
    acl_op: all_version

  - func: npu_conv2d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version

  - func: npu_conv2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_conv3d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_conv3d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    exposed: all_version

  - func: npu_conv3d_backward(Tensor input, Tensor grad, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_conv_transpose2d(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
    acl_op: all_version

  - func: npu_conv_transpose2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_conv_transpose3d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_convert_weight_to_int4pack(Tensor weight, int inner_k_tiles=0) -> Tensor
    op_api: all_version
    exposed: all_version
    internal_format_opapi: all_version

  - func: npu_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
    acl_op: all_version

  - func: npu_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_convolution_transpose(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
    acl_op: all_version

  - func: npu_convolution_transpose_backward(Tensor input, Tensor grad, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] grad_input_mask) -> (Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_deep_norm(Tensor x, Tensor gx, Tensor beta, Tensor gamma, float alpha=0.3, float epsilon=1e-06) -> (Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: npu_deep_norm_backward(Tensor dy, Tensor x, Tensor gx, Tensor gamma, Tensor mean, Tensor rstd, float alpha=0.3) -> (Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: npu_deformable_conv2d(Tensor input, Tensor weight, Tensor offset, Tensor? bias, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor)
    acl_op: all_version
    exposed: all_version

  - func: npu_deformable_conv2dbk(Tensor input, Tensor grad_output, Tensor offset_out, Tensor weight, Tensor offset, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_diou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_diou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)
    acl_op: all_version

  - func: npu_dropout_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: npu_dropout_do_mask(Tensor self, Tensor mask, float p) -> (Tensor, Tensor)
    acl_op: all_version
    tags: nondeterministic_seeded

  - func: npu_dropout_gen_mask(int[] size, float p, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: all_version
    tags: nondeterministic_seeded
    dispatch:
      CompositeExplicitAutograd: npu_dropout_gen_mask

  - func: npu_dropout_with_add_softmax(Tensor self, Tensor x1, Scalar alpha, float prob, int dim) -> (Tensor, Tensor, Tensor)
    acl_op: all_version
    tags: nondeterministic_seeded
    exposed: all_version

  - func: npu_dropout_with_add_softmax_backward(Tensor grad, Tensor mask, Tensor softmax_out, Scalar alpha, float prob, int dim) -> (Tensor, Tensor)
    acl_op: all_version

  - func: npu_dtype_cast(Tensor self, ScalarType dtype) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: npu_dtype_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
    acl_op: all_version

  - func: npu_dtype_cast_backward(Tensor grad, ScalarType dtype) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: npu_dynamic_quant(Tensor input, *, Tensor? smooth_scales=None, Tensor? group_index=None, ScalarType? dst_type=None) -> (Tensor, Tensor)
    op_api: [v2.1, newest]
    exposed: [v2.1, newest]

  - func: npu_dynamic_quant_asymmetric(Tensor input, *, Tensor? smooth_scales=None, Tensor? group_index=None, ScalarType? dst_type=None) -> (Tensor, Tensor, Tensor)
    op_api: [v2.1, newest]
    exposed: [v2.1, newest]

  - func: npu_fast_gelu(Tensor self) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_fast_gelu_backward(Tensor grad, Tensor self) -> Tensor
    acl_op: all_version

  - func: npu_ffn(Tensor x, Tensor weight1, Tensor weight2, str activation, *, int[]? expert_tokens=None, int[]? expert_tokens_index=None, Tensor? bias1=None, Tensor? bias2=None, Tensor? scale=None, Tensor? offset=None, Tensor? deq_scale1=None, Tensor? deq_scale2=None, Tensor? antiquant_scale1=None, Tensor? antiquant_scale2=None, Tensor? antiquant_offset1=None, Tensor? antiquant_offset2=None, int? inner_precise=None, ScalarType? output_dtype=None) -> Tensor
    op_api: all_version
    exposed: all_version

  - func: npu_flash_attention(Tensor query, Tensor key, Tensor value, int head_num, str input_layout, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, float scale=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, int inner_precise=1, int[]? prefix=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None, int sparse_mode=0, bool gen_mask_parallel=True, bool sync=False) -> (Tensor, Tensor, Tensor, Tensor, int, int, int)
    op_api: v1.11

  - func: npu_flash_attention_grad(Tensor query, Tensor key, Tensor value, Tensor dy, int head_num, str input_layout, *, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? softmax_max=None, Tensor? softmax_sum=None, Tensor? softmax_in=None, Tensor? attention_in=None, float scale_value=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, int inner_precise=1, int seed=0, int offset=0, int numels=0, int[]? prefix=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None, int sparse_mode=0, bool gen_mask_parallel=True, bool sync=False) -> (Tensor, Tensor, Tensor, Tensor)
    op_api: v1.11

  - func: npu_fused_attention_score(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_fused_attention_score_backward(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_fused_attention_score_fwd(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_fused_attention_score_grad(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_fused_infer_attention_score(Tensor query, Tensor key, Tensor value, *, Tensor? pse_shift=None, Tensor? atten_mask=None, SymInt[]? actual_seq_lengths=None, SymInt[]? actual_seq_lengths_kv=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? key_antiquant_scale=None, Tensor? key_antiquant_offset=None, Tensor? value_antiquant_scale=None, Tensor? value_antiquant_offset=None, Tensor? block_table=None, Tensor? query_padding_size=None, Tensor? kv_padding_size=None, Tensor? key_shared_prefix=None, Tensor? value_shared_prefix=None, SymInt[]? actual_shared_prefix_len=None, Tensor? query_rope=None, Tensor? key_rope=None, Tensor? key_rope_antiquant_scale=None, int num_heads=1, float scale=1.0, int pre_tokens=2147483647, int next_tokens=2147483647, str input_layout="BSH", int num_key_value_heads=0, int sparse_mode=0, int inner_precise=0, int block_size=0, int antiquant_mode=0, int key_antiquant_mode=0, int value_antiquant_mode=0, bool softmax_lse_flag=False) -> (Tensor, Tensor)
    op_api: [v2.1, newest]
    exposed: v2.1
    internal_format_opapi: [v2.1, newest]

  - func: npu_fused_infer_attention_score.out(Tensor query, Tensor key, Tensor value, *, Tensor? pse_shift=None, Tensor? atten_mask=None, SymInt[]? actual_seq_lengths=None, SymInt[]? actual_seq_lengths_kv=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? key_antiquant_scale=None, Tensor? key_antiquant_offset=None, Tensor? value_antiquant_scale=None, Tensor? value_antiquant_offset=None, Tensor? block_table=None, Tensor? query_padding_size=None, Tensor? kv_padding_size=None, Tensor? key_shared_prefix=None, Tensor? value_shared_prefix=None, SymInt[]? actual_shared_prefix_len=None, Tensor? query_rope=None, Tensor? key_rope=None, Tensor? key_rope_antiquant_scale=None, int num_heads=1, float scale=1.0, int pre_tokens=2147483647, int next_tokens=2147483647, str input_layout="BSH", int num_key_value_heads=0, int sparse_mode=0, int inner_precise=0, int block_size=0, int antiquant_mode=0, int key_antiquant_mode=0, int value_antiquant_mode=0, bool softmax_lse_flag=False, Tensor? workspace=None, Tensor(a!) attention_out, Tensor(b!) softmax_lse) -> (Tensor(a!), Tensor(b!))
    op_api: [v2.1, newest]
    internal_format_opapi: [v2.1, newest]

  - func: _npu_fused_infer_attention_score_get_max_workspace(Tensor query, Tensor key, Tensor value, *, Tensor? pse_shift=None, Tensor? atten_mask=None, SymInt[]? actual_seq_lengths=None, SymInt[]? actual_seq_lengths_kv=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? key_antiquant_scale=None, Tensor? key_antiquant_offset=None, Tensor? value_antiquant_scale=None, Tensor? value_antiquant_offset=None, Tensor? block_table=None, Tensor? query_padding_size=None, Tensor? kv_padding_size=None, Tensor? key_shared_prefix=None, Tensor? value_shared_prefix=None, SymInt[]? actual_shared_prefix_len=None, Tensor? query_rope=None, Tensor? key_rope=None, Tensor? key_rope_antiquant_scale=None, int num_heads=1, float scale=1.0, int pre_tokens=2147483647, int next_tokens=2147483647, str input_layout="BSH", int num_key_value_heads=0, int sparse_mode=0, int inner_precise=0, int block_size=0, int antiquant_mode=0, int key_antiquant_mode=0, int value_antiquant_mode=0, bool softmax_lse_flag=False) -> Tensor
    op_api: [v2.1, newest]

  - func: npu_fusion_attention(Tensor query, Tensor key, Tensor value, int head_num, str input_layout, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, float scale=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, int inner_precise=0, int[]? prefix=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None, int sparse_mode=0, bool gen_mask_parallel=True, bool sync=False) -> (Tensor, Tensor, Tensor, Tensor, int, int, int)
    op_api: [v2.0, newest]
    exposed: [v2.0, newest]

  - func: npu_fusion_attention_grad(Tensor query, Tensor key, Tensor value, Tensor dy, int head_num, str input_layout, *, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? softmax_max=None, Tensor? softmax_sum=None, Tensor? softmax_in=None, Tensor? attention_in=None, float scale_value=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, int inner_precise=0, int seed=0, int offset=0, int numels=0, int[]? prefix=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None, int sparse_mode=0, bool gen_mask_parallel=True, bool sync=False) -> (Tensor, Tensor, Tensor, Tensor)
    op_api: [v2.0, newest]

  - func: npu_fusion_attention_v2(Tensor query, Tensor key, Tensor value, int head_num, str input_layout, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? query_rope=None, Tensor? key_rope=None, float scale=1., float keep_prob=1., int pre_tokens=2147483647, int next_tokens=2147483647, int inner_precise=0, int[]? prefix=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None, int sparse_mode=0, bool gen_mask_parallel=True, bool sync=False, int pse_type=1, int[]? q_start_idx=None, int[]? kv_start_idx=None) -> (Tensor, Tensor, Tensor, Tensor, int, int, int)
    op_api: [v2.1, newest]

  - func: npu_fusion_attention_grad_v2(Tensor query, Tensor key, Tensor value, Tensor dy, int head_num, str input_layout, *, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? softmax_max=None, Tensor? softmax_sum=None, Tensor? softmax_in=None, Tensor? attention_in=None, Tensor? query_rope=None, Tensor? key_rope=None, float scale_value=1., float keep_prob=1., int pre_tokens=2147483647, int next_tokens=2147483647, int inner_precise=0, int seed=0, int offset=0, int numels=0, int[]? prefix=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None, int sparse_mode=0, bool gen_mask_parallel=True, bool sync=False, int pse_type=1, int[]? q_start_idx=None, int[]? kv_start_idx=None) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    op_api: [v2.1, newest]

  - func: npu_fused_attention_layernorm_qkv_fwd(Tensor x, Tensor kernel_query, Tensor kernel_key, Tensor kernel_value, Tensor gamma, Tensor beta, Tensor? bias_query=None, Tensor? bias_key=None, Tensor? bias_value=None, int seq_len=128, int num_heads=12, float eps=1e-05) -> Tensor[]
    acl_op: all_version

  - func: npu_fused_attention_qkv_grad(Tensor grad_output_query, Tensor grad_output_key, Tensor grad_output_value, Tensor query_kernel, Tensor key_kernel, Tensor value_kernel, Tensor hidden_states, Tensor grad_output_ln) -> Tensor[]
    acl_op: all_version

  - func: npu_geglu(Tensor self, int dim=-1, int approximate=1, bool activate_left=False) -> (Tensor, Tensor)
    op_api: all_version
    exposed: v1.11, v2.0

  - func: npu_geglu_grad(Tensor grad_output, Tensor self, Tensor gelu, int dim=-1, int approximate=1, bool activate_left=False) -> Tensor
    op_api: all_version

  - func: npu_get_float_status(Tensor self, int mode=0) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_giou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_giou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)
    acl_op: all_version

  - func: npu_grid_assign_positive(Tensor self, Tensor overlaps, Tensor box_responsible_flags, Tensor max_overlaps, Tensor argmax_overlaps, Tensor gt_max_overlaps, Tensor gt_argmax_overlaps, int num_gts, float pos_iou_thr, float min_pos_iou, bool gt_max_assign_all) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_group_norm_silu(Tensor input, Tensor? weight, Tensor? bias, int group, float eps=0.00001) -> (Tensor, Tensor, Tensor)
    op_api: all_version
    exposed: all_version
    gen_opapi:
      out0:
        size: input
        dtype: input
      out1:
        size: '{input.size(0), group}'
        dtype: input
      out2:
        size: '{input.size(0), group}'
        dtype: input
      exec: aclnnGroupNormSilu

  - func: npu_grouped_matmul(Tensor[] x, Tensor[] weight, *, Tensor[] bias, Tensor[] scale, Tensor[] offset, Tensor[] antiquant_scale, Tensor[] antiquant_offset, int[]? group_list=None, int? split_item=0, ScalarType? output_dtype=None) -> Tensor[]
    op_api: v1.11, v2.0
    exposed: v1.11, v2.0

  - func: npu_grouped_matmul(Tensor[] x, Tensor[] weight, *, Tensor[]? bias=None, Tensor[]? scale=None, Tensor[]? offset=None, Tensor[]? antiquant_scale=None, Tensor[]? antiquant_offset=None, Tensor[]? per_token_scale=None, Tensor? group_list=None, Tensor[]? activation_input=None, Tensor[]? activation_quant_scale=None, Tensor[]? activation_quant_offset=None, int? split_item=0, int? group_type=-1, int? group_list_type=0, int? act_type=0, int[]? tuning_config=None, ScalarType? output_dtype=None) -> Tensor[]
    op_api: [v2.1, newest]
    exposed: [v2.1, newest]
    internal_format_opapi: [v2.1, newest]

  - func: npu_grouped_matmul.List(Tensor[] x, Tensor[] weight, *, Tensor[]? bias=None, Tensor[]? scale=None, Tensor[]? offset=None, Tensor[]? antiquant_scale=None, Tensor[]? antiquant_offset=None, Tensor[]? per_token_scale=None, int[]? group_list=None, Tensor[]? activation_input=None, Tensor[]? activation_quant_scale=None, Tensor[]? activation_quant_offset=None, int? split_item=0, int? group_type=-1, int? group_list_type=0, int? act_type=0, ScalarType? output_dtype=None) -> Tensor[]
    op_api: [v2.1, newest]
    exposed: [v2.1, newest]

  - func: npu_grouped_matmul_finalize_routing(Tensor x, Tensor w, Tensor group_list, *, Tensor? scale=None, Tensor? bias=None, Tensor? pertoken_scale=None, Tensor? shared_input=None, Tensor? logit=None, Tensor? row_index=None, ScalarType? dtype=None, float? shared_input_weight=1.0, int? shared_input_offset=0, int? output_bs=0, int? group_list_type=1) -> Tensor
    op_api: all_version
    exposed: all_version
    internal_format_opapi: all_version

  - func: npu_gru(Tensor input, Tensor hx, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version
    tags: nondeterministic_seeded

  - func: npu_gru_backward(Tensor? grady, Tensor? gradh, Tensor input, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, Tensor hx, Tensor y_output, Tensor h_output, Tensor output_updata, Tensor output_reset, Tensor output_new, Tensor hidden_new) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_hans_encode.out(Tensor input, bool statistic=False, bool reshuff=False, *, Tensor(a!) pdf, Tensor(b!) mantissa, Tensor(c!) fixed, Tensor(d!) var) -> (Tensor(a!), Tensor(b!), Tensor(c!), Tensor(d!))
    op_api: all_version
    gen_opapi:
      exec: aclnnHansEncode, input, pdf, statistic, reshuff, mantissa, fixed, var

  - func: npu_hans_decode.out(Tensor mantissa, Tensor fixed, Tensor var, Tensor pdf, bool reshuff=False, *, Tensor(a!) out) -> Tensor(a!)
    op_api: all_version
    gen_opapi:
      exec: aclnnHansDecode

  - func: npu_ifmr(Tensor data, Tensor data_min, Tensor data_max, Tensor cumsum, float min_percentile, float max_percentile, float search_start, float search_end, float search_step, bool with_offset) -> (Tensor, Tensor)
    acl_op: all_version

  - func: npu_incre_flash_attention(Tensor query, Tensor key, Tensor value, *, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? pse_shift=None, SymInt[]? actual_seq_lengths=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? block_table=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? kv_padding_size=None, int num_heads=1, float scale_value=1.0, str input_layout="BSH", int num_key_value_heads=0, int block_size=0, int inner_precise=1) -> Tensor
    op_api: [v2.1, newest]
    exposed: [v2.1, newest]

  - func: npu_interleave_rope(Tensor x, Tensor cos, Tensor sin) -> Tensor
    op_api: all_version

  - func: npu_indexing(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_indexing.out(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    exposed: all_version
    device_check: NoCheck

  - func: npu_indexing_trans_contiguous.out(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11, v2.0, v2.1

  - func: npu_iou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_layer_norm_eval(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05) -> Tensor
    acl_op: all_version

  - func: npu_layernorm_grad(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias) -> (Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
    acl_op: all_version
    op_api: all_version
    exposed: all_version

  - func: npu_linear_backward(Tensor grad, Tensor input, Tensor weight) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: npu_lstm(Tensor input, Tensor weight, Tensor bias, Tensor seq_mask, Tensor h, Tensor c, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, bool flag_seq, bool direction) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version
    tags: nondeterministic_seeded
    exposed: all_version

  - func: npu_lstm_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor weight, Tensor bias, Tensor hx, Tensor cx, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_lstm_cell(Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_lstm_cell_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_lstm_data(Tensor input, Tensor batch_sizes, Tensor weight, Tensor bias, Tensor seq_mask, Tensor h, Tensor c, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, bool flag_seq, bool direction) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version
    tags: nondeterministic_seeded
    device_check: NoCheck

  - func: npu_lstm_data_backward(Tensor? grady_opt, Tensor? gradh_opt, Tensor? gradc_opt, Tensor input, Tensor batch_sizes, Tensor weight, Tensor bias, Tensor init_h, Tensor init_c, Tensor y, Tensor h, Tensor c, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc, bool flag_direction) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version
    device_check: NoCheck

  - func: npu_masked_fill_range(Tensor self, Tensor start, Tensor end, Tensor value, int axis=-1) -> Tensor
    acl_op: all_version

  - func: npu_masked_softmax_with_rel_pos_bias(Tensor x, Tensor? atten_mask, Tensor relative_pos_bias, float scale_value=1.0, int inner_precision_mode=0) -> Tensor
    op_api: all_version
    gen_opapi:
      out:
        size: x
        dtype: x
      exec: aclnnMaskedSoftmaxWithRelPosBias

  - func: npu_max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    acl_op: all_version
    exposed: all_version

  - func: npu_max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    acl_op: all_version
    exposed: all_version

  - func: npu_max_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor
    acl_op: [v2.0, newest]

  - func: npu_max_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim) -> Tensor
    acl_op: v1.11

  - func: npu_min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    acl_op: all_version

  - func: npu_min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    acl_op: all_version

  - func: npu_min_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor
    acl_op: [v2.0, newest]

  - func: npu_min_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim) -> Tensor
    acl_op: v1.11

  - func: npu_mish(Tensor self) -> Tensor
    acl_op: all_version

  - func: npu_mish_backward(Tensor grad, Tensor input) -> Tensor
    acl_op: all_version

  - func: npu_mla_prolog(Tensor token_x, Tensor weight_dq, Tensor weight_uq_qr, Tensor weight_uk, Tensor weight_dkv_kr, Tensor rmsnorm_gamma_cq, Tensor rmsnorm_gamma_ckv, Tensor rope_sin, Tensor rope_cos, Tensor cache_index, Tensor kv_cache, Tensor kr_cache, *, Tensor? dequant_scale_x=None, Tensor? dequant_scale_w_dq=None, Tensor? dequant_scale_w_uq_qr=None, Tensor? dequant_scale_w_dkv_kr=None, Tensor? quant_scale_ckv=None, Tensor? quant_scale_ckr=None, Tensor? smooth_scales_cq=None, float rmsnorm_epsilon_cq=1e-05, float rmsnorm_epsilon_ckv=1e-05, str cache_mode="PA_BSND") -> (Tensor, Tensor, Tensor, Tensor)
    acl_op: [v2.1, newest]
    exposed: [v2.1, newest]
    internal_format_opapi: [v2.1, newest]

  - func: npu_mla_prolog_v2(Tensor token_x, Tensor weight_dq, Tensor weight_uq_qr, Tensor weight_uk, Tensor weight_dkv_kr, Tensor rmsnorm_gamma_cq, Tensor rmsnorm_gamma_ckv, Tensor rope_sin, Tensor rope_cos, Tensor cache_index, Tensor kv_cache, Tensor kr_cache, *, Tensor? dequant_scale_x=None, Tensor? dequant_scale_w_dq=None, Tensor? dequant_scale_w_uq_qr=None, Tensor? dequant_scale_w_dkv_kr=None, Tensor? quant_scale_ckv=None, Tensor? quant_scale_ckr=None, Tensor? smooth_scales_cq=None, float rmsnorm_epsilon_cq=1e-05, float rmsnorm_epsilon_ckv=1e-05, str cache_mode="PA_BSND") -> (Tensor, Tensor, Tensor, Tensor, Tensor)
    op_api: [v2.1, newest]
    exposed: [v2.1, newest]
    internal_format_opapi: [v2.1, newest]

  - func: npu_mm_all_reduce_base(Tensor x1, Tensor x2, str hcom, *, str reduce_op='sum', Tensor? bias=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? x3=None, Tensor? dequant_scale=None, Tensor? pertoken_scale=None, Tensor? comm_quant_scale_1=None, Tensor? comm_quant_scale_2=None, int antiquant_group_size=0, int comm_turn=0) -> Tensor
    op_api: all_version
    exposed: all_version
    internal_format_opapi: all_version

  - func: npu_mm_reduce_scatter_base(Tensor self, Tensor x2, str hcom, int world_size, *, str reduce_op='sum', Tensor? bias=None, int comm_turn=0) -> Tensor
    op_api: all_version
    exposed: all_version

  - func: npu_moe_compute_expert_tokens(Tensor sorted_expert_for_source_row, int num_expert) -> Tensor
    op_api: all_version
    exposed: all_version
    gen_opapi:
      out:
        size: '{num_expert}'
        dtype: sorted_expert_for_source_row
      exec: aclnnMoeComputeExpertTokens

  - func: npu_moe_finalize_routing(Tensor expanded_permuted_rows, Tensor? skip1, Tensor? skip2, Tensor? bias, Tensor? scales, Tensor expanded_src_to_dst_row, Tensor? export_for_source_row, int? drop_pad_mode=0) -> Tensor
    op_api: all_version
    exposed: all_version

  - func: npu_moe_gating_top_k_softmax(Tensor x, Tensor? finished=None, int k=1) -> (Tensor, Tensor, Tensor)
    op_api: all_version
    exposed: all_version

  - func: npu_moe_gating_top_k(Tensor x, int k, *, Tensor? bias=None, int k_group=1, int group_count=1, int group_select_mode=0, int renorm=0, int norm_type=0, bool out_flag=False, float routed_scaling_factor=1.0, float eps=1e-20) -> (Tensor, Tensor, Tensor)
    op_api: all_version

  - func: npu_moe_init_routing(Tensor x, Tensor row_idx, Tensor expert_idx, int active_num) -> (Tensor, Tensor, Tensor)
    op_api: all_version
    exposed: all_version

  - func: npu_moe_init_routing_v2(Tensor x, Tensor expert_idx, *, Tensor? scale=None, Tensor? offset=None, int active_num=-1, int expert_capacity=-1, int expert_num=-1, int drop_pad_mode=0, int expert_tokens_num_type=0, bool expert_tokens_num_flag=False, int quant_mode=0, int[2] active_expert_range=[], int row_idx_type=0) -> (Tensor, Tensor, Tensor, Tensor)
    op_api: all_version

  - func: npu_moe_distribute_dispatch(Tensor x, Tensor expert_ids, str group_ep, int ep_world_size, int ep_rank_id, int moe_expert_num, *, Tensor? scales=None, Tensor? x_active_mask=None, Tensor? expert_scales=None, str group_tp="", int tp_world_size=0, int tp_rank_id=0, int expert_shard_type=0, int shared_expert_num=1, int shared_expert_rank_num=0, int quant_mode=0, int global_bs=0, int expert_token_nums_type=1) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    op_api: all_version

  - func: npu_moe_distribute_combine(Tensor expand_x, Tensor expert_ids, Tensor expand_idx, Tensor ep_send_counts, Tensor expert_scales, str group_ep, int ep_world_size, int ep_rank_id, int moe_expert_num, *, Tensor? tp_send_counts=None, Tensor? x_active_mask=None, Tensor? activation_scale=None, Tensor? weight_scale=None, Tensor? group_list=None, Tensor? expand_scales=None, Tensor? shared_expert_x=None, str group_tp="", int tp_world_size=0, int tp_rank_id=0, int expert_shard_type=0, int shared_expert_num=1, int shared_expert_rank_num=0, int global_bs=0, int out_dtype=0, int comm_quant_mode=0, int group_list_type=0) -> Tensor
    op_api: all_version

  - func: npu_moe_re_routing(Tensor tokens, Tensor expert_token_num_per_rank, *, Tensor? per_token_scales=None, int expert_token_num_type=1, int idx_type=0) -> (Tensor, Tensor, Tensor, Tensor)
    op_api: all_version

  - func: npu_multi_head_attention(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor attn_mask, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor? dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version
    tags: nondeterministic_seeded
    exposed: all_version

  - func: npu_multi_head_attention_backward(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor query_res, Tensor key_res, Tensor value_res, Tensor attn_scores, Tensor attn_res, Tensor context, Tensor y_grad, Tensor dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    acl_op: all_version

  - func: npu_multi_head_attention_v2(Tensor query, Tensor key, Tensor value, Tensor? atten_mask=None, Tensor? alibi_mask=None, float scale=1.0, int head_num=1, str input_layout="BNSD", float keep_prob=1., int pre_tokens=2147483647, int next_tokens=1, bool gen_mask_parallel=True, bool sync=False) -> (Tensor, Tensor, int, int, int)
    op_api: all_version

  - func: npu_multi_head_attention_v2_grad(Tensor attention_score_grad, Tensor query, Tensor key, Tensor value, Tensor softmax_log_max_sum, Tensor attention_score, Tensor? atten_mask=None, Tensor? alibi_mask=None, float scale=1.0, int head_num=1, str input_layout="BNSD", float keep_prob=1., int pre_tokens=2147483647, int next_tokens=1, int seed=0, int offset=0, int numels=0, bool gen_mask_parallel=True, bool sync=False) -> (Tensor, Tensor, Tensor)
    op_api: all_version

  - func: npu_nms_rotated(Tensor self, Tensor scores, float iou_threshold, float scores_threshold=0, int max_output_size=-1, int mode=0) -> (Tensor, Tensor)
    acl_op: all_version

  - func: npu_nms_v4(Tensor self, Tensor scores, Scalar max_output_size, Tensor iou_threshold, Tensor scores_threshold, bool pad_to_max_output_size=False) -> (Tensor, Tensor)
    acl_op: all_version
    exposed: all_version

  - func: npu_nms_with_mask(Tensor input, Scalar iou_threshold) -> (Tensor, Tensor, Tensor)
    acl_op: all_version
    exposed: all_version

  - func: npu_normalize_batch(Tensor self, Tensor seq_len, int normalize_type=0) -> Tensor
    acl_op: all_version

  - func: npu_one_hot(Tensor self, int num_classes=-1, int depth=1, Scalar on_value=1, Scalar off_value=0) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_pad(Tensor input, int[] paddings) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_prompt_flash_attention(Tensor query, Tensor key, Tensor value, *, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? pse_shift=None, int[]? actual_seq_lengths=None, Tensor? deq_scale1=None, Tensor? quant_scale1=None, Tensor? deq_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, int num_heads=1, float scale_value=1.0, int pre_tokens=2147473647, int next_tokens=0, str input_layout="BSH", int num_key_value_heads=0, int[]? actual_seq_lengths_kv=None, int sparse_mode=0) -> Tensor
    op_api: [v2.1, newest]
    exposed: [v2.1, newest]

  - func: npu_ps_roi_pooling(Tensor self, Tensor rois, float spatial_scale, int group_size, int output_dim) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_ps_roi_pooling_backward(Tensor output_grad, Tensor rois, float spatial_scale, int group_size, int output_dim, SymInt[] input_size) -> Tensor
    acl_op: [v2.0, newest]

  - func: npu_ps_roi_pooling_backward(Tensor output_grad, Tensor rois, float spatial_scale, int group_size, int output_dim, int[] input_size) -> Tensor
    acl_op: v1.11

  - func: npu_ptiou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor
    acl_op: all_version

  - func: npu_prefetch(Tensor self, Tensor? dependency, int max_size, int offset=0) -> ()
    op_api: [v2.1, newest]
    exposed: [v2.1, newest]
    internal_format_opapi: all_version

  - func: npu_quant_conv2d(Tensor input, Tensor weight, Tensor scale, int[2] strides=1, int[2] pads=0, int[2] dilations=1, int groups=1, int offset_x=0, str round_mode='rint', ScalarType? output_dtype=None, Tensor? bias=None, Tensor? offset=None) -> Tensor
    acl_op: [v2.1, newest]

  - func: npu_quant_matmul(Tensor x1, Tensor x2, Tensor scale, *, Tensor? offset=None, Tensor? pertoken_scale=None, Tensor? bias=None, ScalarType? output_dtype=None) -> Tensor
    op_api: all_version
    exposed: all_version
    internal_format_opapi: all_version

  - func: npu_quant_matmul(Tensor x1, Tensor x2, Tensor scale, Tensor? offset=None, Tensor? bias=None, str? output_dtype=None) -> Tensor
    op_api: v2.0
    internal_format_opapi: v2.0

  - func: npu_quant_matmul_dequant(Tensor x, Tensor quantized_weight, Tensor weight_scale, *, Tensor? bias=None, Tensor? x_scale=None, Tensor? x_offset=None, Tensor? smooth_scale=None, str? quant_mode='pertoken') -> Tensor
    op_api: all_version
    gen_opapi:
      out:
        size: '{x.size(0), weight_scale.size(0)}'
        dtype: x
      new_params:
        quant_mode_attr: 'quant_mode.has_value() ? const_cast<char *>(quant_mode.value().data()) : nullptr'
        trans: 'true'
      exec: aclnnQuantMatmulDequant, x, quantized_weight, weight_scale, bias, x_scale, x_offset, smooth_scale, quant_mode_attr, trans, out

  - func: npu_quant_grouped_matmul_dequant(Tensor x, Tensor quantized_weight, Tensor weight_scale, Tensor group_list, *, Tensor? bias=None, Tensor? x_scale=None, Tensor? x_offset=None, Tensor? smooth_scale=None, str? quant_mode='pertoken') -> Tensor
    op_api: all_version
    gen_opapi:
      out:
        size: '{x.size(0), weight_scale.size(1)}'
        dtype: x
      new_params:
        quant_mode_attr: 'quant_mode.has_value() ? const_cast<char *>(quant_mode.value().data()) : nullptr'
        trans: 'true'
      exec: aclnnQuantGroupedMatmulDequant, x, quantized_weight, weight_scale, group_list, bias, x_scale, x_offset, smooth_scale, quant_mode_attr, trans, out

  - func: npu_quant_scatter(Tensor self, Tensor indices, Tensor updates, Tensor quant_scales, Tensor? quant_zero_points=None, int axis=0, int quant_axis=1, str reduce='update') -> Tensor
    op_api: all_version
    exposed: all_version

  - func: npu_quant_scatter_(Tensor(a!) self, Tensor indices, Tensor updates, Tensor quant_scales, Tensor? quant_zero_points=None, int axis=0, int quant_axis=1, str reduce='update') -> Tensor(a!)
    op_api: all_version
    exposed: all_version

  - func: npu_quantize(Tensor self, Tensor scales, Tensor? zero_points, ScalarType dtype, int axis=1, bool div_mode=True) -> Tensor
    acl_op: all_version
    op_api: all_version
    exposed: all_version

  - func: npu_kronecker_quant(Tensor x, Tensor kronecker_p1, Tensor kronecker_p2, float? clip_ratio=None, ScalarType? dst_dtype=None) -> (Tensor out, Tensor quant_scale)
    op_api: all_version
    gen_opapi:
      out:
        size: kronecker_quant_out_size(x)
        dtype: at::kInt
      quant_scale:
        size: kronecker_quant_scale_size(x)
        dtype: at::kFloat
      new_params:
        clip_ratio_attr: clip_ratio.value_or(1.0)
      exec: aclnnFlatQuant, x, kronecker_p1, kronecker_p2, clip_ratio_attr, out, quant_scale


  - func: npu_group_quant(Tensor x, Tensor scale, Tensor group_index, *, Tensor? offset=None, ScalarType? dst_dtype=None) -> Tensor
    op_api: all_version
    gen_opapi:
      out:
        size: 'npu_group_quant_out_size(x, dst_dtype)'
        dtype: dst_type
      new_params:
        dst_type: npu_group_quant_dst_type(dst_dtype)
      exec: aclnnGroupQuant, x, scale, group_index, offset, dst_type, out

  - func: npu_random_choice_with_mask(Tensor x, int count=256, int seed=0, int seed2=0) -> (Tensor, Tensor)
    acl_op: all_version
    tags: nondeterministic_seeded
    exposed: all_version

  - func: npu_reshape(Tensor self, int[] shape, bool can_refresh=False) -> Tensor
    acl_op: all_version
    device_check: NoCheck

  - func: npu_reshape.out(Tensor self, int[] shape, bool can_refresh=False, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    device_check: NoCheck

  - func: npu_rms_norm(Tensor self, Tensor gamma, float epsilon=1e-06) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version
    exposed: all_version

  - func: npu_gemma_rms_norm(Tensor self, Tensor gamma, float epsilon=1e-06) -> (Tensor, Tensor)
    op_api: all_version

  - func: npu_rms_norm_backward(Tensor dy, Tensor self, Tensor gamma, Tensor rstd) -> (Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: npu_roi_align(Tensor self, Tensor rois, float spatial_scale, int pooled_height, int pooled_width, int sample_num, int roi_end_mode) -> Tensor
    acl_op: all_version
    op_api: all_version
    exposed: all_version

  - func: npu_roi_alignbk(Tensor self, Tensor rois, int[] xdiff_shape, int pooled_width, int pooled_height, float spatial_scale, int sample_num, int? roi_end_mode=None) -> Tensor
    acl_op: all_version
    op_api: all_version

  - func: npu_rotary_mul(Tensor self, Tensor r1, Tensor r2, str rotary_mode='half') -> Tensor
    acl_op: all_version
    op_api: all_version
    exposed: all_version

  - func: npu_rotary_mul_backward(Tensor grad, Tensor self, Tensor r1, Tensor r2, str rotary_mode='half') -> (Tensor, Tensor, Tensor)
    acl_op: all_version
    op_api: all_version

  - func: npu_mrope(Tensor positions, Tensor query, Tensor key, Tensor cos_sin_cache, int head_size, *, int[]? mrope_section=None, str? rotary_mode='half') -> (Tensor, Tensor)
    op_api: all_version
    exposed: all_version
    gen_opapi:
      query_out:
        size: query
        dtype: query
      key_out:
        size: key
        dtype: key
      new_params:
        is_neox_style_value: 'is_neox_style(static_cast<std::string>(rotary_mode.value_or("half")))'
        mrope_section_value: 'mrope_section.value_or(at::IntArrayRef{0, 0, 0})'
      exec: aclnnRopeWithSinCosCache, positions, query, key, cos_sin_cache, mrope_section_value, head_size, is_neox_style_value, query_out, key_out

  - func: npu_rotated_box_decode(Tensor self, Tensor deltas, Tensor weight) -> Tensor
    acl_op: all_version

  - func: npu_rotated_box_encode(Tensor self, Tensor gt_bboxes, Tensor weight) -> Tensor
    acl_op: all_version

  - func: npu_rotated_iou(Tensor self, Tensor query_boxes, bool trans=False, int mode=0, bool is_cross=True, float v_threshold=0.0, float e_threshold=0.0) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_rotated_overlaps(Tensor self, Tensor query_boxes, bool trans=False) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_scaled_masked_softmax(Tensor x, Tensor mask, Scalar scale=1, bool fixed_triu_mask=False) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_scaled_masked_softmax_backward(Tensor y_grad, Tensor y, Tensor mask, Scalar scale, bool fixed_triu_mask) -> Tensor
    acl_op: all_version

  - func: npu_scatter(Tensor self, Tensor indices, Tensor updates, int dim) -> Tensor
    acl_op: all_version

  - func: npu_scatter_list(Tensor[] self, Tensor indices, Tensor updates, Tensor? mask=None, str reduce='update', int axis=-2) -> Tensor[]
    op_api: [v2.1, newest]

  - func: npu_scatter_list_(Tensor(a!)[] self, Tensor indices, Tensor updates, Tensor? mask=None, str reduce='update', int axis=-2) -> ()
    op_api: [v2.1, newest]

  - func: npu_scatter_nd_update(Tensor self, Tensor indices, Tensor updates) -> Tensor
    op_api: all_version
    exposed: all_version

  - func: npu_scatter_nd_update_(Tensor(a!) self, Tensor indices, Tensor updates) -> Tensor(a!)
    op_api: all_version
    exposed: all_version

  - func: npu_sign_bits_pack(Tensor self, int size) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_sign_bits_unpack(Tensor input, int size, ScalarType dtype) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_silu(Tensor self) -> Tensor
    acl_op: all_version

  - func: npu_silu_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version

  - func: npu_silu_backward(Tensor grad_output, Tensor x0, Tensor x1) -> Tensor
    acl_op: all_version

  - func: npu_slice(Tensor self, int[] offsets, int[] size) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_slice.out(Tensor self, int[] offsets, int[] size, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    exposed: all_version
    device_check: NoCheck

  - func: npu_softmax_cross_entropy_with_logits(Tensor self, Tensor labels) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_softmax_cross_entropy_with_logits_backward(Tensor grad, Tensor self, Tensor labels) -> Tensor
    acl_op: all_version

  - func: npu_sort_v2(Tensor self, int dim=-1, bool descending=False) -> Tensor
    acl_op: all_version

  - func: npu_sort_v2.out(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version

  - func: npu_stride_add(Tensor self, Tensor other, Scalar offset1, Scalar offset2, Scalar c1_len) -> Tensor
    acl_op: all_version

  - func: npu_stride_copy(Tensor self, int[] shape, int[] stride, Scalar storage_offset) -> Tensor
    acl_op: all_version

  - func: npu_stride_copy.out(Tensor self, int[] shape, int[] stride, Scalar storage_offset, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version

  - func: npu_sub_sample(Tensor self, int per_images, float positive_fraction) -> Tensor
    acl_op: all_version

  - func: npu_swiglu(Tensor self, int dim=-1) -> Tensor
    op_api: all_version
    exposed: all_version
    gen_opapi:
      out:
        size: 'swiglu_backward_infershape(self, dim)'
        dtype: self
      exec: aclnnSwiGlu

  - func: npu_swiglu_backward(Tensor grad_output, Tensor self, int dim=-1) -> Tensor
    op_api: all_version
    gen_opapi:
      out:
        size: self
        dtype: self
      exec: aclnnSwiGluGrad

  - func: npu_dequant_swiglu_quant(Tensor x, *, Tensor? weight_scale=None, Tensor? activation_scale=None, Tensor? bias=None, Tensor? quant_scale=None, Tensor? quant_offset=None, Tensor? group_index=None, bool activate_left=False, int quant_mode=0) -> (Tensor, Tensor)
    op_api: all_version

  - func: npu_dequant_rope_quant_kvcache(Tensor x, Tensor cos, Tensor sin, Tensor k_cache, Tensor v_cache, Tensor indices, Tensor scale_k, Tensor scale_v, int[3] size_splits, *, Tensor? offset_k=None, Tensor? offset_v=None, Tensor? weight_scale=None, Tensor? activation_scale=None, Tensor? bias=None, int quant_mode=0, str input_layout="BSND", bool kv_output=False, str cache_mode="contiguous") -> (Tensor, Tensor, Tensor, Tensor, Tensor)
    op_api: all_version

  - func: npu_rope_quant_kvcache(Tensor x, Tensor cos, Tensor sin, Tensor k_cache, Tensor v_cache, Tensor indices, Tensor scale_k, Tensor scale_v, int[3] size_splits, *, Tensor? offset_k=None, Tensor? offset_v=None, int quant_mode=0, str input_layout="BSND", bool kv_output=False, str cache_mode="contiguous") -> (Tensor, Tensor, Tensor, Tensor, Tensor)
    op_api: all_version

  - func: npu_dequant_bias(Tensor x, Tensor weight_scale, Tensor? activation_scale, Tensor? bias, *, ScalarType? output_dtype=None) -> Tensor
    op_api: all_version

  - func: npu_trans_quant_param(Tensor scale, Tensor? offset=None, int? round_mode=0) -> Tensor
    op_api: all_version
    exposed: all_version

  - func: npu_transpose(Tensor self, int[] perm, bool require_contiguous=True) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: npu_transpose.out(Tensor self, int[] perm, bool require_contiguous=True, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: all_version
    exposed: all_version

  - func: npu_transpose_trans_contiguous.out(Tensor self, int[] perm, bool require_contiguous=True, *, Tensor(a!) out) -> Tensor(a!)
    acl_op: v1.11, v2.0, v2.1

  - func: npu_view_copy(Tensor(a!) self, Tensor other, bool non_blocking) -> Tensor(a!)
    acl_op: all_version

  - func: npu_weight_quant_batchmatmul(Tensor x, Tensor weight, Tensor antiquant_scale, Tensor? antiquant_offset=None, Tensor? quant_scale=None, Tensor? quant_offset=None, Tensor? bias=None, int antiquant_group_size=0, int inner_precise=0) -> Tensor
    op_api: all_version
    exposed: all_version
    internal_format_opapi: all_version

  - func: npu_transpose_batchmatmul(Tensor input, Tensor weight, *, Tensor? bias=None, Tensor? scale=None, int[]? perm_x1=None, int[]? perm_x2=None, int[]? perm_y=None, int? batch_split_factor=1) -> Tensor
    op_api: all_version
    exposed: all_version

  - func: npu_yolo_boxes_encode(Tensor self, Tensor gt_bboxes, Tensor stride, bool performance_mode=False) -> Tensor
    acl_op: all_version
    exposed: all_version

  - func: one_(Tensor(a!) self) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version

  - func: repeat_interleave_backward_int(Tensor grad, Tensor self, SymInt repeats, int? dim=None) -> Tensor
    op_api: [v2.0, newest]

  - func: repeat_interleave_backward_int(Tensor grad, Tensor self, int repeats, int? dim=None) -> Tensor
    op_api: v1.11

  - func: repeat_interleave_backward_tensor(Tensor grad, Tensor self, Tensor repeats, int? dim=None) -> Tensor
    op_api: all_version

  - func: reverse(Tensor self, int[] axis) -> Tensor
    acl_op: v1.11, v2.0, v2.1

  - func: scatter_update(Tensor self, Tensor indices, Tensor updates, int axis) -> Tensor
    acl_op: all_version
    op_api: all_version
    exposed: all_version

  - func: scatter_update_(Tensor(a!) self, Tensor indices, Tensor updates, int axis) -> Tensor(a!)
    acl_op: all_version
    op_api: all_version
    exposed: all_version

  - func: slow_conv_transpose2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
    acl_op: all_version
    op_api: all_version

  - func: stft_backward(Tensor grad_output, Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor
    op_api: [v2.1, newest]

  - func: fft_r2c_backward(Tensor grad, int[] dim, int normalization, bool onesided, int last_dim_size) -> Tensor
    op_api: [v2.1, newest]

  - func: fft_c2r_backward(Tensor grad, int[] dim, int normalization) -> Tensor
    op_api: [v2.1, newest]

  - func: npu_cross_entropy_loss(Tensor input, Tensor target, Tensor? weight=None, str reduction='mean', int ignore_index=-100, float label_smoothing=0.0, float lse_square_scale_for_zloss=0.0, bool return_zloss=False) -> (Tensor, Tensor, Tensor, Tensor)
    op_api: all_version
    gen_opapi:
      loss:
        size: 'npu_cross_entropy_loss_loss_output_size(input, reduction)'
        dtype: input
      log_prob:
        size: input
        dtype: input
      zloss:
        size: 'npu_cross_entropy_loss_zloss_output_size(input, reduction, return_zloss)'
        dtype: input
      lse_for_zloss:
        size: 'npu_cross_entropy_loss_lse_for_zloss_output_size(input, lse_square_scale_for_zloss)'
        dtype: input
      new_params:
        reduction_char: 'const_cast<char *>(reduction.data())'
      exec: aclnnCrossEntropyLoss, input, target, weight, reduction_char, ignore_index, label_smoothing, lse_square_scale_for_zloss, return_zloss, loss, log_prob, zloss, lse_for_zloss
    exposed: all_version

  - func: npu_cross_entropy_loss_backward(Tensor grad_loss, Tensor log_prob, Tensor target, Tensor? weight=None, Tensor? grad_zloss=None, Tensor? lse_for_zloss=None, str reduction='mean', int ignore_index=-100, float label_smoothing=0.0, float lse_square_scale_for_zloss=0.0) -> Tensor
    op_api: all_version
    gen_opapi:
      x_grad_out:
        size: log_prob
        dtype: grad_loss
      new_params:
        reduction_char: 'const_cast<char *>(reduction.data())'
      exec: aclnnCrossEntropyLossGrad, grad_loss, log_prob, target, weight, grad_zloss, lse_for_zloss, reduction_char, ignore_index, label_smoothing, lse_square_scale_for_zloss, x_grad_out

  - func: npu_group_norm_swish(Tensor input, int num_groups, Tensor weight, Tensor bias, float? eps=1e-5, float? swish_scale=1.0) -> (Tensor, Tensor, Tensor)
    op_api: all_version
    exposed: all_version
    gen_opapi:
      out:
        size: input
        dtype: input
      mean:
        size: '{input.size(0), num_groups}'
        dtype: weight
      rstd:
        size: '{input.size(0), num_groups}'
        dtype: weight
      new_params:
        data_format: '"NCHW"'
        eps_value: 'eps.value_or(1e-5)'
        activate_swish: 'true'
        swish_scale_value: 'swish_scale.value_or(1.0)'
      exec: aclnnGroupNormSwish, input, weight, bias, num_groups, data_format, eps_value, activate_swish, swish_scale_value, out, mean, rstd

  - func: npu_group_norm_swish_grad(Tensor grad, Tensor input, int num_groups, Tensor weight, Tensor bias, Tensor mean, Tensor rstd, bool[3] grad_input_mask, float? swish_scale=1.0) -> (Tensor, Tensor, Tensor)
    op_api: all_version
    gen_opapi:
      grad_x:
        size: input
        dtype: input
      grad_weight:
        size: weight
        dtype: weight
      grad_bias:
        size: bias
        dtype: bias
      new_params:
        data_format: '"NCHW"'
        swish_scale_value: 'swish_scale.value_or(1.0)'
        dweight_require_grad: 'grad_input_mask[1] ? true : false'
        dbias_require_grad: 'grad_input_mask[2] ? true : false'
      exec: aclnnGroupNormSwishGrad, grad, mean, rstd, input, weight, bias, num_groups, data_format, swish_scale_value, dweight_require_grad, dbias_require_grad, grad_x, grad_weight, grad_bias

  - func: npu_advance_step_flashattn(Tensor(a!) input_tokens, Tensor sampled_token_ids, Tensor(b!) input_positions, Tensor(c!) seq_lens, Tensor(d!) slot_mapping, Tensor block_tables, int num_seqs, int num_queries, int block_size) -> ()
    op_api: all_version
    exposed: all_version
    gen_opapi:
      exec: aclnnAdvanceStep

  - func: npu_grouped_matmul_add_(Tensor(a!) self, Tensor x, Tensor weight, Tensor group_list, *, bool transpose_x=True, bool transpose_weight=False, int group_type=2) -> Tensor(a!)
    op_api: all_version
    gen_opapi:
      exec: aclnnGroupedMatmulAdd, x, weight, group_list, self, transpose_x, transpose_weight, group_type

  - func: npu_attn_softmax_(Tensor(a!) self) -> Tensor(a!)
    op_api: all_version
    gen_opapi:
      new_params:
        dim: '(int64_t)-1'
      exec: aclnnSoftmax, self, dim, self

  - func: npu_attn_softmax_backward_(Tensor(a!) self, Tensor grad_output, Tensor values) -> Tensor(a!)
    op_api: all_version
  
  - func: npu_gather_sparse_index(Tensor input, Tensor index) -> Tensor
    op_api: all_version
    exposed: all_version
    gen_opapi:
      out:
        size: 'npu_gather_sparse_index_out_size(input, index)'
        dtype: input
      new_params:
        dim: '(int64_t)0'
        batch_dims: '(int64_t)0'
        mode: '(int64_t)1'
      exec: aclnnGatherV3, input, dim, index, batch_dims, mode, out

  - func: npu_nsa_compress(Tensor input, Tensor weight, int compress_block_size, int compress_stride, *, int[]? actual_seq_len=None) -> Tensor
    op_api: all_version
    gen_opapi:
      out:
        size: npu_nsa_compress_out_size(input, actual_seq_len_type, actual_seq_len, compress_block_size, compress_stride)
        dtype: input
      new_params:
        layout: '"TND"'
        actual_seq_len_type: '0L'
        actual_seq_len_value: 'actual_seq_len.value_or(at::IntArrayRef{})'
      exec: aclnnNsaCompress, input, weight, actual_seq_len_value, layout, compress_block_size, compress_stride, actual_seq_len_type, out

  - func: npu_nsa_compress_grad(Tensor grad, Tensor input, Tensor weight, int compress_block_size, int compress_stride, *, int[]? actual_seq_len=None) -> (Tensor, Tensor)
    op_api: all_version
    gen_opapi:
      input_grad:
        size: input
        dtype: input
      weight_grad:
        size: weight
        dtype: weight
      new_params:
        layout: '"TND"'
        actual_seq_len_value: 'actual_seq_len.value_or(at::IntArrayRef{})'
        actual_seq_len_type: '0L'
      exec: aclnnNsaCompressGrad, grad, input, weight, actual_seq_len_value, compress_block_size, compress_stride, actual_seq_len_type, layout, input_grad, weight_grad

  - func: npu_nsa_compress_infer.cache(Tensor input, Tensor weight, Tensor slot_mapping, int compress_block_size, int compress_stride, int page_block_size, *, Tensor? block_table=None, int[]? actual_seq_len=None, Tensor(a!) cache) -> Tensor(a!)
    op_api: all_version
    gen_opapi:
      new_params:
        layout: '"TND"'
        actual_seq_len_value: 'actual_seq_len.value_or(at::IntArrayRef{})'
        actual_seq_len_type: '1L'
      exec: aclnnNsaCompressWithCache, input, weight, slot_mapping, actual_seq_len_value, block_table, layout, compress_block_size, compress_stride, actual_seq_len_type, page_block_size, cache

  - func: npu_nsa_compress_attention(Tensor query, Tensor key, Tensor value, float scale_value, int head_num, int compress_block_size, int compress_stride, int select_block_size, int select_block_count, *, Tensor? topk_mask=None, Tensor? atten_mask=None, int[]? actual_seq_qlen=None, int[]? actual_cmp_seq_kvlen=None, int[]? actual_sel_seq_kvlen=None) -> (Tensor, Tensor, Tensor, Tensor)
    op_api: all_version
    gen_opapi:
      attention_out:
        size: '{query.size(0), query.size(1), value.size(2)}'
        dtype: query
      topk_indices_out:
        size: '{query.size(0), key.size(1), select_block_count}'
        dtype: at::kInt
      softmax_max_out:
        size: '{query.size(0), query.size(1), 8L}'
        dtype: at::kFloat
      softmax_sum_out:
        size: '{query.size(0), query.size(1), 8L}'
        dtype: at::kFloat
      new_params:
        layout: '"TND"'
        actual_seq_qlen_value: 'actual_seq_qlen.value_or(at::IntArrayRef{})'
        actual_cmp_seq_kvlen_value: 'actual_cmp_seq_kvlen.value_or(at::IntArrayRef{})'
        actual_sel_seq_kvlen_value: 'actual_sel_seq_kvlen.value_or(at::IntArrayRef{})'
        sparse_mode: '1L'
      exec: aclnnNsaCompressAttentionVarLenScore, query, key, value, atten_mask, topk_mask, actual_seq_qlen_value, actual_cmp_seq_kvlen_value, actual_sel_seq_kvlen_value, scale_value, head_num, layout, sparse_mode, compress_block_size, compress_stride, select_block_size, select_block_count, softmax_max_out, softmax_sum_out, attention_out, topk_indices_out

  - func: npu_nsa_compress_attention_infer(Tensor query, Tensor key, Tensor value, float scale_value, int head_num, int key_value_head_num, int select_block_size, int select_block_count, int page_block_size, int compress_block_size, int compress_stride, *, Tensor? atten_mask=None, Tensor? block_table=None, Tensor? topk_mask=None, int[]? actual_seq_qlen=None, int[]? actual_cmp_seq_kvlen=None, int[]? actual_sel_seq_kvlen=None) -> (Tensor, Tensor)
    op_api: all_version
    gen_opapi:
      out:
        size: '{query.size(0), query.size(1), value.size(2) / key_value_head_num}'
        dtype: query
      topk_indices_out:
        size: '{query.size(0), key_value_head_num, select_block_count}'
        dtype: at::kInt
      new_params:
        layout: '"TND"'
        actual_seq_qlen_value: 'actual_seq_qlen.value_or(at::IntArrayRef{})'
        actual_cmp_seq_kvlen_value: 'actual_cmp_seq_kvlen.value_or(at::IntArrayRef{})'
        actual_sel_seq_kvlen_value: 'actual_sel_seq_kvlen.value_or(at::IntArrayRef{})'
        sparse_mode: '0L'
      exec: aclnnNsaCompressAttentionInfer, query, key, value, atten_mask, block_table, actual_seq_qlen_value, actual_cmp_seq_kvlen_value, actual_sel_seq_kvlen_value, topk_mask, head_num, key_value_head_num, select_block_size, select_block_count, compress_block_size, compress_stride, scale_value, layout, page_block_size, sparse_mode, out, topk_indices_out

  - func: npu_nsa_select_attention(Tensor query, Tensor key, Tensor value, Tensor topk_indices, float scale_value, int head_num, int select_block_size, int select_block_count, *, Tensor? atten_mask=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None) -> (Tensor, Tensor, Tensor)
    op_api: all_version
    gen_opapi:
      attention_out:
        size: '{query.size(0), query.size(1), value.size(2)}'
        dtype: query
      softmax_max_out:
        size: '{query.size(0), query.size(1), 8L}'
        dtype: at::kFloat
      softmax_sum_out:
        size: '{query.size(0), query.size(1), 8L}'
        dtype: at::kFloat
      new_params:
        layout: '"TND"'
        actual_seq_qlen_value: 'actual_seq_qlen.value_or(at::IntArrayRef{})'
        actual_seq_kvlen_value: 'actual_seq_kvlen.value_or(at::IntArrayRef{})'
        sparse_mode: '2L'
      exec: aclnnNsaSelectedAttention, query, key, value, topk_indices, atten_mask, actual_seq_qlen_value, actual_seq_kvlen_value, scale_value, head_num, layout, sparse_mode, select_block_size, select_block_count, softmax_max_out, softmax_sum_out, attention_out

  - func: npu_nsa_select_attention_grad(Tensor grad, Tensor query, Tensor key, Tensor value, Tensor attention_out, Tensor softmax_max, Tensor softmax_sum, Tensor topk_indices, float scale_value, int head_num, int select_block_size, int select_block_count, *, Tensor? atten_mask=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None) -> (Tensor, Tensor, Tensor)
    op_api: all_version
    gen_opapi:
      query_grad:
        size: query
        dtype: query
      key_grad:
        size: key
        dtype: key
      value_grad:
        size: value
        dtype: value
      new_params:
        layout: '"TND"'
        actual_seq_qlen_value: 'actual_seq_qlen.value_or(at::IntArrayRef{})'
        actual_seq_kvlen_value: 'actual_seq_kvlen.value_or(at::IntArrayRef{})'
        sparse_mode: '2L'
      exec: aclnnNsaSelectedAttentionGrad, query, key, value, attention_out, grad, softmax_max, softmax_sum, topk_indices, actual_seq_qlen_value, actual_seq_kvlen_value, atten_mask, scale_value, select_block_size, select_block_count, head_num, layout, sparse_mode, query_grad, key_grad, value_grad

  - func: npu_nsa_select_attention_infer(Tensor query, Tensor key, Tensor value, Tensor topk_indices, float scale_value, int head_num, int key_value_head_num, int select_block_size, int select_block_count, int page_block_size, *, str layout='BSND', Tensor? atten_mask=None, Tensor? block_table=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None) -> Tensor
    op_api: all_version
    gen_opapi:
      out:
        size: '{query.size(0), query.size(1), query.size(2), value.size(3)}'
        dtype: query
      new_params:
        layout_ptr: 'const_cast<char *>(layout.data())'
        sparse_mode: '0L'
        actual_seq_qlen_value: 'actual_seq_qlen.value_or(at::IntArrayRef{})'
        actual_seq_kvlen_value: 'actual_seq_kvlen.value_or(at::IntArrayRef{})'
      exec: aclnnNsaSelectAttentionInfer, query, key, value, topk_indices, atten_mask, block_table, actual_seq_qlen_value, actual_seq_kvlen_value, layout_ptr, head_num, key_value_head_num, select_block_size, select_block_count, page_block_size, scale_value, sparse_mode, out

symint:
  - func: npu_gather_backward(Tensor grad, SymInt[] self_size, int dim, Tensor index, bool sparse_grad) -> Tensor
    op_api: all_version

  - func: embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor
    acl_op: [v2.0, newest]
    op_api: [v2.1, newest]

  - func: npu_bmm_v2_mat1_backward(Tensor grad, Tensor mat1, Tensor mat2, SymInt[] size) -> Tensor
    acl_op: [v2.0, newest]

  - func: npu_bmm_v2_mat2_backward(Tensor grad, Tensor mat1, Tensor mat2, SymInt[] size) -> Tensor
    acl_op: [v2.0, newest]

  - func: npu_confusion_transpose_backward(Tensor grad, int[] perm, SymInt[] shape, bool transpose_first) -> Tensor
    acl_op: [v2.0, newest]

  - func: npu_fused_infer_attention_score(Tensor query, Tensor key, Tensor value, *, Tensor? pse_shift=None, Tensor? atten_mask=None, SymInt[]? actual_seq_lengths=None, SymInt[]? actual_seq_lengths_kv=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? key_antiquant_scale=None, Tensor? key_antiquant_offset=None, Tensor? value_antiquant_scale=None, Tensor? value_antiquant_offset=None, Tensor? block_table=None, Tensor? query_padding_size=None, Tensor? kv_padding_size=None, Tensor? key_shared_prefix=None, Tensor? value_shared_prefix=None, SymInt[]? actual_shared_prefix_len=None, Tensor? query_rope=None, Tensor? key_rope=None, Tensor? key_rope_antiquant_scale=None, int num_heads=1, float scale=1.0, int pre_tokens=2147483647, int next_tokens=2147483647, str input_layout="BSH", int num_key_value_heads=0, int sparse_mode=0, int inner_precise=0, int block_size=0, int antiquant_mode=0, int key_antiquant_mode=0, int value_antiquant_mode=0, bool softmax_lse_flag=False) -> (Tensor, Tensor)
    op_api: [v2.1, newest]
    internal_format_opapi: [v2.1, newest]

  - func: npu_fused_infer_attention_score.out(Tensor query, Tensor key, Tensor value, *, Tensor? pse_shift=None, Tensor? atten_mask=None, SymInt[]? actual_seq_lengths=None, SymInt[]? actual_seq_lengths_kv=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? key_antiquant_scale=None, Tensor? key_antiquant_offset=None, Tensor? value_antiquant_scale=None, Tensor? value_antiquant_offset=None, Tensor? block_table=None, Tensor? query_padding_size=None, Tensor? kv_padding_size=None, Tensor? key_shared_prefix=None, Tensor? value_shared_prefix=None, SymInt[]? actual_shared_prefix_len=None, Tensor? query_rope=None, Tensor? key_rope=None, Tensor? key_rope_antiquant_scale=None, int num_heads=1, float scale=1.0, int pre_tokens=2147483647, int next_tokens=2147483647, str input_layout="BSH", int num_key_value_heads=0, int sparse_mode=0, int inner_precise=0, int block_size=0, int antiquant_mode=0, int key_antiquant_mode=0, int value_antiquant_mode=0, bool softmax_lse_flag=False, Tensor? workspace=None, Tensor(a!) attention_out, Tensor(b!) softmax_lse) -> (Tensor(a!), Tensor(b!))
    op_api: [v2.1, newest]
    internal_format_opapi: [v2.1, newest]

  - func: _npu_fused_infer_attention_score_get_max_workspace(Tensor query, Tensor key, Tensor value, *, Tensor? pse_shift=None, Tensor? atten_mask=None, SymInt[]? actual_seq_lengths=None, SymInt[]? actual_seq_lengths_kv=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? key_antiquant_scale=None, Tensor? key_antiquant_offset=None, Tensor? value_antiquant_scale=None, Tensor? value_antiquant_offset=None, Tensor? block_table=None, Tensor? query_padding_size=None, Tensor? kv_padding_size=None, Tensor? key_shared_prefix=None, Tensor? value_shared_prefix=None, SymInt[]? actual_shared_prefix_len=None, Tensor? query_rope=None, Tensor? key_rope=None, Tensor? key_rope_antiquant_scale=None, int num_heads=1, float scale=1.0, int pre_tokens=2147483647, int next_tokens=2147483647, str input_layout="BSH", int num_key_value_heads=0, int sparse_mode=0, int inner_precise=0, int block_size=0, int antiquant_mode=0, int key_antiquant_mode=0, int value_antiquant_mode=0, bool softmax_lse_flag=False) -> Tensor
    op_api: [v2.1, newest]

  - func: npu_incre_flash_attention(Tensor query, Tensor key, Tensor value, *, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? pse_shift=None, SymInt[]? actual_seq_lengths=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? block_table=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? kv_padding_size=None, int num_heads=1, float scale_value=1.0, str input_layout="BSH", int num_key_value_heads=0, int block_size=0, int inner_precise=1) -> Tensor
    op_api: [v2.1, newest]

  - func: npu_max_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor
    acl_op: [v2.0, newest]

  - func: npu_min_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor
    acl_op: [v2.0, newest]

  - func: npu_ps_roi_pooling_backward(Tensor output_grad, Tensor rois, float spatial_scale, int group_size, int output_dim, SymInt[] input_size) -> Tensor
    acl_op: [v2.0, newest]

  - func: repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, SymInt? output_size=None) -> Tensor
    acl_op: [v2.2, newest]
    op_api: [v2.2, newest]

  - func: repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, SymInt? output_size=None) -> Tensor
    acl_op: [v2.2, newest]
    op_api: [v2.2, newest]

  - func: repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, int? output_size=None) -> Tensor
    acl_op: v2.1
    op_api: v2.1

  - func: repeat_interleave_backward_int(Tensor grad, Tensor self, SymInt repeats, int? dim=None) -> Tensor
    op_api: [v2.0, newest]

  - func: zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    acl_op: [v2.1, newest]
tocpu:
  - func: _cholesky_helper
    version: v1.11, v2.0

  - func: _cholesky_solve_helper
    version: v1.11, v2.0

  - func: _compute_linear_combination
    version: v1.11, v2.0

  - func: _compute_linear_combination.out
    version: v1.11, v2.0

  - func: _dirichlet_grad
    version: v1.11, v2.0

  - func: _efficientzerotensor
    version: v1.11

  - func: _empty_affine_quantized
    version: v1.11, v2.0

  - func: _empty_per_channel_affine_quantized
    version: v1.11, v2.0

  - func: _fft_c2c
    version: v1.11, v2.0

  - func: _fft_c2c.out
    version: v1.11, v2.0

  - func: _fft_c2r
    version: v1.11, v2.0

  - func: _fft_c2r.out
    version: v1.11, v2.0

  - func: _fft_r2c
    version: v1.11, v2.0

  - func: _fft_r2c.out
    version: v1.11, v2.0

  - func: _foreach_lgamma
    version: v1.11, v2.0

  - func: _foreach_lgamma_
    version: v1.11, v2.0

  - func: _linalg_inv_out_helper_
    version: v1.11

  - func: _linalg_qr_helper
    version: v1.11

  - func: _logcumsumexp
    version: v1.11, v2.0

  - func: _logcumsumexp.out
    version: v1.11, v2.0

  - func: _lu_with_info
    version: v1.11, v2.0

  - func: _pdist_backward
    version: v1.11, v2.0

  - func: _sample_dirichlet
    version: v1.11, v2.0

  - func: _solve_helper
    version: v1.11

  - func: _standard_gamma
    version: v1.11, v2.0

  - func: _standard_gamma_grad
    version: v1.11, v2.0

  - func: _test_optional_filled_intlist
    version: v1.11, v2.0

  - func: _test_optional_floatlist
    version: v1.11, v2.0

  - func: _test_optional_intlist
    version: v1.11, v2.0

  - func: _unique
    version: v2.0

  - func: adaptive_max_pool3d
    version: v1.11, v2.0

  - func: adaptive_max_pool3d.out
    version: v1.11, v2.0

  - func: adaptive_max_pool3d_backward
    version: v1.11, v2.0

  - func: adaptive_max_pool3d_backward.grad_input
    version: v1.11, v2.0

  - func: batch_norm_update_stats
    version: v1.11, v2.0

  - func: binomial
    version: v1.11, v2.0

  - func: cauchy_
    version: v1.11, v2.0

  - func: cholesky_inverse
    version: v1.11, v2.0

  - func: cholesky_inverse.out
    version: v1.11, v2.0

  - func: copysign.Scalar
    version: v1.11, v2.0

  - func: copysign.Tensor
    version: v1.11, v2.0

  - func: copysign.out
    version: v1.11, v2.0

  - func: copysign_.Scalar
    version: v1.11, v2.0

  - func: copysign_.Tensor
    version: v1.11, v2.0

  - func: digamma
    version: v1.11, v2.0

  - func: digamma.out
    version: v1.11, v2.0

  - func: digamma_
    version: v1.11, v2.0

  - func: fft_fftshift
    version: v1.11, v2.0

  - func: fft_ifftshift
    version: v1.11, v2.0

  - func: fmax
    version: v1.11, v2.0

  - func: fmax.out
    version: v1.11, v2.0

  - func: fmin
    version: v1.11, v2.0

  - func: fmin.out
    version: v1.11, v2.0

  - func: fractional_max_pool2d
    version: v1.11, v2.0

  - func: fractional_max_pool2d.output
    version: v1.11, v2.0

  - func: fractional_max_pool2d_backward
    version: v1.11, v2.0

  - func: fractional_max_pool2d_backward.grad_input
    version: v1.11, v2.0

  - func: fractional_max_pool3d
    version: v1.11, v2.0

  - func: fractional_max_pool3d.output
    version: v1.11, v2.0

  - func: fractional_max_pool3d_backward
    version: v1.11, v2.0

  - func: fractional_max_pool3d_backward.grad_input
    version: v1.11, v2.0

  - func: geometric_
    version: v1.11, v2.0

  - func: heaviside.out
    version: v1.11, v2.0

  - func: hypot
    version: v1.11, v2.0

  - func: hypot.out
    version: v1.11, v2.0

  - func: i0.out
    version: v1.11, v2.0

  - func: igamma
    version: v1.11, v2.0

  - func: igamma.out
    version: v1.11, v2.0

  - func: igamma_
    version: v1.11, v2.0

  - func: igammac
    version: v1.11, v2.0

  - func: igammac.out
    version: v1.11, v2.0

  - func: igammac_
    version: v1.11, v2.0

  - func: lcm.out
    version: v1.11, v2.0

  - func: lgamma
    version: v1.11, v2.0

  - func: lgamma.out
    version: v1.11, v2.0

  - func: lgamma_
    version: v1.11, v2.0

  - func: linalg_slogdet
    version: v2.0

  - func: linalg_slogdet.out
    version: v2.0

  - func: linalg_vector_norm
    version: v2.0

  - func: linalg_vector_norm.out
    version: v2.0

  - func: log_normal_
    version: v1.11, v2.0

  - func: logit
    version: v1.11, v2.0

  - func: logit.out
    version: v1.11, v2.0

  - func: logit_
    version: v1.11, v2.0

  - func: logit_backward
    version: v1.11, v2.0

  - func: logit_backward.grad_input
    version: v1.11, v2.0

  - func: matrix_exp
    version: v1.11, v2.0

  - func: mode
    version: v1.11, v2.0

  - func: multi_margin_loss
    version: v1.11, v2.0

  - func: multi_margin_loss.out
    version: v1.11, v2.0

  - func: multi_margin_loss_backward
    version: v1.11, v2.0

  - func: multi_margin_loss_backward.grad_input
    version: v1.11, v2.0

  - func: multilabel_margin_loss_backward
    version: v1.11, v2.0

  - func: multilabel_margin_loss_backward.grad_input
    version: v1.11, v2.0

  - func: nanmedian
    version: v2.0

  - func: nanmedian.dim_values
    version: v1.11, v2.0

  - func: narrow_copy.out
    version: v1.11, v2.0

  - func: nextafter
    version: v1.11, v2.0

  - func: nextafter.out
    version: v1.11, v2.0

  - func: orgqr
    version: v1.11, v2.0

  - func: orgqr.out
    version: v1.11, v2.0

  - func: poisson
    version: v1.11, v2.0

  - func: polygamma.out
    version: v1.11, v2.0

  - func: real
    version: v1.11, v2.0

  - func: repeat_interleave.Tensor
    version: v1.11, v2.0

  - func: scatter_.reduce
    version: v1.11, v2.0

  - func: scatter_.value_reduce
    version: v1.11, v2.0

  - func: slow_conv_dilated3d
    version: v1.11, v2.0

  - func: slow_conv_transpose3d
    version: v1.11, v2.0

  - func: slow_conv_transpose3d.out
    version: v1.11, v2.0

  - func: sspaddmm.out
    version: v1.11, v2.0

  - func: tensordot.out
    version: v1.11, v2.0

  - func: to_mkldnn
    version: v1.11, v2.0

  - func: to_sparse
    version: v1.11, v2.0

  - func: to_sparse.sparse_dim
    version: v1.11, v2.0

  - func: trace
    version: v2.0

  - func: unfold_backward
    version: v1.11, v2.0

  - func: unique_dim_consecutive
    version: v1.11, v2.0
unsupported:
  - func: _conj
    version: v2.0

  - func: _conj_physical
    version: v1.11, v2.0

  - func: _det_lu_based_helper
    version: v1.11, v2.0

  - func: bitwise_left_shift.Scalar_Tensor
    version: v1.11, v2.0

  - func: bitwise_left_shift.Tensor
    version: v1.11, v2.0

  - func: bitwise_left_shift.Tensor_Scalar
    version: v1.11, v2.0

  - func: bitwise_left_shift.Tensor_Scalar_out
    version: v1.11, v2.0

  - func: bitwise_left_shift.Tensor_out
    version: v1.11, v2.0

  - func: bitwise_left_shift_.Tensor
    version: v1.11, v2.0

  - func: bitwise_left_shift_.Tensor_Scalar
    version: v1.11, v2.0

  - func: bitwise_right_shift.Scalar_Tensor
    version: v1.11, v2.0

  - func: bitwise_right_shift.Tensor
    version: v1.11, v2.0

  - func: bitwise_right_shift.Tensor_Scalar
    version: v1.11, v2.0

  - func: bitwise_right_shift.Tensor_Scalar_out
    version: v1.11, v2.0

  - func: bitwise_right_shift.Tensor_out
    version: v1.11, v2.0

  - func: bitwise_right_shift_.Tensor
    version: v1.11, v2.0

  - func: bitwise_right_shift_.Tensor_Scalar
    version: v1.11, v2.0

  - func: cholesky
    version: v1.11, v2.0

  - func: cholesky.out
    version: v1.11, v2.0

  - func: conj
    version: v2.0

  - func: conj_physical
    version: v1.11, v2.0

  - func: conj_physical.out
    version: v1.11, v2.0

  - func: conj_physical_
    version: v1.11, v2.0

  - func: frexp.Tensor
    version: v1.11, v2.0

  - func: frexp.Tensor_out
    version: v1.11, v2.0

  - func: geqrf
    version: v1.11, v2.0

  - func: geqrf.a
    version: v1.11, v2.0

  - func: linalg_cholesky_ex
    version: v1.11, v2.0

  - func: linalg_cholesky_ex.L
    version: v1.11, v2.0

  - func: linalg_det
    version: v1.11, v2.0

  - func: linalg_det.out
    version: v1.11, v2.0

  - func: linalg_eig
    version: v1.11, v2.0

  - func: linalg_eig.out
    version: v1.11, v2.0

  - func: linalg_eigh
    version: v1.11, v2.0

  - func: linalg_eigh.eigvals
    version: v1.11, v2.0

  - func: linalg_eigvalsh
    version: v1.11, v2.0

  - func: linalg_eigvalsh.out
    version: v1.11, v2.0

  - func: linalg_lstsq
    version: v1.11, v2.0

  - func: linalg_lstsq.out
    version: v1.11, v2.0

  - func: linalg_lu_factor_ex
    version: v1.11, v2.0

  - func: linalg_lu_factor_ex.out
    version: v1.11, v2.0

  - func: logdet
    version: v2.0

  - func: lu_solve
    version: v1.11, v2.0

  - func: lu_solve.out
    version: v1.11, v2.0

  - func: lu_unpack
    version: v1.11, v2.0

  - func: lu_unpack.out
    version: v1.11, v2.0

  - func: special_entr
    version: v1.11, v2.0

  - func: special_entr.out
    version: v1.11, v2.0

  - func: special_erfcx
    version: v1.11, v2.0

  - func: special_erfcx.out
    version: v1.11, v2.0

  - func: special_zeta
    version: v1.11, v2.0

  - func: special_zeta.other_scalar
    version: v1.11, v2.0

  - func: special_zeta.other_scalar_out
    version: v1.11, v2.0

  - func: special_zeta.out
    version: v1.11, v2.0

  - func: special_zeta.self_scalar
    version: v1.11, v2.0

  - func: special_zeta.self_scalar_out
    version: v1.11, v2.0
quant:
  - func: q_scale(Tensor self) -> float
    acl_op: v2.2

  - func: q_per_channel_scales(Tensor self) -> Tensor
    acl_op: v2.2

  - func: q_zero_point(Tensor self) -> int
    acl_op: v2.2

  - func: q_per_channel_zero_points(Tensor self) -> Tensor
    acl_op: v2.2

  - func: q_per_channel_axis(Tensor self) -> int
    acl_op: v2.2

  - func: qscheme(Tensor self) -> QScheme
    acl_op: v2.2

  - func: dequantize.self(Tensor self) -> Tensor
    acl_op: [v2.2, newest]

  - func: int_repr(Tensor self) -> Tensor
    acl_op: [v2.2, newest]

  - func: _empty_affine_quantized(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor
    acl_op: [v2.2, newest]
