# Defines derivative formulas and Python signatures of methods on Variable
#
#If you need any guidance, please refer to the comments in derivatives.yaml in PyTorch.

- name: fast_gelu(Tensor self) -> Tensor
  self: npu_fast_gelu_backward(grad, self)

- name: npu_rotary_mul(Tensor self, Tensor r1, Tensor r2) -> Tensor
  self, r1, r2: npu_rotary_mul_backward(grad, self, r1, r2)

- name: npu_diou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
  self, gtboxes: npu_diou_backward(grad, self, gtboxes, trans, is_cross, mode)

- name: npu_giou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
  self, gtboxes: npu_giou_backward(grad, self, gtboxes, trans, is_cross, mode)

- name: npu_mish(Tensor self) -> Tensor
  self: npu_mish_backward(grad, self)

- name: npu_softmax_cross_entropy_with_logits(Tensor self, Tensor labels) -> Tensor
  self: npu_softmax_cross_entropy_with_logits_backward(grad, self, labels)

- name: npu_linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
  input, weight: npu_linear_backward(grad, input, weight)
  bias: maybe_multiply(grad, 1)

- name: npu_dropout_with_add_softmax(Tensor self, Tensor x1, Scalar alpha, float prob, int dim) -> (Tensor, Tensor, Tensor)
  output_differentiability: [False, False, True]
  self, x1: npu_dropout_with_add_softmax_backward(grad, result0, result1, alpha, prob, dim)

- name: npu_dtype_cast(Tensor self, ScalarType dtype) -> Tensor
  self: npu_dtype_cast_backward(grad, self.scalar_type())

- name: npu_convolution_transpose(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
  input, weight, bias: npu_convolution_transpose_backward(input, grad, weight, padding, output_padding, stride, dilation, groups, grad_input_mask)

- name: npu_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
  input, weight, bias: npu_convolution_backward(input, grad, weight, stride, padding, dilation, groups, grad_input_mask)

- name: npu_deformable_conv2d(Tensor input, Tensor weight, Tensor offset, Tensor? bias, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor)
  input, weight, offset, bias: npu_deformable_conv2dbk(input, grad, result1, weight, offset, kernel_size, stride, padding, dilation, groups, deformable_groups, modulated)

- name: npu_scaled_masked_softmax(Tensor x, Tensor mask, Scalar scale=1, bool fixed_triu_mask=False) -> Tensor
  x: npu_scaled_masked_softmax_backward(grad, result, mask, scale, fixed_triu_mask)

- name: binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor
  self: npu_binary_cross_entropy_with_logits_backward(grad, self, target, weight, pos_weight, reduction)
