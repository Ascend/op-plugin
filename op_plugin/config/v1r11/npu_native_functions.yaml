backend: NPU
cpp_namespace: at_npu::native
supported:
 - func: __ilshift__.Scalar
   wrap_impl: __ilshift__
 - func: __ilshift__.Tensor
   wrap_impl: __ilshift__
 - func: __ior__.Scalar
   wrap_impl: __ior__
   op_api: False
 - func: __ior__.Tensor
   wrap_impl: __ior__
   op_api: False
 - func: __irshift__.Scalar
   wrap_impl: __irshift__
 - func: __irshift__.Tensor
   wrap_impl: __irshift__
 - func: __lshift__.Scalar
   wrap_impl: __lshift__
 - func: __lshift__.Tensor
   wrap_impl: __lshift__
 - func: __rshift__.Scalar
   wrap_impl: __rshift__
 - func: __rshift__.Tensor
   wrap_impl: __rshift__
 - func: __xor__.Scalar
   wrap_impl: __xor__
 - func: __xor__.Tensor
   wrap_impl: __xor__
 - func: _adaptive_avg_pool2d
   wrap_impl: _adaptive_avg_pool2d
   op_api: True
 - func: _adaptive_avg_pool2d_backward
   wrap_impl: _adaptive_avg_pool2d_backward
   op_api: True
 - func: argmin.out
   wrap_impl: argmin_out
   op_api: True
 - func: _adaptive_avg_pool3d
   wrap_impl: _adaptive_avg_pool3d
 - func: _adaptive_avg_pool3d_backward
   wrap_impl: _adaptive_avg_pool3d_backward
 - func: _add_relu.Tensor
   wrap_impl: _add_relu
 - func: _add_relu.out
   wrap_impl: _add_relu_out
 - func: _add_relu_.Tensor
   wrap_impl: _add_relu_
 - func: _aminmax
   wrap_impl: _aminmax
 - func: _aminmax.dim
   wrap_impl: _aminmax
 - func: aminmax
   wrap_impl: aminmax
   op_api: False
 - func: aminmax.out
   wrap_impl: aminmax_out
   op_api: False
 - _amp_foreach_non_finite_check_
 - func: _amp_foreach_non_finite_check_and_unscale_
   wrap_impl: _amp_foreach_non_finite_check_and_unscale_
   op_api: True
 - func: _batch_norm_impl_index
   wrap_impl: _batch_norm_impl_index
 - func: _batch_norm_impl_index_backward
   wrap_impl: _batch_norm_impl_index_backward
 - func: _cat
   wrap_impl: _cat
   op_api: True
 - func: _cat.out
   wrap_impl: _cat_out
   op_api: True
 - func: _cdist_backward
   wrap_impl: _cdist_backward
 - func: _cdist_forward
   wrap_impl: _cdist_forward
 - func: _conv_depthwise2d
   wrap_impl: _conv_depthwise2d
 - func: _conv_depthwise2d.out
   wrap_impl: _conv_depthwise2d_out
 - func: convolution
   wrap_impl: convolution
   op_api: True
 - func: _convolution
   wrap_impl: _convolution
   op_api: False
 - func: _ctc_loss
   wrap_impl: _ctc_loss
   op_api: True
 - func: _ctc_loss_backward
   wrap_impl: _ctc_loss_backward
   op_api: True
 - func: _cummax_helper
   wrap_impl: _cummax_helper
   op_api: False
 - func: _cummin_helper
   wrap_impl: _cummin_helper
   op_api: False
 - func: _dim_arange
   wrap_impl: _dim_arange
 - func: _embedding_bag
   wrap_impl: _embedding_bag
 - func: _embedding_bag_backward
   wrap_impl: _embedding_bag_backward
 - func: _embedding_bag_forward_only
   wrap_impl: _embedding_bag_forward_only
 - func: _index_copy_
   wrap_impl: _index_copy_
   op_api: False
 - func: _index_put_impl_
   wrap_impl: _index_put_impl_
   op_api: True
 - func: _linalg_svd.U
   wrap_impl: _linalg_svd
 - _local_scalar_dense
 - func: _log_softmax
   wrap_impl: _log_softmax
   op_api: True
 - func: _log_softmax.out
   wrap_impl: _log_softmax_out
   op_api: True
 - func: _log_softmax_backward_data
   wrap_impl: _log_softmax_backward_data
   op_api: True
 - func: _log_softmax_backward_data.out
   wrap_impl: _log_softmax_backward_data_out
   op_api: True
 - func: _nnpack_spatial_convolution
   wrap_impl: _nnpack_spatial_convolution
 - func: _pack_padded_sequence
   wrap_impl: _pack_padded_sequence
 - func: _pad_packed_sequence
   wrap_impl: _pad_packed_sequence
 - func: _pdist_forward
   wrap_impl: _pdist_forward
 - _pin_memory
 - func: _s_where
   wrap_impl: _s_where
   op_api: True
 - func: _slow_conv2d_backward.output_mask
   wrap_impl: _slow_conv2d_backward_out
 - func: _slow_conv2d_forward
   wrap_impl: _slow_conv2d_forward
 - func: _slow_conv2d_forward.output
   wrap_impl: _slow_conv2d_forward_out
 - func: _softmax
   wrap_impl: _softmax
   op_api: True
 - func: _softmax.out
   wrap_impl: _softmax_out
   op_api: True
 - func: _softmax_backward_data
   wrap_impl: _softmax_backward_data
   op_api: True
 - func: _softmax_backward_data.out
   wrap_impl: _softmax_backward_data_out
   op_api: True
 - func: _symeig_helper
   wrap_impl: _symeig_helper
 - func: _unique2
   wrap_impl: _unique2
   op_api: True
 - func: _unique
   wrap_impl: _unique
   op_api: False
 - func: dequantize.self
   wrap_impl: dequantize
   op_api: False
 - _reshape_alias
 - func: abs
   wrap_impl: abs
   op_api: True
 - func: abs.out
   wrap_impl: abs_out
   op_api: True
 - func: abs_
   wrap_impl: abs_
   op_api: True
 - func: acos
   wrap_impl: acos
   op_api: True
 - func: acos.out
   wrap_impl: acos_out
   op_api: True
 - func: acos_
   wrap_impl: acos_
   op_api: True
 - func: acosh
   wrap_impl: acosh
   op_api: True
 - func: acosh.out
   wrap_impl: acosh_out
   op_api: True
 - func: acosh_
   wrap_impl: acosh_
   op_api: True
 - func: adaptive_avg_pool1d
   wrap_impl: adaptive_avg_pool1d
 - func: adaptive_avg_pool2d
   wrap_impl: adaptive_avg_pool2d
   op_api: True
 - func: adaptive_avg_pool2d.out
   wrap_impl: adaptive_avg_pool2d_out
   op_api: True
 - func: adaptive_avg_pool3d
   wrap_impl: adaptive_avg_pool3d
 - func: adaptive_avg_pool3d.out
   wrap_impl: adaptive_avg_pool3d_out
 - func: adaptive_avg_pool3d_backward.grad_input
   wrap_impl: adaptive_avg_pool3d_backward_out
 - func: adaptive_max_pool2d
   wrap_impl: adaptive_max_pool2d
   op_api: True
 - func: adaptive_max_pool2d.out
   wrap_impl: adaptive_max_pool2d_out
   op_api: True
 - func: adaptive_max_pool2d_backward
   wrap_impl: adaptive_max_pool2d_backward
 - func: adaptive_max_pool2d_backward.grad_input
   wrap_impl: adaptive_max_pool2d_backward_out
 - func: add.Scalar
   wrap_impl: add
   op_api: True
 - func: add.Tensor
   wrap_impl: add
   op_api: True
 - func: add.out
   wrap_impl: add_out
   op_api: True
 - func: add_.Scalar
   wrap_impl: add_
   op_api: True
 - func: add_.Tensor
   wrap_impl: add_
   op_api: True
 - func: addbmm
   wrap_impl: addbmm
   op_api: False
 - func: addbmm.out
   wrap_impl: addbmm_out
   op_api: False
 - func: addbmm_
   wrap_impl: addbmm_
   op_api: False
 - func: addcdiv
   wrap_impl: addcdiv
   op_api: True
 - func: addcdiv.out
   wrap_impl: addcdiv_out
   op_api: True
 - func: addcdiv_
   wrap_impl: addcdiv_
   op_api: True
 - func: addcmul
   wrap_impl: addcmul
   op_api: True
 - func: addcmul.out
   wrap_impl: addcmul_out
   op_api: True
 - func: addcmul_
   wrap_impl: addcmul_
   op_api: True
 - func: addmm
   wrap_impl: addmm
   op_api: True
 - func: addmm.out
   wrap_impl: addmm_out
   op_api: True
 - func: addmm_
   wrap_impl: addmm_
   op_api: True
 - func: addmv
   wrap_impl: addmv
   op_api: True
 - func: addmv.out
   wrap_impl: addmv_out
   op_api: True
 - func: addmv_
   wrap_impl: addmv_
   op_api: True
 - func: addr
   wrap_impl: addr
   op_api: False
 - func: addr.out
   wrap_impl: addr_out
   op_api: False
 - func: addr_
   wrap_impl: addr_
   op_api: False
 - func: affine_grid_generator
   wrap_impl: affine_grid_generator
 - func: affine_grid_generator_backward
   wrap_impl: affine_grid_generator_backward
 - func: all
   wrap_impl: all
   op_api: True
 - func: all.dim
   wrap_impl: all
   op_api: True
 - func: all.out
   wrap_impl: all_out
   op_api: True
 - func: all.all_out
   wrap_impl: all_out
   op_api: True
 - func: amax
   wrap_impl: amax
   op_api: False
 - func: amax.out
   wrap_impl: amax_out
   op_api: False
 - func: amin
   wrap_impl: amin
   op_api: False
 - func: amin.out
   wrap_impl: amin_out
   op_api: False
 - func: any
   wrap_impl: any
   op_api: True
 - func: any.all_out
   wrap_impl: any_out
   op_api: True
 - func: any.dim
   wrap_impl: any
   op_api: True
 - func: any.out
   wrap_impl: any_out
   op_api: True
 - func: arange
   wrap_impl: arange
   op_api: True
 - func: arange.out
   wrap_impl: arange_out
   op_api: True
 - func: arange.start
   wrap_impl: arange
   op_api: True
 - func: arange.start_out
   wrap_impl: arange_out
   op_api: True
 - func: arange.start_step
   wrap_impl: arange
   op_api: True
 - func: argsort
   wrap_impl: argsort
   op_api: False
 - func: argsort.dimname
   wrap_impl: argsort
   op_api: False
 - as_strided
 - as_strided_
 - func: asin
   wrap_impl: asin
 - func: asin.out
   wrap_impl: asin_out
 - func: asin_
   wrap_impl: asin_
 - func: asinh
   wrap_impl: asinh
   op_api: False
 - func: asinh.out
   wrap_impl: asinh_out
   op_api: False
 - func: asinh_
   wrap_impl: asinh_
   op_api: False
 - func: atan
   wrap_impl: atan
   op_api: True
 - func: atan.out
   wrap_impl: atan_out
   op_api: True
 - func: atan_
   wrap_impl: atan_
   op_api: True
 - func: atan2
   wrap_impl: atan2
   op_api: False
 - func: atan2.out
   wrap_impl: atan2_out
   op_api: False
 - func: atan2_
   wrap_impl: atan2_
   op_api: False
 - func: atanh
   wrap_impl: atanh
   op_api: False
 - func: atanh.out
   wrap_impl: atanh_out
   op_api: False
 - func: atanh_
   wrap_impl: atanh_
   op_api: False
 - func: avg_pool2d
   wrap_impl: avg_pool2d
   op_api: True
 - func: avg_pool2d.out
   wrap_impl: avg_pool2d_out
   op_api: True
 - func: avg_pool2d_backward
   wrap_impl: avg_pool2d_backward
   op_api: False
 - func: avg_pool2d_backward.grad_input
   wrap_impl: avg_pool2d_backward_out
   op_api: False
 - func: avg_pool3d
   wrap_impl: avg_pool3d
 - func: avg_pool3d.out
   wrap_impl: avg_pool3d_out
 - func: avg_pool3d_backward
   wrap_impl: avg_pool3d_backward
 - func: avg_pool3d_backward.grad_input
   wrap_impl: avg_pool3d_backward_out
 - func: baddbmm
   wrap_impl: baddbmm
   op_api: True
 - func: baddbmm.out
   wrap_impl: baddbmm_out
   op_api: True
 - func: baddbmm_
   wrap_impl: baddbmm_
   op_api: True
 - bartlett_window
 - bartlett_window.periodic
 - func: batch_norm
   wrap_impl: batch_norm
 - func: batch_norm_backward_elemt
   wrap_impl: batch_norm_backward_elemt
   op_api: True
 - func: batch_norm_backward_reduce
   wrap_impl: batch_norm_backward_reduce
   op_api: True
 - func: batch_norm_elemt
   wrap_impl: batch_norm_elemt
   op_api: True
 - func: batch_norm_elemt.out
   wrap_impl: batch_norm_elemt_out
   op_api: True
 - batch_norm_gather_stats_update
 - func: batch_norm_gather_stats_with_counts
   wrap_impl: batch_norm_gather_stats_with_counts
   op_api: True
 - batch_norm_reduce
 - func: batch_norm_stats
   wrap_impl: batch_norm_stats
   op_api: True
 - func: bernoulli
   wrap_impl: bernoulli
   op_api: True
 - func: bernoulli.out
   wrap_impl: bernoulli_out
   op_api: True
 - func: bernoulli.p
   wrap_impl: bernoulli
   op_api: True
 - func: bernoulli_.Tensor
   wrap_impl: bernoulli_
   op_api: True
 - func: bernoulli_.float
   wrap_impl: bernoulli_
   op_api: True
 - func: binary_cross_entropy
   wrap_impl: binary_cross_entropy
   op_api: False
 - func: binary_cross_entropy.out
   wrap_impl: binary_cross_entropy_out
   op_api: False
 - func: binary_cross_entropy_backward
   wrap_impl: binary_cross_entropy_backward
   op_api: True
 - func: binary_cross_entropy_backward.grad_input
   wrap_impl: binary_cross_entropy_backward_out
   op_api: True
 - func: binary_cross_entropy_with_logits
   op_api: False
 - func: bincount
   wrap_impl: bincount
   op_api: True
 - func: bitwise_and.Scalar
   wrap_impl: bitwise_and
   op_api: True
 - func: bitwise_and.Scalar_out
   wrap_impl: bitwise_and_out
   op_api: True
 - func: bitwise_and.Tensor
   wrap_impl: bitwise_and
   op_api: True
 - func: bitwise_and.Tensor_out
   wrap_impl: bitwise_and_out
   op_api: True
 - func: bitwise_and_.Scalar
   wrap_impl: bitwise_and_
   op_api: True
 - func: bitwise_and_.Tensor
   wrap_impl: bitwise_and_
   op_api: True
 - func: bitwise_not
   wrap_impl: bitwise_not
   op_api: True
 - func: bitwise_not.out
   wrap_impl: bitwise_not_out
   op_api: True
 - func: bitwise_not_
   wrap_impl: bitwise_not_
   op_api: True
 - func: bitwise_or.Scalar
   wrap_impl: bitwise_or
   op_api: True
 - func: bitwise_or.Scalar_out
   wrap_impl: bitwise_or_out
   op_api: True
 - func: bitwise_or.Tensor
   wrap_impl: bitwise_or
   op_api: True
 - func: bitwise_or.Tensor_out
   wrap_impl: bitwise_or_out
   op_api: True
 - func: bitwise_xor.Scalar
   wrap_impl: bitwise_xor
   op_api: False
 - func: bitwise_xor.Scalar_out
   wrap_impl: bitwise_xor_out
   op_api: False
 - func: bitwise_xor.Tensor
   wrap_impl: bitwise_xor
   op_api: False
 - func: bitwise_xor.Tensor_out
   wrap_impl: bitwise_xor_out
   op_api: False
 - func: bitwise_xor_.Scalar
   wrap_impl: bitwise_xor_
   op_api: False
 - func: bitwise_xor_.Tensor
   wrap_impl: bitwise_xor_
   op_api: False
 - blackman_window
 - blackman_window.periodic
 - func: bmm
   wrap_impl: bmm
   op_api: True
 - func: bmm.out
   wrap_impl: bmm_out
   op_api: True
 - func: cat
   wrap_impl: cat
   op_api: True
 - func: cat.names
   wrap_impl: cat
   op_api: True
 - func: cat.names_out
   wrap_impl: cat_out
   op_api: True
 - func: cat.out
   wrap_impl: cat_out
   op_api: True
 - func: cdist
   wrap_impl: cdist
 - func: ceil
   wrap_impl: ceil
   op_api: True
 - func: ceil.out
   wrap_impl: ceil_out
   op_api: True
 - func: ceil_
   wrap_impl: ceil_
 - func: channel_shuffle
   wrap_impl: channel_shuffle
 - func: clamp
   wrap_impl: clamp
   op_api: True
 - func: clamp.out
   wrap_impl: clamp_out
   op_api: True
 - func: clamp_
   wrap_impl: clamp_
   op_api: True
 - func: clamp_max
   wrap_impl: clamp_max
   op_api: False
 - func: clamp_max.out
   wrap_impl: clamp_max_out
   op_api: False
 - func: clamp_max_
   wrap_impl: clamp_max_
   op_api: False
 - func: clamp_min
   wrap_impl: clamp_min
   op_api: True
 - func: clamp_min.out
   wrap_impl: clamp_min_out
   op_api: True
 - func: clamp_min_
   wrap_impl: clamp_min_
   op_api: True
 - func: clamp.Tensor
   wrap_impl: clamp
   op_api: True
 - func: clamp.Tensor_out
   wrap_impl: clamp_out
   op_api: True
 - func: clamp_.Tensor
   wrap_impl: clamp_
   op_api: True
 - func: clamp_max.Tensor
   wrap_impl: clamp_max
   op_api: False
 - func: clamp_max.Tensor_out
   wrap_impl: clamp_max_out
   op_api: False
 - func: clamp_max_.Tensor
   wrap_impl: clamp_max_
   op_api: False
 - func: clamp_min.Tensor
   wrap_impl: clamp_min
   op_api: False
 - func: clamp_min.Tensor_out
   wrap_impl: clamp_min_out
   op_api: False
 - func: clamp_min_.Tensor
   wrap_impl: clamp_min_
   op_api: False
 - func: clone
   wrap_impl: clone
   op_api: True
 - func: col2im
   wrap_impl: col2im
   op_api: False
 - func: col2im.out
   wrap_impl: col2im_out
   op_api: False
 - func: col2im_backward
   wrap_impl: col2im_backward
 - func: col2im_backward.grad_input
   wrap_impl: col2im_backward_out
 - func: constant_pad_nd
   wrap_impl: constant_pad_nd
   op_api: False
 - contiguous
 - func: conv_tbc
   wrap_impl: conv_tbc
   op_api: False
 - func: conv_tbc_backward
   wrap_impl: conv_tbc_backward
   op_api: False
 - func: conv_transpose2d.input
   wrap_impl: conv_transpose2d
 - func: conv_transpose3d.input
   wrap_impl: conv_transpose3d
 - func: convolution_backward
   wrap_impl: convolution_backward
   op_api: True
 - func: convolution_backward_overrideable
   wrap_impl: convolution_backward_overrideable
 - func: convolution_overrideable
   wrap_impl: convolution_overrideable
 - func: count_nonzero
   wrap_impl: count_nonzero
   op_api: True
 - func: count_nonzero.dim_IntList
   wrap_impl: count_nonzero
   op_api: True
 - func: copy_
   wrap_impl: copy_
   op_api: True
 - copy_memory_
 - func: cos
   wrap_impl: cos
   op_api: True
 - func: cos.out
   wrap_impl: cos_out
   op_api: True
 - func: cos_
   wrap_impl: cos_
   op_api: True
 - func: cosh
   wrap_impl: cosh
   op_api: False
 - func: cosh.out
   wrap_impl: cosh_out
   op_api: False
 - func: cosh_
   wrap_impl: cosh_
   op_api: False
 - crop_and_resize
 - func: ctc_loss.IntList
   wrap_impl: ctc_loss
   op_api: True
 - func: ctc_loss.Tensor
   wrap_impl: ctc_loss
   op_api: True
 - func: cumprod.dimname_out
   wrap_impl: cumprod_out
 - func: cumprod.out
   wrap_impl: cumprod_out
 - func: cumprod_
   wrap_impl: cumprod_
 - func: cumprod_.dimname
   wrap_impl: cumprod_
 - func: cumsum.dimname_out
   wrap_impl: cumsum_out
   op_api: True
 - func: cumsum.out
   wrap_impl: cumsum_out
   op_api: True
 - func: cumsum
   wrap_impl: cumsum
   op_api: True
 - decode_jpeg
 - func: diag
   wrap_impl: diag
 - func: diag.out
   wrap_impl: diag_out
 - func: div.Scalar
   wrap_impl: div
   op_api: True
 - func: div.Scalar_mode
   wrap_impl: div
   op_api: True
 - func: div.Tensor
   wrap_impl: div
   op_api: True
 - func: div.Tensor_mode
   wrap_impl: div
   op_api: True
 - func: div.out
   wrap_impl: div_out
   op_api: True
 - func: div.out_mode
   wrap_impl: div_out
   op_api: True
 - func: div_.Scalar
   wrap_impl: div_
   op_api: True
 - func: div_.Scalar_mode
   wrap_impl: div_
   op_api: True
 - func: div_.Tensor
   wrap_impl: div_
   op_api: True
 - func: div_.Tensor_mode
   wrap_impl: div_
   op_api: True
 - func: dot
   wrap_impl: dot
   op_api: True
 - func: dot.out
   wrap_impl: dot_out
   op_api: True
 - func: dropout
   wrap_impl: dropout
   op_api: False
 - dropout_with_byte_mask
 - func: embedding
   wrap_impl: embedding
   op_api: True
 - func: embedding_backward
   wrap_impl: embedding_backward
   op_api: True
 - func: embedding_dense_backward
   wrap_impl: embedding_dense_backward
   op_api: True
 - func: embedding_renorm_
   wrap_impl: embedding_renorm_
   op_api: True
 - empty.memory_format
 - empty_like
 - empty_strided
 - empty_with_format
 - empty_with_format.names
 - func: eq.Scalar
   wrap_impl: eq
   op_api: True
 - func: eq.Scalar_out
   wrap_impl: eq_out
   op_api: True
 - func: eq.Tensor
   wrap_impl: eq
   op_api: True
 - func: eq.Tensor_out
   wrap_impl: eq_out
   op_api: True
 - func: eq_.Scalar
   wrap_impl: eq_
   op_api: True
 - func: eq_.Tensor
   wrap_impl: eq_
   op_api: True
 - func: equal
   wrap_impl: equal
   op_api: True
 - func: erf
   wrap_impl: erf
   op_api: True
 - func: erf.out
   wrap_impl: erf_out
   op_api: True
 - func: erf_
   wrap_impl: erf_
   op_api: True
 - func: erfc
   wrap_impl: erfc
   op_api: False
 - func: erfc.out
   wrap_impl: erfc_out
   op_api: False
 - func: erfc_
   wrap_impl: erfc_
   op_api: False
 - func: erfinv
   wrap_impl: erfinv
   op_api: False
 - func: erfinv.out
   wrap_impl: erfinv_out
   op_api: False
 - func: erfinv_
   wrap_impl: erfinv_
   op_api: False
 - func: exp
   wrap_impl: exp
   op_api: True
 - func: exp.out
   wrap_impl: exp_out
   op_api: True
 - func: exp2
   wrap_impl: exp2
   op_api: False
 - func: exp2.out
   wrap_impl: exp2_out
   op_api: False
 - func: exp2_
   wrap_impl: exp2_
   op_api: False
 - func: exp_
   wrap_impl: exp_
   op_api: True
 - func: expm1
   wrap_impl: expm1
   op_api: False
 - func: expm1.out
   wrap_impl: expm1_out
   op_api: False
 - func: expm1_
   wrap_impl: expm1_
   op_api: True
 - func: eye
   wrap_impl: eye
   op_api: True
 - func: eye.m
   wrap_impl: eye
   op_api: True
 - func: eye.m_out
   wrap_impl: eye_out
   op_api: True
 - func: eye.out
   wrap_impl: eye_out
   op_api: True
 - func: fill_.Tensor
   wrap_impl: fill_
   op_api: True
 - func: fill_.Scalar
   wrap_impl: fill_
   op_api: True
 - func: fill_diagonal_
   wrap_impl: fill_diagonal_
   op_api: True
 - func: flip
   wrap_impl: flip
   op_api: True
 - func: floor
   wrap_impl: floor
   op_api: True
 - func: floor.out
   wrap_impl: floor_out
   op_api: True
 - func: floor_
   wrap_impl: floor_
   op_api: False
 - func: floor_divide
   wrap_impl: floor_divide
   op_api: False
 - func: floor_divide.Scalar
   wrap_impl: floor_divide
   op_api: False
 - func: floor_divide.out
   wrap_impl: floor_divide_out
   op_api: False
 - func: floor_divide_.Scalar
   wrap_impl: floor_divide_
   op_api: False
 - func: floor_divide_.Tensor
   wrap_impl: floor_divide_
   op_api: False
 - func: fmod.Scalar
   wrap_impl: fmod
   op_api: False
 - func: fmod.Scalar_out
   wrap_impl: fmod_out
   op_api: False
 - func: fmod.Tensor
   wrap_impl: fmod
   op_api: False
 - func: fmod.Tensor_out
   wrap_impl: fmod_out
   op_api: False
 - func: fmod_.Scalar
   wrap_impl: fmod_
   op_api: False
 - func: fmod_.Tensor
   wrap_impl: fmod_
   op_api: False
 - func: frac
   wrap_impl: frac
   op_api: False
 - func: frac.out
   wrap_impl: frac_out
   op_api: False
 - func: frac_
   wrap_impl: frac_
   op_api: False
 - full
 - full.names
 - full.out
 - func: gather
   wrap_impl: gather
   op_api: True
 - func: gather.dimname
   wrap_impl: gather
   op_api: True
 - func: gather.dimname_out
   wrap_impl: gather_out
   op_api: True
 - func: gather.out
   wrap_impl: gather_out
   op_api: True
 - func: gcd.out
   wrap_impl: gcd.out
   op_api: True
 - func: ge.Scalar
   wrap_impl: ge
   op_api: True
 - func: ge.Scalar_out
   wrap_impl: ge_out
   op_api: True
 - func: ge.Tensor
   wrap_impl: ge
   op_api: True
 - func: ge.Tensor_out
   wrap_impl: ge_out
   op_api: True
 - func: ge_.Scalar
   wrap_impl: ge_
   op_api: False
 - func: ge_.Tensor
   wrap_impl: ge_
   op_api: False
 - func: gelu
   wrap_impl: gelu
   op_api: True
 - func: gelu.out
   wrap_impl: gelu_out
   op_api: True
 - func: gelu_backward
   wrap_impl: gelu_backward
   op_api: True
 - func: ger
   wrap_impl: ger
   op_api: True
 - func: ger.out
   wrap_impl: ger_out
   op_api: True
 - func: glu
   wrap_impl: glu
   op_api: False
 - func: glu.out
   wrap_impl: glu_out
   op_api: False
 - func: glu_backward
   wrap_impl: glu_backward
   op_api: False
 - func: glu_backward.grad_input
   wrap_impl: glu_backward_out
   op_api: False
 - func: grid_sampler_2d
   wrap_impl: grid_sampler_2d
   op_api: True
 - func: grid_sampler_2d_backward
   wrap_impl: grid_sampler_2d_backward
   op_api: True
 - func: grid_sampler_3d
   wrap_impl: grid_sampler_3d
 - func: grid_sampler_3d_backward
   wrap_impl: grid_sampler_3d_backward
 - func: gru.input
   wrap_impl: gru
 - func: gt.Scalar
   wrap_impl: gt
   op_api: True
 - func: gt.Scalar_out
   wrap_impl: gt_out
   op_api: True
 - func: gt.Tensor
   wrap_impl: gt
   op_api: True
 - func: gt.Tensor_out
   wrap_impl: gt_out
   op_api: True
 - func: gt_.Scalar
   wrap_impl: gt_
   op_api: False
 - func: gt_.Tensor
   wrap_impl: gt_
   op_api: False
 - hamming_window
 - hamming_window.periodic
 - hamming_window.periodic_alpha
 - hamming_window.periodic_alpha_beta
 - hann_window
 - hann_window.periodic
 - func: hardshrink
   wrap_impl: hardshrink
 - func: hardshrink_backward
   wrap_impl: hardshrink_backward
 - func: hardshrink_backward.grad_input
   wrap_impl: hardshrink_backward_out
 - func: hardsigmoid
   wrap_impl: hardsigmoid
   op_api: True
 - func: hardsigmoid.out
   wrap_impl: hardsigmoid_out
   op_api: True
 - func: hardsigmoid_
   wrap_impl: hardsigmoid_
   op_api: True
 - func: hardsigmoid_backward
   wrap_impl: hardsigmoid_backward
   op_api: True
 - func: hardswish
   wrap_impl: hardswish
   op_api: True
 - func: hardswish.out
   wrap_impl: hardswish_out
   op_api: True
 - func: hardswish_
   wrap_impl: hardswish_
   op_api: True
 - func: hardswish_backward
   wrap_impl: hardswish_backward
   op_api: True
 - func: hardtanh
   wrap_impl: hardtanh
   op_api: True
 - func: hardtanh.out
   wrap_impl: hardtanh_out
 - func: hardtanh_
   wrap_impl: hardtanh_
   op_api: True
 - func: hardtanh_backward
   wrap_impl: hardtanh_backward
   op_api: True
 - func: hardtanh_backward.grad_input
   wrap_impl: hardtanh_backward_out
 - func: histc
   wrap_impl: histc
   op_api: True
 - func: histc.out
   wrap_impl: histc_out
   op_api: True
 - func: im2col
   wrap_impl: im2col
 - func: im2col.out
   wrap_impl: im2col_out
 - func: im2col_backward
   wrap_impl: im2col_backward
   op_api: True
 - func: im2col_backward.grad_input
   wrap_impl: im2col_backward_out
   op_api: True
 - image_normalize
 - image_normalize_
 - img_to_tensor
 - func: index.Tensor
   wrap_impl: index
   op_api: True
 - func: index_add
   wrap_impl: index_add
   op_api: True
 - func: index_add.dimname
   wrap_impl: index_add
 - func: index_add.out
   wrap_impl: index_add_out
   op_api: True
 - func: index_fill.int_Scalar
   wrap_impl: index_fill
   op_api: False
 - func: index_fill.int_Tensor
   wrap_impl: index_fill
   op_api: False
 - func: index_fill_.int_Scalar
   wrap_impl: index_fill_
   op_api: False
 - func: index_fill_.int_Tensor
   wrap_impl: index_fill_
   op_api: True
 - func: index_select
   wrap_impl: index_select
   op_api: True
 - func: index_put
   wrap_impl: index_put
   op_api: True
 - func: index_put_
   wrap_impl: index_put_
   op_api: True
 - func: index_select.dimname
   wrap_impl: index_select
   op_api: False
 - func: index_select.dimname_out
   wrap_impl: index_select_out
   op_api: False
 - func: index_select.out
   wrap_impl: index_select_out
   op_api: True
 - func: inverse
   wrap_impl: inverse
   op_api: False
 - func: inverse.out
   wrap_impl: inverse_out
   op_api: False
 - is_pinned
 - is_set_to
 - func: isin.Tensor_Scalar_out
   wrap_impl: isin_out
   op_api: True
 - func: isin.Tensor_Scalar
   wrap_impl: isin
   op_api: True
 - func: isin.Scalar_Tensor_out
   wrap_impl: isin_out
   op_api: True
 - func: isclose
   wrap_impl: isclose
   op_api: False
 - func: isfinite
   wrap_impl: isfinite
 - func: isposinf.out
   wrap_impl: isposinf.out
   op_api: True
 - func: isneginf.out
   wrap_impl: isneginf.out
   op_api: True
 - isnan
 - func: kl_div
   wrap_impl: kl_div
   op_api: True
 - func: kl_div_backward
   wrap_impl: kl_div_backward
   op_api: True
 - func: kthvalue
   wrap_impl: kthvalue
   op_api: True
 - func: kthvalue.dimname
   wrap_impl: kthvalue
   op_api: True
 - func: kthvalue.dimname_out
   wrap_impl: kthvalue_out
   op_api: True
 - func: kthvalue.values
   wrap_impl: kthvalue
   op_api: False
 - func: l1_loss
   wrap_impl: l1_loss
   op_api: False
 - func: l1_loss.out
   wrap_impl: l1_loss_out
   op_api: False
 - func: l1_loss_backward
   wrap_impl: l1_loss_backward
   op_api: False
 - func: l1_loss_backward.grad_input
   wrap_impl: l1_loss_backward_out
   op_api: False
 - func: le.Scalar
   wrap_impl: le
   op_api: True
 - func: le.Scalar_out
   wrap_impl: le_out
   op_api: True
 - func: le.Tensor
   wrap_impl: le
   op_api: True
 - func: le.Tensor_out
   wrap_impl: le_out
   op_api: True
 - func: le_.Scalar
   wrap_impl: le_
   op_api: False
 - func: le_.Tensor
   wrap_impl: le_
   op_api: False
 - func: leaky_relu
   wrap_impl: leaky_relu
   op_api: True
 - func: leaky_relu.out
   wrap_impl: leaky_relu_out
   op_api: True
 - func: leaky_relu_
   wrap_impl: leaky_relu_
   op_api: False
 - func: leaky_relu_backward
   wrap_impl: leaky_relu_backward
   op_api: True
 - func: leaky_relu_backward.grad_input
   wrap_impl: leaky_relu_backward_out
   op_api: True
 - func: lerp.Scalar
   wrap_impl: lerp
   op_api: False
 - func: lerp.Scalar_out
   wrap_impl: lerp_out
   op_api: False
 - func: lerp.Tensor
   wrap_impl: lerp
   op_api: False
 - func: lerp.Tensor_out
   wrap_impl: lerp_out
   op_api: False
 - func: lerp_.Scalar
   wrap_impl: lerp_
   op_api: False
 - func: lerp_.Tensor
   wrap_impl: lerp_
   op_api: False
 - func: linalg_cross
   wrap_impl: linalg_cross
   op_api: True
 - func: linalg_cross.out
   wrap_impl: linalg_cross_out
   op_api: True
 - func: linspace
   wrap_impl: linspace
 - func: linspace.out
   wrap_impl: linspace_out
 - func: log
   wrap_impl: log
   op_api: True
 - func: log.out
   wrap_impl: log_out
   op_api: True
 - func: log10
   wrap_impl: log10
   op_api: True
 - func: log10.out
   wrap_impl: log10_out
   op_api: True
 - func: log10_
   wrap_impl: log10_
   op_api: True
 - func: log1p
   wrap_impl: log1p
   op_api: False
 - func: log1p.out
   wrap_impl: log1p_out
   op_api: False
 - func: log1p_
   wrap_impl: log1p_
   op_api: False
 - func: log2
   wrap_impl: log2
   op_api: True
 - func: log2.out
   wrap_impl: log2_out
   op_api: True
 - func: log2_
   wrap_impl: log2_
   op_api: False
 - func: log_
   wrap_impl: log_
   op_api: False
 - func: log_sigmoid
   wrap_impl: log_sigmoid
   op_api: False
 - func: log_sigmoid.out
   wrap_impl: log_sigmoid_out
   op_api: False
 - func: log_sigmoid_backward
   wrap_impl: log_sigmoid_backward
   op_api: False
 - func: log_sigmoid_backward.grad_input
   wrap_impl: log_sigmoid_backward_out
   op_api: False
 - func: log_sigmoid_forward
   wrap_impl: log_sigmoid_forward
   op_api: False
 - func: log_sigmoid_forward.output
   wrap_impl: log_sigmoid_forward_out
   op_api: False
 - func: log_softmax.Dimname
   wrap_impl: log_softmax
 - func: log_softmax.int
   wrap_impl: log_softmax
 - func: logaddexp
   wrap_impl: logaddexp
   op_api: False
 - func: logaddexp.out
   wrap_impl: logaddexp_out
   op_api: False
 - func: logaddexp2
   wrap_impl: logaddexp2
   op_api: False
 - func: logaddexp2.out
   wrap_impl: logaddexp2_out
   op_api: False
 - func: logical_and
   wrap_impl: logical_and
   op_api: True
 - func: logical_and.out
   wrap_impl: logical_and_out
   op_api: True
 - func: logical_and_
   wrap_impl: logical_and_
   op_api: False
 - func: logical_not
   wrap_impl: logical_not
   op_api: False
 - func: logical_not.out
   wrap_impl: logical_not_out
   op_api: False
 - func: logical_not_
   wrap_impl: logical_not_
   op_api: False
 - func: logical_or
   wrap_impl: logical_or
   op_api: True
 - func: logical_or.out
   wrap_impl: logical_or_out
   op_api: True
 - func: logical_or_
   wrap_impl: logical_or_
   op_api: False
 - func: logical_xor
   wrap_impl: logical_xor
   op_api: False
 - func: logical_xor.out
   wrap_impl: logical_xor_out
   op_api: False
 - func: logspace
   wrap_impl: logspace
 - func: logspace.out
   wrap_impl: logspace_out
 - func: logsumexp
   wrap_impl: logsumexp
   op_api: False
 - func: logsumexp.names
   wrap_impl: logsumexp
   op_api: False
 - func: logsumexp.names_out
   wrap_impl: logsumexp_out
   op_api: False
 - func: logsumexp.out
   wrap_impl: logsumexp_out
   op_api: False
 - func: lstm.data
   wrap_impl: lstm
 - func: lstm.input
   wrap_impl: lstm
 - func: lstm_cell
   wrap_impl: lstm_cell
 - func: lt.Scalar
   wrap_impl: lt
   op_api: True
 - func: lt.Scalar_out
   wrap_impl: lt_out
   op_api: True
 - func: lt.Tensor
   wrap_impl: lt
   op_api: True
 - func: lt.Tensor_out
   wrap_impl: lt_out
   op_api: True
 - func: lt_.Scalar
   wrap_impl: lt_
   op_api: True
 - func: lt_.Tensor
   wrap_impl: lt_
   op_api: True
 - func: masked_fill_.Scalar
   wrap_impl: masked_fill_
   op_api: True
 - func: masked_fill_.Tensor
   wrap_impl: masked_fill_
   op_api: True
 - func: masked_scatter_
   wrap_impl: masked_scatter_
   op_api: True
 - func: masked_select
   wrap_impl: masked_select
   op_api: True
 - func: masked_select.out
   wrap_impl: masked_select_out
   op_api: True
 - func: max
   wrap_impl: max
   op_api: True
 - func: max.dim
   wrap_impl: max
   op_api: True
 - func: max.dim_max
   wrap_impl: max
   op_api: True
 - func: max.names_dim
   wrap_impl: max
 - func: max.names_dim_max
   wrap_impl: max
 - func: max.out
   wrap_impl: max_out
   op_api: False
 - max_pool2d
 - func: max_pool2d_with_indices
   wrap_impl: max_pool2d_with_indices
 - func: max_pool2d_with_indices.out
   wrap_impl: max_pool2d_with_indices_out
 - func: max_pool2d_with_indices_backward
   wrap_impl: max_pool2d_with_indices_backward
 - func: max_pool2d_with_indices_backward.grad_input
   wrap_impl: max_pool2d_with_indices_backward_out
 - func: max_pool3d_with_indices
   wrap_impl: max_pool3d_with_indices
 - func: max_pool3d_with_indices.out
   wrap_impl: max_pool3d_with_indices_out
 - func: max_pool3d_with_indices_backward
   wrap_impl: max_pool3d_with_indices_backward
 - func: max_pool3d_with_indices_backward.grad_input
   wrap_impl: max_pool3d_with_indices_backward_out
 - func: max_unpool2d
   wrap_impl: max_unpool2d
   op_api: True
 - func: max_unpool2d.out
   wrap_impl: max_unpool2d_out
   op_api: True
 - func: max_unpool2d_backward
   wrap_impl: max_unpool2d_backward
   op_api: True
 - func: max_unpool2d_backward.grad_input
   wrap_impl: max_unpool2d_backward_out
   op_api: True
 - func: max_unpool3d
   wrap_impl: max_unpool3d
 - func: max_unpool3d.out
   wrap_impl: max_unpool3d_out
 - func: max_unpool3d_backward
   wrap_impl: max_unpool3d_backward
 - func: max_unpool3d_backward.grad_input
   wrap_impl: max_unpool3d_backward_out
 - func: maximum
   wrap_impl: maximum
   op_api: True
 - func: maximum.out
   wrap_impl: maximum_out
   op_api: True
 - func: mean
   wrap_impl: mean
   op_api: True
 - func: mean.dim
   wrap_impl: mean
   op_api: True
 - func: mean.names_dim
   wrap_impl: mean
   op_api: True
 - func: mean.names_out
   wrap_impl: mean_out
   op_api: True
 - func: mean.out
   wrap_impl: mean_out
   op_api: True
 - func: median
   wrap_impl: median
   op_api: True
 - func: median.dim
   wrap_impl: median
   op_api: True
 - func: median.dim_values
   wrap_impl: median
   op_api: True
 - func: median.names_dim
   wrap_impl: median
   op_api: True
 - func: median.names_dim_values
   wrap_impl: median
   op_api: True
 - func: min
   wrap_impl: min
   op_api: True
 - func: min.dim
   wrap_impl: min
   op_api: True
 - func: min.dim_min
   wrap_impl: min
   op_api: True
 - func: min.names_dim
   wrap_impl: min
 - func: min.names_dim_min
   wrap_impl: min
 - func: min.out
   wrap_impl: min_out
   op_api: False
 - func: minimum
   wrap_impl: minimum
   op_api: True
 - func: minimum.out
   wrap_impl: minimum_out
   op_api: True
 - func: mish
   wrap_impl: mish
 - func: mish.out
   wrap_impl: mish_out
 - func: mish_
   wrap_impl: mish_
 - func: mish_backward
   wrap_impl: mish_backward
   op_api: True
 - func: mm
   wrap_impl: mm
   op_api: True
 - func: mm.out
   wrap_impl: mm_out
   op_api: True
 - func: mse_loss
   wrap_impl: mse_loss
   op_api: True
 - func: mse_loss.out
   wrap_impl: mse_loss_out
   op_api: True
 - func: mse_loss_backward
   wrap_impl: mse_loss_backward
   op_api: True
 - func: mse_loss_backward.grad_input
   wrap_impl: mse_loss_backward_out
   op_api: True
 - func: mul.Scalar
   wrap_impl: mul
   op_api: True
 - func: mul.Tensor
   wrap_impl: mul
   op_api: True
 - func: mul.out
   wrap_impl: mul_out
   op_api: True
 - func: mul_.Scalar
   wrap_impl: mul_
   op_api: True
 - func: mul_.Tensor
   wrap_impl: mul_
   op_api: True
 - func: multilabel_margin_loss
   wrap_impl: multilabel_margin_loss
 - func: multilabel_margin_loss.out
   wrap_impl: multilabel_margin_loss_out
 - func: multilabel_margin_loss_forward
   wrap_impl: multilabel_margin_loss_forward
 - func: multilabel_margin_loss_forward.output
   wrap_impl: multilabel_margin_loss_forward_out
 - func: multinomial
   wrap_impl: multinomial
   op_api: True
 - func: multinomial.out
   wrap_impl: multinomial_out
   op_api: True
 - func: mv
   wrap_impl: mv
 - func: mv.out
   wrap_impl: mv_out
 - func: nanmedian
   wrap_impl: nanmedian
   op_api: False
 - func: nanmedian.dim
   wrap_impl: nanmedian
   op_api: True
 - func: native_batch_norm
   wrap_impl: native_batch_norm
   op_api: True
 - func: native_batch_norm.out
   wrap_impl: native_batch_norm_out
   op_api: True
 - func: native_batch_norm_backward
   wrap_impl: native_batch_norm_backward
   op_api: True
 - func: native_dropout
   wrap_impl: native_dropout
   op_api: False
 - func: native_dropout_backward
   wrap_impl: native_dropout_backward
   op_api: False
 - func: native_group_norm
   wrap_impl: native_group_norm
 - func: native_group_norm_backward
   wrap_impl: native_group_norm_backward
 - func: native_layer_norm
   wrap_impl: native_layer_norm
   op_api: True
 - func: native_layer_norm_backward
   wrap_impl: native_layer_norm_backward
   op_api: True
 - func: ne.Scalar
   wrap_impl: ne
   op_api: True
 - func: ne.Scalar_out
   wrap_impl: ne_out
   op_api: True
 - func: ne.Tensor
   wrap_impl: ne
   op_api: True
 - func: ne.Tensor_out
   wrap_impl: ne_out
   op_api: True
 - func: ne_.Scalar
   wrap_impl: ne_
   op_api: False
 - func: ne_.Tensor
   wrap_impl: ne_
   op_api: False
 - func: neg
   wrap_impl: neg
   op_api: True
 - func: neg.out
   wrap_impl: neg_out
   op_api: True
 - func: neg_
   wrap_impl: neg_
   op_api: False
 - func: nll_loss
   wrap_impl: nll_loss
 - func: nll_loss.out
   wrap_impl: nll_loss_out
 - func: nll_loss2d
   wrap_impl: nll_loss2d
 - func: nll_loss2d.out
   wrap_impl: nll_loss2d_out
 - func: nll_loss2d_backward
   wrap_impl: nll_loss2d_backward
   op_api: True
 - func: nll_loss2d_backward.grad_input
   wrap_impl: nll_loss2d_backward_out
   op_api: True
 - func: nll_loss2d_forward
   wrap_impl: nll_loss2d_forward
   op_api: True
 - func: nll_loss2d_forward.output
   wrap_impl: nll_loss2d_forward_out
   op_api: True
 - func: nll_loss_backward
   wrap_impl: nll_loss_backward
   op_api: True
 - func: nll_loss_backward.grad_input
   wrap_impl: nll_loss_backward_out
   op_api: True
 - func: nll_loss_forward
   wrap_impl: nll_loss_forward
   op_api: True
 - func: nll_loss_forward.output
   wrap_impl: nll_loss_forward_out
   op_api: True
 - func: nonzero
   wrap_impl: nonzero
   op_api: True
 - func: nonzero.out
   wrap_impl: nonzero_out
   op_api: True
 - func: norm.Scalar
   wrap_impl: norm
 - func: norm.ScalarOpt_dim
   wrap_impl: norm
 - func: norm.ScalarOpt_dim_dtype
   wrap_impl: norm
 - func: norm.ScalarOpt_dtype
   wrap_impl: norm
 - func: norm.dtype_out
   wrap_impl: norm_out
 - func: norm.out
   wrap_impl: norm_out
   op_api: True
 - func: normal.Tensor_Tensor
   wrap_impl: normal
   op_api: False
 - func: normal.Tensor_Tensor_out
   wrap_impl: normal_out
   op_api: False
 - func: normal.Tensor_float
   wrap_impl: normal
   op_api: False
 - func: normal.Tensor_float_out
   wrap_impl: normal_out
   op_api: False
 - func: normal.float_Tensor
   wrap_impl: normal
   op_api: False
 - func: normal.float_Tensor_out
   wrap_impl: normal_out
   op_api: False
 - func: normal.float_float
   wrap_impl: normal
   op_api: False
 - func: normal.float_float_out
   wrap_impl: normal_out
   op_api: False
 - func: normal_
   wrap_impl: normal_
   op_api: True
 - func: one_
   wrap_impl: one_
   op_api: True
 - func: one_hot
   wrap_impl: one_hot
   op_api: False
 - func: ones
   wrap_impl: ones
   op_api: True
 - func: ones.names
   wrap_impl: ones
   op_api: True
 - func: ones.out
   wrap_impl: ones_out
   op_api: True
 - func: ones_like
   wrap_impl: ones_like
   op_api: True
 - func: pdist
   wrap_impl: pdist
 - func: pow.Scalar
   wrap_impl: pow
   op_api: True
 - func: pow.Scalar_out
   wrap_impl: pow_out
   op_api: True
 - func: pow.Tensor_Scalar
   wrap_impl: pow
   op_api: True
 - func: pow.Tensor_Scalar_out
   wrap_impl: pow_out
   op_api: True
 - func: pow.Tensor_Tensor
   wrap_impl: pow
   op_api: True
 - func: pow.Tensor_Tensor_out
   wrap_impl: pow_out
   op_api: True
 - func: pow_.Scalar
   wrap_impl: pow_
   op_api: True
 - func: pow_.Tensor
   wrap_impl: pow_
   op_api: True
 - func: prelu
   wrap_impl: prelu
   op_api: True
 - func: prelu_backward
   wrap_impl: prelu_backward
 - func: prod
   wrap_impl: prod
   op_api: True
 - func: prod.dim_int
   wrap_impl: prod
   op_api: True
 - func: prod.int_out
   wrap_impl: prod_out
   op_api: True
 - func: put_
   wrap_impl: put_
   op_api: True
 - func: qr
   wrap_impl: qr
   op_api: False
 - func: qr.Q
   wrap_impl: qr
   op_api: False
 - func: quantize_per_channel
   wrap_impl: quantize_per_channel
 - func: quantize_per_tensor
   wrap_impl: quantize_per_tensor
 - func: random_
   wrap_impl: random_
   op_api: True
 - func: random_.from
   wrap_impl: random_
   op_api: True
 - func: random_.to
   wrap_impl: random_
   op_api: True
 - func: randperm
   wrap_impl: randperm
   op_api: True
 - func: randperm.generator
   wrap_impl: randperm
   op_api: True
 - func: randperm.generator_out
   wrap_impl: randperm_out
   op_api: True
 - func: randperm.out
   wrap_impl: randperm_out
   op_api: True
 - func: range
   wrap_impl: range
 - func: range.out
   wrap_impl: range_out
   op_api: True
 - func: range.step
   wrap_impl: range
 - func: reciprocal
   wrap_impl: reciprocal
   op_api: True
 - func: reciprocal.out
   wrap_impl: reciprocal_out
   op_api: True
 - func: reciprocal_
   wrap_impl: reciprocal_
   op_api: True
 - func: reflection_pad1d
   wrap_impl: reflection_pad1d
   op_api: True
 - func: reflection_pad1d.out
   wrap_impl: reflection_pad1d_out
   op_api: True
 - func: reflection_pad1d_backward
   wrap_impl: reflection_pad1d_backward
   op_api: False
 - func: reflection_pad1d_backward.grad_input
   wrap_impl: reflection_pad1d_backward_out
   op_api: False
 - func: reflection_pad2d
   wrap_impl: reflection_pad2d
   op_api: True
 - func: reflection_pad2d.out
   wrap_impl: reflection_pad2d_out
   op_api: True
 - func: reflection_pad2d_backward
   wrap_impl: reflection_pad2d_backward
   op_api: True
 - func: reflection_pad2d_backward.grad_input
   wrap_impl: reflection_pad2d_backward_out
   op_api: True
 - func: relu
   wrap_impl: relu
   op_api: True
 - func: relu_
   wrap_impl: relu_
   op_api: True
 - func: remainder.Scalar
   wrap_impl: remainder
   op_api: False
 - func: remainder.Scalar_out
   wrap_impl: remainder_out
   op_api: False
 - func: remainder.Tensor
   wrap_impl: remainder
   op_api: False
 - func: remainder.Tensor_out
   wrap_impl: remainder_out
   op_api: False
 - func: remainder_.Scalar
   wrap_impl: remainder_
   op_api: False
 - func: remainder_.Tensor
   wrap_impl: remainder_
   op_api: False
 - func: remainder.Scalar_Tensor
   wrap_impl: remainder
   op_api: False
 - func: renorm
   wrap_impl: renorm
   op_api: False
 - func: renorm.out
   wrap_impl: renorm_out
   op_api: False
 - func: renorm_
   wrap_impl: renorm_
   op_api: False
 - func: repeat
   wrap_impl: repeat
   op_api: False
 - func: repeat_interleave.self_Tensor
   wrap_impl: repeat_interleave
   op_api: True
 - func: repeat_interleave.self_int
   wrap_impl: repeat_interleave
   op_api: True
 - func: replication_pad1d
   wrap_impl: replication_pad1d
   op_api: False
 - func: replication_pad1d.out
   wrap_impl: replication_pad1d_out
   op_api: False
 - func: replication_pad1d_backward
   wrap_impl: replication_pad1d_backward
   op_api: False
 - func: replication_pad1d_backward.grad_input
   wrap_impl: replication_pad1d_backward_out
   op_api: False
 - func: replication_pad2d
   wrap_impl: replication_pad2d
   op_api: False
 - func: replication_pad2d.out
   wrap_impl: replication_pad2d_out
   op_api: False
 - func: replication_pad2d_backward
   wrap_impl: replication_pad2d_backward
   op_api: True
 - func: replication_pad2d_backward.grad_input
   wrap_impl: replication_pad2d_backward_out
   op_api: True
 - resize_
 - resize_as_
 - reverse
 - func: roll
   wrap_impl: roll
   op_api: True
 - func: round
   wrap_impl: round
   op_api: True
 - func: round.out
   wrap_impl: round_out
   op_api: True
 - func: round_
   wrap_impl: round_
   op_api: True
 - func: rrelu_with_noise
   wrap_impl: rrelu_with_noise
   op_api: True
 - func: rrelu_with_noise.out
   wrap_impl: rrelu_with_noise_out
   op_api: True
 - func: rrelu_with_noise_
   wrap_impl: rrelu_with_noise_
   op_api: True
 - func: rrelu_with_noise_backward
   wrap_impl: rrelu_with_noise_backward
 - func: square.out
   wrap_impl: square_out
   op_api: True
 - func: rsqrt
   wrap_impl: rsqrt
   op_api: False
 - func: rsqrt.out
   wrap_impl: rsqrt_out
   op_api: False
 - func: rsqrt_
   wrap_impl: rsqrt_
   op_api: False
 - func: rsub.Scalar
   wrap_impl: rsub
   op_api: False
 - func: rsub.Tensor
   wrap_impl: rsub
   op_api: False
 - func: scalar_tensor
   wrap_impl: scalar_tensor
   op_api: False
 - func: scatter.src_out
   wrap_impl: scatter_out
   op_api: True
 - func: scatter.value_out
   wrap_impl: scatter_out
   op_api: True
 - func: scatter.reduce_out
   wrap_impl: scatter_out
   op_api: True
 - func: scatter.value_reduce_out
   wrap_impl: scatter_out
   op_api: True
 - func: scatter_add
   wrap_impl: scatter_add
   op_api: True
 - func: scatter_add.dimname
   wrap_impl: scatter_add
   op_api: True
 - func: scatter_add_
   wrap_impl: scatter_add_
   op_api: True
 - func: searchsorted.Scalar
   wrap_impl: searchsorted
   op_api: True
 - func: searchsorted.Tensor
   wrap_impl: searchsorted
   op_api: True
 - func: searchsorted.Tensor_out
   wrap_impl: searchsorted_out
   op_api: True
 - set_
 - set_.source_Storage
 - set_.source_Storage_storage_offset
 - set_.source_Tensor
 - func: sgn
   wrap_impl: sgn
   op_api: True
 - func: sgn.out
   wrap_impl: sgn_out
   op_api: True
 - func: sigmoid
   wrap_impl: sigmoid
   op_api: True
 - func: sigmoid.out
   wrap_impl: sigmoid_out
   op_api: True
 - func: sigmoid_
   wrap_impl: sigmoid_
   op_api: True
 - func: sigmoid_backward
   wrap_impl: sigmoid_backward
   op_api: True
 - func: sigmoid_backward.grad_input
   wrap_impl: sigmoid_backward_out
   op_api: True
 - func: sign
   wrap_impl: sign
   op_api: True
 - func: sign.out
   wrap_impl: sign_out
   op_api: True
 - func: sign_
   wrap_impl: sign_
   op_api: True
 - func: signbit.out
   wrap_impl: signbit_out
   op_api: True
 - func: sin
   wrap_impl: sin
   op_api: True
 - func: sin.out
   wrap_impl: sin_out
   op_api: True
 - func: sin_
   wrap_impl: sin_
   op_api: True
 - func: sinc
   wrap_impl: sinc
   op_api: True
 - func: sinc.out
   wrap_impl: sinc_out
   op_api: True
 - func: sinc_
   wrap_impl: sinc_
   op_api: True
 - func: sinh
   wrap_impl: sinh
   op_api: True
 - func: sinh.out
   wrap_impl: sinh_out
   op_api: True
 - func: sinh_
   wrap_impl: sinh_
   op_api: True
 - func: linalg_slogdet
   wrap_impl: linalg_slogdet
   op_api: False
 - func: linalg_slogdet.out
   wrap_impl: linalg_slogdet_out
   op_api: False
 - func: slow_conv3d
   wrap_impl: slow_conv3d
 - func: slow_conv3d.out
   wrap_impl: slow_conv3d_out
 - func: slow_conv3d_forward
   wrap_impl: slow_conv3d_forward
 - func: slow_conv3d_forward.output
   wrap_impl: slow_conv3d_forward_out
 - func: slow_conv_dilated2d
   wrap_impl: slow_conv_dilated2d
 - slow_conv_dilated2d_backward
 - func: slow_conv_transpose2d
   wrap_impl: slow_conv_transpose2d
 - func: slow_conv_transpose2d.out
   wrap_impl: slow_conv_transpose2d_out
 - slow_conv_transpose2d_backward
 - func: smooth_l1_loss
   wrap_impl: smooth_l1_loss
   op_api: True
 - func: smooth_l1_loss.out
   wrap_impl: smooth_l1_loss_out
   op_api: True
 - func: smooth_l1_loss_backward
   wrap_impl: smooth_l1_loss_backward
   op_api: True
 - func: smooth_l1_loss_backward.grad_input
   wrap_impl: smooth_l1_loss_backward_out
   op_api: True
 - func: soft_margin_loss
   wrap_impl: soft_margin_loss
 - func: soft_margin_loss.out
   wrap_impl: soft_margin_loss_out
 - func: soft_margin_loss_backward
   wrap_impl: soft_margin_loss_backward
 - func: soft_margin_loss_backward.grad_input
   wrap_impl: soft_margin_loss_backward_out
 - func: softmax.Dimname
   wrap_impl: softmax
 - func: softmax.int
   wrap_impl: softmax
 - func: softplus
   wrap_impl: softplus
   op_api: True
 - func: softplus.out
   wrap_impl: softplus_out
   op_api: True
 - func: softplus_backward.grad_input
   wrap_impl: softplus_backward_out
   op_api: True
 - func: softshrink
   wrap_impl: softshrink
 - func: softshrink.out
   wrap_impl: softshrink_out
 - func: softshrink_backward
   wrap_impl: softshrink_backward
 - func: softshrink_backward.grad_input
   wrap_impl: softshrink_backward_out
 - func: sort
   wrap_impl: sort
   op_api: True
 - func: sort.stable
   wrap_impl: sort
   op_api: True
 - func: sort.dimname
   wrap_impl: sort
   op_api: True
 - func: sort.dimname_values
   wrap_impl: sort_out
   op_api: True
 - func: sort.values
   wrap_impl: sort_out
   op_api: True
 - func: sort.values_stable
   wrap_impl: sort_out
   op_api: True
 - func: sqrt
   wrap_impl: sqrt
   op_api: True
 - func: sqrt.out
   wrap_impl: sqrt_out
   op_api: True
 - func: sqrt_
   wrap_impl: sqrt_
   op_api: True
 - squeeze
 - squeeze.dim
 - func: stack
   wrap_impl: stack
   op_api: True
 - func: stack.out
   wrap_impl: stack_out
   op_api: True
 - func: std.correction
   wrap_impl: std
   op_api: True
 - func: std.correction_out
   wrap_impl: std_out
   op_api: True
 - func: std.correction_names
   wrap_impl: std
   op_api: True
 - func: std.correction_names_out
   wrap_impl: std_out
   op_api: True
 - func: std.names_out
   wrap_impl: std_out
   op_api: True
 - func: std.out
   wrap_impl: std_out
   op_api: True
 - func: std_mean.correction
   wrap_impl: std_mean
 - func: std_mean.correction_names
   wrap_impl: std_mean
 - func: sub.Scalar
   wrap_impl: sub
   op_api: True
 - func: sub.Tensor
   wrap_impl: sub
   op_api: True
 - func: sub.out
   wrap_impl: sub_out
   op_api: True
 - func: sub_.Scalar
   wrap_impl: sub_
   op_api: True
 - func: sub_.Tensor
   wrap_impl: sub_
   op_api: True
 - func: sum
   wrap_impl: sum
   op_api: True
 - func: sum.DimnameList_out
   wrap_impl: sum_out
   op_api: True
 - func: sum.IntList_out
   wrap_impl: sum_out
   op_api: True
 - func: sum.dim_DimnameList
   wrap_impl: sum
   op_api: True
 - func: sum.dim_IntList
   wrap_impl: sum
   op_api: True
 - func: take
   wrap_impl: take
   op_api: True
 - func: take.out
   wrap_impl: take_out
   op_api: True
 - func: tanh
   wrap_impl: tanh
   op_api: True
 - func: tanh.out
   wrap_impl: tanh_out
   op_api: True
 - func: tanh_
   wrap_impl: tanh_
   op_api: True
 - func: tan
   wrap_impl: tan
   op_api: True
 - func: tan.out
   wrap_impl: tan_out
   op_api: True
 - func: tan_
   wrap_impl: tan_
   op_api: True
 - func: tanh_backward
   wrap_impl: tanh_backward
   op_api: True
 - func: tanh_backward.grad_input
   wrap_impl: tanh_backward_out
   op_api: True
 - func: threshold
   wrap_impl: threshold
   op_api: True
 - func: threshold.out
   wrap_impl: threshold_out
   op_api: True
 - func: threshold_
   wrap_impl: threshold_
   op_api: True
 - func: threshold_backward
   wrap_impl: threshold_backward
   op_api: True
 - to.device
 - func: to.dtype
   wrap_impl: to
   op_api: True
 - to.dtype_layout
 - to.other
 - func: topk
   wrap_impl: topk
   op_api: True
 - func: topk.values
   wrap_impl: topk
   op_api: True
 - func: triangular_solve.X
   wrap_impl: triangular_solve_out
 - func: tril
   wrap_impl: tril
   op_api: True
 - func: tril.out
   wrap_impl: tril_out
   op_api: True
 - func: tril_
   wrap_impl: tril_
   op_api: True
 - tril_indices
 - func: triu
   wrap_impl: triu
   op_api: True
 - func: triu.out
   wrap_impl: triu_out
   op_api: True
 - func: triu_
   wrap_impl: triu_
   op_api: True
 - triu_indices
 - func: true_divide.Scalar
   wrap_impl: true_divide
   op_api: False
 - func: true_divide.Tensor
   wrap_impl: true_divide
   op_api: False
 - func: true_divide.out
   wrap_impl: true_divide_out
   op_api: False
 - func: true_divide_.Scalar
   wrap_impl: true_divide_
   op_api: False
 - func: true_divide_.Tensor
   wrap_impl: true_divide_
   op_api: False
 - func: trace
   wrap_impl: trace
   op_api: True
 - func: trunc
   wrap_impl: trunc
   op_api: False
 - func: trunc.out
   wrap_impl: trunc_out
   op_api: False
 - func: trunc_
   wrap_impl: trunc_
   op_api: False
 - unfold
 - func: uniform_
   wrap_impl: uniform_
   op_api: True
 - func: unique_consecutive
   wrap_impl: unique_consecutive
   op_api: True
 - unsqueeze
 - func: upsample_bicubic2d
   wrap_impl: upsample_bicubic2d
 - func: upsample_bicubic2d.out
   wrap_impl: upsample_bicubic2d_out
 - func: upsample_bicubic2d.vec
   wrap_impl: upsample_bicubic2d
 - func: upsample_bicubic2d_backward
   wrap_impl: upsample_bicubic2d_backward
 - func: upsample_bicubic2d_backward.grad_input
   wrap_impl: upsample_bicubic2d_backward_out
 - func: upsample_bicubic2d_backward.vec
   wrap_impl: upsample_bicubic2d_backward
 - func: upsample_bilinear2d
   wrap_impl: upsample_bilinear2d
   op_api: True
 - func: upsample_bilinear2d.out
   wrap_impl: upsample_bilinear2d_out
   op_api: True
 - func: upsample_bilinear2d.vec
   wrap_impl: upsample_bilinear2d
   op_api: True
 - func: upsample_bilinear2d_backward
   wrap_impl: upsample_bilinear2d_backward
   op_api: True
 - func: upsample_bilinear2d_backward.grad_input
   wrap_impl: upsample_bilinear2d_backward_out
   op_api: True
 - func: upsample_bilinear2d_backward.vec
   wrap_impl: upsample_bilinear2d_backward
   op_api: True
 - func: upsample_linear1d
   wrap_impl: upsample_linear1d
 - func: upsample_linear1d.out
   wrap_impl: upsample_linear1d_out
 - func: upsample_linear1d.vec
   wrap_impl: upsample_linear1d
 - func: upsample_linear1d_backward
   wrap_impl: upsample_linear1d_backward
 - func: upsample_linear1d_backward.vec
   wrap_impl: upsample_linear1d_backward
 - func: upsample_nearest1d
   wrap_impl: upsample_nearest1d
   op_api: True
 - func: upsample_nearest1d.out
   wrap_impl: upsample_nearest1d_out
   op_api: True
 - func: upsample_nearest1d.vec
   wrap_impl: upsample_nearest1d
   op_api: True
 - func: upsample_nearest1d_backward
   wrap_impl: upsample_nearest1d_backward
   op_api: False
 - func: upsample_nearest1d_backward.grad_input
   wrap_impl: upsample_nearest1d_backward_out
   op_api: False
 - func: upsample_nearest1d_backward.vec
   wrap_impl: upsample_nearest1d_backward
   op_api: False
 - func: upsample_nearest2d
   wrap_impl: upsample_nearest2d
   op_api: True
 - func: upsample_nearest2d.out
   wrap_impl: upsample_nearest2d_out
   op_api: True
 - func: upsample_nearest2d.vec
   wrap_impl: upsample_nearest2d
   op_api: True
 - func: upsample_nearest2d_backward
   wrap_impl: upsample_nearest2d_backward
   op_api: True
 - func: upsample_nearest2d_backward.grad_input
   wrap_impl: upsample_nearest2d_backward_out
   op_api: True
 - func: upsample_nearest2d_backward.vec
   wrap_impl: upsample_nearest2d_backward
   op_api: True
 - func: upsample_nearest3d
   wrap_impl: upsample_nearest3d
 - func: upsample_nearest3d.out
   wrap_impl: upsample_nearest3d_out
 - func: upsample_nearest3d.vec
   wrap_impl: upsample_nearest3d
 - func: upsample_nearest3d_backward
   wrap_impl: upsample_nearest3d_backward
 - func: upsample_nearest3d_backward.grad_input
   wrap_impl: upsample_nearest3d_backward_out
 - func: upsample_nearest3d_backward.vec
   wrap_impl: upsample_nearest3d_backward
 - func: upsample_trilinear3d
   wrap_impl: upsample_trilinear3d
 - func: upsample_trilinear3d.out
   wrap_impl: upsample_trilinear3d_out
 - func: upsample_trilinear3d.vec
   wrap_impl: upsample_trilinear3d
 - func: upsample_trilinear3d_backward
   wrap_impl: upsample_trilinear3d_backward
 - func: upsample_trilinear3d_backward.grad_input
   wrap_impl: upsample_trilinear3d_backward_out
 - func: upsample_trilinear3d_backward.vec
   wrap_impl: upsample_trilinear3d_backward
 - func: var.correction
   wrap_impl: var
   op_api: False
 - func: var.correction_names
   wrap_impl: var
   op_api: False
 - func: var.out
   wrap_impl: var_out
   op_api: False
 - func: var.names_out
   wrap_impl: var_out
   op_api: False
 - func: var.correction_out
   wrap_impl: var_out
   op_api: False
 - func: var.correction_names_out
   wrap_impl: var_out
   op_api: False
 - func: var_mean.correction
   wrap_impl: var_mean
 - func: var_mean.correction_names
   wrap_impl: var_mean
 - view
 - view_as_complex
 - view_as_real
 - func: where
   wrap_impl: where
 - func: where.self
   wrap_impl: where
   op_api: True
 - func: xlogy.OutScalar_Other
   wrap_impl: xlogy_out
   op_api: False
 - func: xlogy.OutScalar_Self
   wrap_impl: xlogy_out
   op_api: False
 - func: xlogy.OutTensor
   wrap_impl: xlogy
   op_api: False
 - func: xlogy.Scalar_Other
   wrap_impl: xlogy
   op_api: False
 - func: xlogy.Scalar_Self
   wrap_impl: xlogy
   op_api: False
 - func: xlogy.Tensor
   wrap_impl: xlogy
   op_api: False
 - func: xlogy_.Scalar_Other
   wrap_impl: xlogy_
   op_api: False
 - func: xlogy_.Tensor
   wrap_impl: xlogy_
   op_api: False
 - func: zero_
   wrap_impl: zero_
   op_api: True
 - func: zeros
   wrap_impl: zeros
   op_api: True
 - func: zeros.names
   wrap_impl: zeros
   op_api: True
 - func: zeros.out
   wrap_impl: zeros_out
   op_api: True
 - func: zeros_like
   wrap_impl: zeros_like
   op_api: False
 - func: silu
   wrap_impl: silu
   op_api: False
 - func: silu_
   wrap_impl: silu_
   op_api: False
 - func: silu.out
   wrap_impl: silu_out
   op_api: False
 - func: silu_backward.grad_input
   wrap_impl: silu_backward_out
   op_api: False
 - func: silu_backward
   wrap_impl: silu_backward
   op_api: False
 - func: celu
   wrap_impl: celu
 - func: celu_
   wrap_impl: celu_

autograd:
  - func: matmul
    op_api: False
  - func: matmul.out
    op_api: False
  - func: elu.out
    op_api: False
  - func: elu
    op_api: False
  - func: elu_
    op_api: False
  - func: binary_cross_entropy_with_logits_backward
    op_api: False
  - func: selu
    op_api: False
  - func: selu_
    op_api: False

tocpu:
  - _cholesky_helper
  - _cholesky_solve_helper
  - _compute_linear_combination
  - _compute_linear_combination.out
  - _dirichlet_grad
  - _embedding_bag_per_sample_weights_backward
  - _efficientzerotensor
  - _fft_r2c
  - _fft_r2c.out
  - _fft_c2r
  - _fft_c2r.out
  - _fft_c2c
  - _fft_c2c.out
  - _foreach_add.Scalar
  - _foreach_add_.Scalar
  - _foreach_sub.Scalar
  - _foreach_sub_.Scalar
  - _foreach_mul.Scalar
  - _foreach_mul_.Scalar
  - _foreach_div.Scalar
  - _foreach_div_.Scalar
  - _foreach_add.List
  - _foreach_add_.List
  - _foreach_sub.List
  - _foreach_sub_.List
  - _foreach_mul.List
  - _foreach_mul_.List
  - _foreach_div.List
  - _foreach_div_.List
  - _foreach_add.ScalarList
  - _foreach_add_.ScalarList
  - _foreach_sub.ScalarList
  - _foreach_sub_.ScalarList
  - _foreach_div.ScalarList
  - _foreach_div_.ScalarList
  - _foreach_mul.ScalarList
  - _foreach_mul_.ScalarList
  - _foreach_exp
  - _foreach_zero_
  - _foreach_exp_
  - _foreach_sqrt
  - _foreach_sqrt_
  - _foreach_abs
  - _foreach_abs_
  - _foreach_acos
  - _foreach_acos_
  - _foreach_asin
  - _foreach_asin_
  - _foreach_atan
  - _foreach_atan_
  - _foreach_ceil
  - _foreach_ceil_
  - _foreach_cos
  - _foreach_cos_
  - _foreach_cosh
  - _foreach_cosh_
  - _foreach_erf
  - _foreach_erf_
  - _foreach_erfc
  - _foreach_erfc_
  - _foreach_expm1
  - _foreach_expm1_
  - _foreach_floor
  - _foreach_floor_
  - _foreach_log
  - _foreach_log_
  - _foreach_log10
  - _foreach_log10_
  - _foreach_log1p
  - _foreach_log1p_
  - _foreach_log2
  - _foreach_log2_
  - _foreach_neg
  - _foreach_neg_
  - _foreach_tan
  - _foreach_tan_
  - _foreach_tanh
  - _foreach_tanh_
  - _foreach_sin
  - _foreach_sin_
  - _foreach_sinh
  - _foreach_sinh_
  - _foreach_round
  - _foreach_round_
  - _foreach_lgamma
  - _foreach_lgamma_
  - _foreach_frac
  - _foreach_frac_
  - _foreach_reciprocal
  - _foreach_reciprocal_
  - _foreach_sigmoid
  - _foreach_sigmoid_
  - _foreach_trunc
  - _foreach_trunc_
  - _foreach_addcdiv_.Scalar
  - _foreach_addcmul_.Scalar
  - _foreach_addcdiv_.ScalarList
  - _foreach_addcmul_.ScalarList
  - _foreach_addcdiv.Scalar
  - _foreach_addcmul.Scalar
  - _foreach_addcdiv.ScalarList
  - _foreach_addcmul.ScalarList
  - _foreach_maximum.List
  - _foreach_minimum.List
  - _linalg_inv_out_helper_
  - _linalg_qr_helper
  - _logcumsumexp
  - _logcumsumexp.out
  - _pdist_backward
  - _sample_dirichlet
  - _standard_gamma_grad
  - _standard_gamma
  - _test_optional_intlist
  - _test_optional_filled_intlist
  - _test_optional_floatlist
  - _solve_helper
  - angle
  - angle.out
  - binomial
  - bucketize.Tensor
  - bucketize.Tensor_out
  - bucketize.Scalar
  - cauchy_
  - cholesky_inverse
  - cholesky_inverse.out
  - complex.out
  - exponential_
  - geometric_
  - linalg_vector_norm
  - linalg_vector_norm.out
  - log_normal_
  - logit
  - logit_
  - logit.out
  - multilabel_margin_loss_backward.grad_input
  - multilabel_margin_loss_backward
  - multi_margin_loss.out
  - multi_margin_loss
  - multi_margin_loss_backward.grad_input
  - multi_margin_loss_backward
  - mode
  - nanmedian.dim_values
  - nansum
  - nansum.dim_IntList
  - nansum.IntList_out
  - narrow_copy.out
  - poisson
  - polar.out
  - replication_pad3d_backward.grad_input
  - replication_pad3d_backward
  - repeat_interleave.Tensor
  - slow_conv_dilated3d
  - slow_conv_transpose3d.out
  - slow_conv_transpose3d
  - sspaddmm.out
  - tensordot.out
  - to_sparse.sparse_dim
  - to_sparse
  - to_mkldnn
  - unfold_backward
  - unique_dim
  - unique_dim_consecutive
  - vdot
  - lcm.out
  - fmin
  - fmin.out
  - fmax
  - fmax.out
  - copysign.Tensor
  - copysign_.Tensor
  - copysign.out
  - copysign.Scalar
  - copysign_.Scalar
  - matrix_exp
  - _empty_affine_quantized
  - _empty_per_channel_affine_quantized
  - lgamma.out
  - lgamma_
  - lgamma
  - digamma.out
  - digamma
  - polygamma.out
  - igamma.out
  - igamma
  - igamma_
  - igammac.out
  - igammac
  - igammac_
  - i0.out
  - hypot.out
  - hypot
  - nextafter.out
  - nextafter
  - batch_norm_update_stats
  - heaviside.out
  - scatter_.reduce
  - scatter_.value_reduce
  - digamma_
  - adaptive_max_pool3d.out
  - adaptive_max_pool3d
  - adaptive_max_pool3d_backward.grad_input
  - adaptive_max_pool3d_backward
  - fractional_max_pool3d.output
  - fractional_max_pool3d
  - fractional_max_pool3d_backward.grad_input
  - fractional_max_pool3d_backward
  - fractional_max_pool2d.output
  - fractional_max_pool2d
  - fractional_max_pool2d_backward.grad_input
  - fractional_max_pool2d_backward
  - replication_pad3d.out
  - replication_pad3d
  - logit_backward.grad_input
  - logit_backward
  - orgqr.out
  - orgqr
  - _lu_with_info

unsupported:
  - _conj
  - conj
  - bitwise_left_shift.Tensor
  - bitwise_left_shift_.Tensor
  - bitwise_left_shift.Tensor_out
  - bitwise_left_shift.Tensor_Scalar
  - bitwise_left_shift_.Tensor_Scalar
  - bitwise_left_shift.Tensor_Scalar_out
  - bitwise_left_shift.Scalar_Tensor
  - bitwise_right_shift.Tensor
  - bitwise_right_shift_.Tensor
  - bitwise_right_shift.Tensor_out
  - bitwise_right_shift.Tensor_Scalar
  - bitwise_right_shift_.Tensor_Scalar
  - bitwise_right_shift.Tensor_Scalar_out
  - bitwise_right_shift.Scalar_Tensor
  - _conj_physical
  - conj_physical
  - conj_physical.out
  - conj_physical_
  - frexp.Tensor
  - frexp.Tensor_out
  - isin.Tensor_Tensor_out
  - isin.Tensor_Tensor
  - cholesky.out
  - cholesky
  - geqrf.a
  - geqrf
  - logdet
  - linalg_lu_factor_ex
  - linalg_lu_factor_ex.out
  - lu_solve.out
  - lu_solve
  - lu_unpack
  - lu_unpack.out
  - _det_lu_based_helper
  - linalg_det
  - linalg_det.out
  - linalg_cholesky_ex
  - linalg_cholesky_ex.L
  - linalg_eig
  - linalg_eig.out
  - linalg_eigh
  - linalg_eigh.eigvals
  - linalg_eigvalsh
  - linalg_eigvalsh.out
  - linalg_solve_triangular.out
  - linalg_solve_triangular
  - linalg_lstsq
  - linalg_lstsq.out
  - special_entr
  - special_entr.out
  - special_erfcx
  - special_erfcx.out
  - special_zeta
  - special_zeta.self_scalar
  - special_zeta.other_scalar
  - special_zeta.out
  - special_zeta.self_scalar_out
  - special_zeta.other_scalar_out

custom:
  - func: _npu_storage_resize(Tensor self, int size) -> Tensor
    wrap_impl: _npu_storage_resize
  - func: npu_change_data_ptr(Tensor dst, Tensor src, int index) -> int
    wrap_impl: npu_change_data_ptr
  - func: npu_transpose(Tensor self, int[] perm, bool require_contiguous=True) -> Tensor
    wrap_impl: npu_transpose
  - func: npu_transpose.out(Tensor self, int[] perm, bool require_contiguous=True, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_transpose_out
  - func: npu_broadcast(Tensor self, int[] size) -> Tensor
    wrap_impl: npu_broadcast
  - func: npu_broadcast.out(Tensor self, int[] size, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_broadcast_out
  - func: npu_dtype_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
    wrap_impl: npu_dtype_cast_
  - func: npu_alloc_float_status(Tensor self) -> Tensor
    wrap_impl: npu_alloc_float_status
  - func: npu_get_float_status(Tensor self) -> Tensor
    wrap_impl: npu_get_float_status
  - func: npu_clear_float_status(Tensor self) -> Tensor
    wrap_impl: npu_clear_float_status
  - func: one_(Tensor(a!) self) -> Tensor(a!)
    wrap_impl: one_
  - func: fast_gelu(Tensor self) -> Tensor
    wrap_impl: fast_gelu
  - func: npu_fast_gelu_backward(Tensor grad, Tensor self) -> Tensor
    wrap_impl: npu_fast_gelu_backward
  - func: _amp_foreach_non_finite_check_(Tensor[] scaled_grads) -> bool
    wrap_impl: _amp_foreach_non_finite_check_
  - func: npu_sign_bits_pack(Tensor self, int size) -> Tensor
    wrap_impl: npu_sign_bits_pack
  - func: npu_bert_apply_adam(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0) -> (Tensor var, Tensor m, Tensor v)
    wrap_impl: npu_bert_apply_adam
  - func: npu_bert_apply_adam.out(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
    wrap_impl: npu_bert_apply_adam_out
  - func: npu_conv_transpose2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_conv_transpose2d_backward
  - func: npu_conv_transpose3d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_conv_transpose3d_backward
  - func: npu_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_convolution_backward
  - func: npu_conv_transpose2d(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
    wrap_impl: npu_conv_transpose2d
  - func: npu_conv2d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
    wrap_impl: npu_conv2d
  - func: npu_conv2d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_conv2d_out
  - func: npu_conv2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_conv2d_backward
  - func: npu_conv3d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
    wrap_impl: npu_conv3d
  - func: npu_conv3d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_conv3d_out
  - func: npu_conv3d_backward(Tensor input, Tensor grad, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_conv3d_backward
  - func: get_npu_format(Tensor self) -> int
  - func: npu_format_cast.Tensor(Tensor self, Tensor dst) -> Tensor
  - func: npu_format_cast_.acl_format(Tensor(a!) self, int acl_format) -> Tensor(a!)
    wrap_impl: npu_format_cast_
  - func: npu_format_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)
    wrap_impl: npu_format_cast_
  - func: empty_with_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
    wrap_impl: empty_with_format
  - func: unsafe_empty_with_format(int[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2, bool keep_format=False) -> Tensor
    wrap_impl: unsafe_empty_with_format
  - func: empty_with_format.names(int[] size, Dimname[]? names, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, int acl_format=2) -> Tensor
    wrap_impl: empty_with_format
  - func: copy_memory_(Tensor(a!) self, Tensor src, bool non_blocking=False) -> Tensor(a!)
    wrap_impl: copy_memory_
  - func: npu_stride_add(Tensor self, Tensor other, Scalar offset1, Scalar offset2, Scalar c1_len) -> Tensor
    wrap_impl: npu_stride_add
  - func: npu_slice(Tensor self, int[] offsets, int[] size) -> Tensor
    wrap_impl: npu_slice
  - func: npu_slice.out(Tensor self, int[] offsets, int[] size, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_slice_out
  - func: npu_indexing(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0) -> Tensor
    wrap_impl: npu_indexing
  - func: npu_indexing.out(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_indexing_out
  - func: npu_softmax_cross_entropy_with_logits_backward(Tensor grad, Tensor self, Tensor labels) -> Tensor
    wrap_impl: npu_softmax_cross_entropy_with_logits_backward
  - func: npu_stride_copy(Tensor self, int[] shape, int[] stride, Scalar storage_offset) -> Tensor
    wrap_impl: npu_stride_copy
  - func: npu_stride_copy.out(Tensor self, int[] shape, int[] stride, Scalar storage_offset, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_stride_copy_out
  - func: npu_roi_align(Tensor self, Tensor rois, float spatial_scale, int pooled_height, int pooled_width, int sample_num, int roi_end_mode) -> Tensor
    wrap_impl: npu_roi_align
  - func: npu_roi_alignbk(Tensor self, Tensor rois, int[] xdiff_shape, int pooled_width, int pooled_height, float spatial_scale, int sample_num, int? roi_end_mode=None) -> Tensor
    wrap_impl: npu_roi_alignbk
  - func: npu_ps_roi_pooling_backward(Tensor output_grad, Tensor rois, float spatial_scale, int group_size, int output_dim, int[] input_size) -> Tensor
    wrap_impl: npu_ps_roi_pooling_backward
  - func: npu_sort_v2.out(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_sort_v2_out
  - func: npu_sort_v2(Tensor self, int dim=-1, bool descending=False) -> Tensor
    wrap_impl: npu_sort_v2
  - func: npu_confusion_transpose_backward(Tensor grad, int[] perm, int[] shape, bool transpose_first) -> Tensor
    wrap_impl: npu_confusion_transpose_backward
  - func: npu_one_hot(Tensor self, int num_classes=-1, int depth=1, Scalar on_value=1, Scalar off_value=0) -> Tensor
    wrap_impl: npu_one_hot
  - func: npu_linear_backward(Tensor grad, Tensor input, Tensor weight) -> (Tensor, Tensor)
    wrap_impl: npu_linear_backward
  - func: npu_anchor_response_flags(Tensor self, int[2] featmap_size, int[2] stride, int num_base_anchors) -> Tensor
    wrap_impl: npu_anchor_response_flags
  - func: npu_dropout_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
    wrap_impl: npu_dropout_backward
  - func: npu_nms_rotated(Tensor self, Tensor scores, float iou_threshold, float scores_threshold=0, int max_output_size=-1, int mode=0) -> (Tensor, Tensor)
    wrap_impl: npu_nms_rotated
  - func: npu_masked_fill_range(Tensor self, Tensor start, Tensor end, Tensor value, int axis=-1) -> Tensor
    wrap_impl: npu_masked_fill_range
  - func: npu_sub_sample(Tensor self, int per_images, float positive_fraction) -> Tensor
    wrap_impl: npu_sub_sample
  - func: npu_yolo_boxes_encode(Tensor self, Tensor gt_bboxes, Tensor stride, bool performance_mode=False) -> Tensor
    wrap_impl: npu_yolo_boxes_encode
  - func: npu_scatter(Tensor self, Tensor indices, Tensor updates, int dim) -> Tensor
    wrap_impl: npu_scatter
  - func: npu_layer_norm_eval(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05) -> Tensor
    wrap_impl: npu_layer_norm_eval
  - func: npu_max_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim=False) -> Tensor
    wrap_impl: npu_max_backward
  - func: npu_rotated_box_encode(Tensor self, Tensor gt_bboxes, Tensor weight) -> Tensor
    wrap_impl: npu_rotated_box_encode
  - func: npu_rotated_box_decode(Tensor self, Tensor deltas, Tensor weight) -> Tensor
    wrap_impl: npu_rotated_box_decode
  - func: npu_rotated_overlaps(Tensor self, Tensor query_boxes, bool trans=False) -> Tensor
    wrap_impl: npu_rotated_overlaps
  - func: npu_silu_backward(Tensor grad_output, Tensor x0, Tensor x1) -> Tensor
    wrap_impl: npu_silu_backward
  - func: npu_rotated_iou(Tensor self, Tensor query_boxes, bool trans=False, int mode=0, bool is_cross=True, float v_threshold=0.0, float e_threshold=0.0) -> Tensor
    wrap_impl: npu_rotated_iou
  - func: npu_nms_with_mask(Tensor input, Scalar iou_threshold) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_nms_with_mask
  - func: npu_gru_backward(Tensor? grady, Tensor? gradh, Tensor input, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, Tensor hx, Tensor y_output, Tensor h_output, Tensor output_updata, Tensor output_reset, Tensor output_new, Tensor hidden_new) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_gru_backward
  - func: npu_mish_backward(Tensor grad, Tensor input) -> Tensor
    wrap_impl: npu_mish_backward
  - func: npu_min_backward(Tensor grad, int dim, Tensor indices, int[] sizes, bool keepdim=False) -> Tensor
    wrap_impl: npu_min_backward
  - func: npu_reshape(Tensor self, int[] shape, bool can_refresh=False) -> Tensor
    wrap_impl: npu_reshape
  - func: npu_reshape.out(Tensor self, int[] shape, bool can_refresh=False, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_reshape_out
  - func: npu_batch_nms(Tensor self, Tensor scores, float score_threshold, float iou_threshold, int max_size_per_class, int max_total_size, bool change_coordinate_frame=False, bool transpose_box=False) -> (Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_batch_nms
  - func: npu_bounding_box_encode(Tensor anchor_box, Tensor ground_truth_box, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3) -> Tensor
    wrap_impl: npu_bounding_box_encode
  - func: npu_bounding_box_decode(Tensor rois, Tensor deltas, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3, int[1] max_shape, float wh_ratio_clip) -> Tensor
    wrap_impl: npu_bounding_box_decode
  - func: npu_apply_adam(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_apply_adam
  - func: npu_apply_adam.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
    wrap_impl: npu_apply_adam_out
  - func: npu_apply_adam_w(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_apply_adam_w
  - func: npu_apply_adam_w.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))
    wrap_impl: npu_apply_adam_w_out
  - func: npu_deformable_conv2dbk(Tensor input, Tensor grad_output, Tensor offset_out, Tensor weight, Tensor offset, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_deformable_conv2dbk
  - func: npu_giou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)
    wrap_impl: npu_giou_backward
  - func: npu_diou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)
    wrap_impl: npu_diou_backward
  - func: npu_iou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor
    wrap_impl: npu_iou
  - func: npu_nms_v4(Tensor self, Tensor scores, Scalar max_output_size, Tensor iou_threshold, Tensor scores_threshold, bool pad_to_max_output_size=False) -> (Tensor, Tensor)
    wrap_impl: npu_nms_v4
  - func: npu_pad(Tensor input, int[] paddings) -> Tensor
    wrap_impl: npu_pad
  - func: npu_random_choice_with_mask(Tensor x, int count=256, int seed=0, int seed2=0) -> (Tensor, Tensor)
    wrap_impl: npu_random_choice_with_mask
  - func: npu_normalize_batch(Tensor self, Tensor seq_len, int normalize_type=0) -> Tensor
    wrap_impl: npu_normalize_batch
  - func: npu_ptiou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor
    wrap_impl: npu_ptiou
  - func: npu_lstm_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor weight, Tensor bias, Tensor hx, Tensor cx, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_lstm_backward
  - func: _dropout_with_byte_mask_backward(Tensor grad_output, Tensor mask, float p) -> Tensor
    wrap_impl: _dropout_with_byte_mask_backward
  - func: dropout_with_byte_mask(Tensor self, float p, bool train) -> Tensor
    wrap_impl: dropout_with_byte_mask
  - func: npu_dropout_with_add_softmax_backward(Tensor grad, Tensor mask, Tensor softmax_out, Scalar alpha, float prob, int dim) -> (Tensor, Tensor)
    wrap_impl: npu_dropout_with_add_softmax_backward
  - func: npu_multi_head_attention_backward(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor query_res, Tensor key_res, Tensor value_res, Tensor attn_scores, Tensor attn_res, Tensor context, Tensor y_grad, Tensor dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_multi_head_attention_backward
  - func: npu_dropout_gen_mask(int[] size, float p, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor
    wrap_impl: npu_dropout_gen_mask
  - func: npu_ciou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, Tensor? atan_sub, bool trans=False, bool is_cross=True, int mode=0) -> (Tensor, Tensor)
    wrap_impl: npu_ciou_backward
  - func: npu_sign_bits_unpack(Tensor input, int size, ScalarType dtype) -> Tensor
    wrap_impl: npu_sign_bits_unpack
  - func: decode_jpeg(Tensor self, int[] image_shape, int channels=3, bool try_recover_truncated=False) -> Tensor
    wrap_impl: decode_jpeg
  - func: crop_and_resize(Tensor self, float[]? boxes, int[] box_index, int[] crop_size, float extrapolation_value=0, str method="bilinear") -> Tensor
    wrap_impl: crop_and_resize
  - func: reverse(Tensor self, int[] axis) -> Tensor
    wrap_impl: reverse
  - func: image_normalize(Tensor self, float[]? mean, float[]? variance, int dtype=0) -> Tensor
    wrap_impl: image_normalize
  - func: image_normalize_(Tensor(a!) self, float[]? mean, float[]? variance, int dtype=0) -> Tensor(a!)
    wrap_impl: image_normalize_
  - func: img_to_tensor(Tensor self) -> Tensor
    wrap_impl: img_to_tensor
  - func: _conv_depthwise2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)
    wrap_impl: _conv_depthwise2d_backward
  - func: slow_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
    wrap_impl: slow_conv_dilated2d_backward
  - func: slow_conv_transpose2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)
    wrap_impl: slow_conv_transpose2d_backward
  - func: npu_lstm_cell_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_lstm_cell_backward
  - func: batch_norm_reduce(Tensor input, float eps) -> (Tensor, Tensor)
    wrap_impl: batch_norm_reduce
  - func: batch_norm_gather_stats_update(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)
    wrap_impl: batch_norm_gather_stats_update
  - func: npu_fused_attention_score_backward(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_fused_attention_score_backward
  - func: npu_flash_attention_grad(Tensor query, Tensor key, Tensor value, Tensor dy, int head_num, str input_layout, *, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? softmax_max=None, Tensor? softmax_sum=None, Tensor? softmax_in=None, Tensor? attention_in=None, float scale_value=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, bool gen_mask_parallel=True, bool sync=False) -> Tensor[]
    wrap_impl: npu_flash_attention_grad
  - func: npu_fused_attention_score_fwd(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_fused_attention_score_fwd
  - func: npu_fused_attention_score_grad(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_fused_attention_score_grad
  - func: npu_fused_attention_qkv_grad(Tensor grad_output_query, Tensor grad_output_key, Tensor grad_output_value, Tensor query_kernel, Tensor key_kernel, Tensor value_kernel, Tensor hidden_states, Tensor grad_output_ln) -> Tensor[]
    wrap_impl: npu_fused_attention_qkv_grad
  - func: npu_fused_attention_layernorm_qkv_fwd(Tensor x, Tensor kernel_query, Tensor kernel_key, Tensor kernel_value, Tensor gamma, Tensor beta, Tensor? bias_query=None, Tensor? bias_key=None, Tensor? bias_value=None, int seq_len=128, int num_heads=12, float eps=1e-05) -> Tensor[]
    wrap_impl: npu_fused_attention_layernorm_qkv_fwd
  - func: npu_layernorm_grad(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_layernorm_grad
  - func: format_contiguous(Tensor self) -> Tensor
    wrap_impl: format_contiguous
  - func: check_match(Tensor self) -> bool
    wrap_impl: check_match
  - func: check_memory_overlaps(Tensor[] inputs, Tensor[] outputs) -> ()
    wrap_impl: check_memory_overlaps
  - func: npu_ifmr(Tensor data, Tensor data_min, Tensor data_max, Tensor cumsum, float min_percentile, float max_percentile, float search_start, float search_end, float search_step, bool with_offset) -> (Tensor, Tensor)
    wrap_impl: npu_ifmr
  - func: npu_grid_assign_positive(Tensor self, Tensor overlaps, Tensor box_responsible_flags, Tensor max_overlaps, Tensor argmax_overlaps, Tensor gt_max_overlaps, Tensor gt_argmax_overlaps, int num_gts, float pos_iou_thr, float min_pos_iou, bool gt_max_assign_all) -> Tensor
    wrap_impl: npu_grid_assign_positive
  - func: get_storage_size(Tensor self) -> int
    wrap_impl: get_storage_size
  - func: npu_hcom_allreduce(Tensor self, str reduction, str group, int fusion, int fusion_id, float alpha, float beta, int? hccl_comm) -> Tensor
    wrap_impl: npu_hcom_allreduce
  - func: npu_hcom_allreduce.out(Tensor self, str reduction, str group, int fusion, int fusion_id, float alpha, float beta, int? hccl_comm, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_hcom_allreduce_out
  - func: npu_hcom_allgather(Tensor self, int rank_size, str group, float alpha, float beta, int? hccl_comm) -> Tensor
    wrap_impl: npu_hcom_allgather
  - func: npu_hcom_allgather.out(Tensor self, int rank_size, str group, float alpha, float beta, int? hccl_comm, *, Tensor(a!) out) -> Tensor(a!)
    wrap_impl: npu_hcom_allgather_out
  - func: bscpp_add(Tensor self, Tensor other) -> Tensor
    wrap_impl: bscpp_add
    bscpp_op: True
  - func: npu_rotary_mul(Tensor self, Tensor r1, Tensor r2) -> Tensor
    wrap_impl: npu_rotary_mul
  - func: npu_rotary_mul_backward(Tensor grad, Tensor self, Tensor r1, Tensor r2) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_rotary_mul_backward
  - func: npu_lstm_cell(Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor? bias=None) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_lstm_cell
  - func: npu_multi_head_attention(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor attn_mask, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor? dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_multi_head_attention
  - func: npu_lstm(Tensor input, Tensor weight, Tensor bias, Tensor seq_mask, Tensor h, Tensor c, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, bool flag_seq, bool direction) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_lstm
  - func: npu_gru(Tensor input, Tensor hx, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_gru
  - func: npu_lstm_data(Tensor input, Tensor batch_sizes, Tensor weight, Tensor bias, Tensor seq_mask, Tensor h, Tensor c, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, bool flag_seq, bool direction) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_lstm_data
  - func: npu_lstm_data_backward(Tensor? grady_opt, Tensor? gradh_opt, Tensor? gradc_opt, Tensor input, Tensor batch_sizes, Tensor weight, Tensor bias, Tensor init_h, Tensor init_c, Tensor y, Tensor h, Tensor c, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc, bool flag_direction) -> (Tensor, Tensor, Tensor, Tensor, Tensor)
    wrap_impl: npu_lstm_data_backward
  - func: _npu_ciou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -> (Tensor, Tensor)
    wrap_impl: _npu_ciou
  - func: npu_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor
    wrap_impl: npu_convolution
  - func: npu_convolution_transpose(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor
    wrap_impl: npu_convolution_transpose
  - func: npu_confusion_transpose(Tensor self, int[] perm, int[] shape, bool transpose_first) -> Tensor
    wrap_impl: npu_confusion_transpose
  - func: npu_ps_roi_pooling(Tensor self, Tensor rois, float spatial_scale, int group_size, int output_dim) -> Tensor
    wrap_impl: npu_ps_roi_pooling
  - func: npu_linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor
    wrap_impl: npu_linear
  - func: _npu_dropout(Tensor self, float p) -> (Tensor, Tensor)
    wrap_impl: _npu_dropout
  - func: npu_softmax_cross_entropy_with_logits(Tensor self, Tensor labels) -> Tensor
    wrap_impl: npu_softmax_cross_entropy_with_logits
  - func: npu_max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    wrap_impl: npu_max
  - func: npu_max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    wrap_impl: npu_max
  - func: npu_format_cast(Tensor self, int acl_format) -> Tensor
    wrap_impl: npu_format_cast
  - func: npu_bmmV2(Tensor self, Tensor mat2, int[] output_sizes) -> Tensor
    wrap_impl: npu_bmmV2
  - func: npu_dtype_cast(Tensor self, ScalarType dtype) -> Tensor
    wrap_impl: npu_dtype_cast
  - func: npu_silu(Tensor self) -> Tensor
    wrap_impl: npu_silu
  - func: npu_silu_(Tensor(a!) self) -> Tensor(a!)
    wrap_impl: npu_silu_
  - func: npu_mish(Tensor self) -> Tensor
    wrap_impl: npu_mish
  - func: npu_min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    wrap_impl: npu_min
  - func: npu_min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)
    wrap_impl: npu_min
  - func: npu_deformable_conv2d(Tensor input, Tensor weight, Tensor offset, Tensor? bias, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor)
    wrap_impl: npu_deformable_conv2d
  - func: npu_giou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
    wrap_impl: npu_giou
  - func: npu_diou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor
    wrap_impl: npu_diou
  - func: _dropout_with_byte_mask(Tensor self, float p) -> (Tensor, Tensor)
    wrap_impl: _dropout_with_byte_mask
  - func: npu_dropout_with_add_softmax(Tensor self, Tensor x1, Scalar alpha, float prob, int dim) -> (Tensor, Tensor, Tensor)
    wrap_impl: npu_dropout_with_add_softmax
  - func: npu_scaled_masked_softmax(Tensor x, Tensor mask, Scalar scale=1, bool fixed_triu_mask=False) -> Tensor
    wrap_impl: npu_scaled_masked_softmax
  - func: npu_dropout_do_mask(Tensor self, Tensor mask, float p) -> (Tensor, Tensor)
    wrap_impl: npu_dropout_do_mask
  - func: npu_ciou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -> Tensor
    wrap_impl: npu_ciou
  - func: npu_fused_attention_score(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> Tensor
    wrap_impl: npu_fused_attention_score

custom_autograd:
  - func: npu_flash_attention(Tensor query, Tensor key, Tensor value, int head_num, str input_layout, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, float scale=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, bool gen_mask_parallel=True, bool sync=False) -> Tensor[]
