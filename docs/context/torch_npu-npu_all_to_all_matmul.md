# torch_npu.npu_all_to_all_matmul

## 产品支持情况

| 产品                                                         | 是否支持 |
| :----------------------------------------------------------- | :------: |
| <term>昇腾910_95 AI处理器</term>                             |    √     |
| <term>Atlas A3 训练系列产品/Atlas A3 推理系列产品</term>     |    ×     |
| <term>Atlas A2 训练系列产品/Atlas 800I A2 推理产品/A200I A2 Box 异构组件</term> |    √     |
| <term>Atlas 200I/500 A2 推理产品</term>                      |    ×     |
| <term>Atlas 推理系列产品</term>                             |    ×     |
| <term>Atlas 训练系列产品</term>                              |    ×     |
| <term>Atlas 200/300/500 推理产品</term>                      |    ×     |
## 功能说明

- API功能：完成AlltoAll通信、Permute(保证通信后地址连续)和Matmul计算的融合，**先通信后计算**。
- 计算公式:
  假设x1输入shape为(BS, H)
  $$
  commOut = AlltoAll(x1.view(worldSize, BS/worldSize, H)) \\
  permutedOut = commOut.permute(1, 0, 2).view(BS/worldSize, worldSize*H) \\
  output = permutedOut @ x2 + bias \\
  $$

## 函数原型

```
torch_npu.npu_all_to_all_matmul(Tensor x1, Tensor x2, str hcom, int world_size, Tensor? bias=None, int[]? all2all_axes=None, bool all2all_out_flag=True) -> Tensor
```

## 参数说明

- **x1** (`Tensor`)：必选参数，融合算子的左矩阵输入，对应公式中的 `x1`。  
  该输入进行 AlltoAll 通信与 Permute 操作后，结果作为 MatMul 计算的左矩阵输入。
  - 数据格式支持 $ND$，暂不支持非连续的 Tensor。
  - 维数为 2 维，shape 为 `(BS, H)`。
  - 数据类型支持 `FLOAT16`、`BFLOAT16`。

- **x2** (`Tensor`)：必选参数，融合算子的右矩阵输入，也是 MatMul 计算的右矩阵。  
  直接作为 MatMul 计算的右矩阵输入。
  - 数据格式支持 $ND$，暂不支持非连续的 Tensor。
  - 维数为 2 维，shape 为 `(H * worldSize, N)`。
  - 数据类型支持 `FLOAT16`、`BFLOAT16`。

- **hcom** (`string`)：必选参数，通信域名。
  - 字符串长度要求为 `(0, 128)`。
  - 数据类型为 `STRING`。

- **world_size** (`int`)：必选参数，通信域内的rank总数。
  - 支持值为 `2`、`4`、`8`、`16`。
  - 数据类型为 `INT`。

- **bias** (`Tensor`)：可选参数，阵乘运算后累加的偏置，对应公式中的 `bias`。
  - 数据类型要求：需与输入 `x1`、`x2` 保持一致，或为 `FLOAT32`。
  - 数据格式支持 $ND$，暂不支持非连续的 Tensor。
  - 维数为 1 维，shape 为 `(N)`。
  - 数据类型支持 `FLOAT16`、`BFLOAT16`、`FLOAT32`。

- **all2all_axes** (`List[int]`)：可选参数，AlltoAll 和 Permute 数据交换的方向。
  - 支持配置为空或 `[-2, -1]`，传入空时默认按 `[-2, -1]` 处理，表示将输入由 `(BS, H)` 转为 `(BS / worldSize, worldSize * H)`。
  - 维数为 1 维，shape 为 `(2)`。
  - 数据类型为 `List[int]`。

- **all2all_out_flag** (`bool`)：可选参数，标识 AlltoAll 通信结果是否保留，默认为True。
  - 当配置为 `True` 时，会保存 AlltoAll 通信结果，其 shape 为 `(BS / worldSize, worldSize * H)`。
  - 数据类型为 `bool`。


## 返回值说明
`(Tensor, Tensor)`

第一个Tensor代表`npu_all_to_all_matmul`的计算结果，对应公式中的output，shape为`(BS / worldSize, H * worldSize)`，数据类型与输入x1和x2一致。
第二个Tensor代表`alltoall`的通信结果，对应公式中的permutedOut，shape为`(BS / worldSize, worldSize * H)`，数据类型和输入x1一致。

## 约束说明
* 默认支持确定性计算
* 参数说明中shape使用的变量BS必须整除worldSize
* x1、x2的数据类型必须一致
* bias的数据类型可以为x1和x2的数据类型，也可以为float32
* 通算融合算子不支持并发调用，不同的通算融合算子也不支持并发调用
* 不支持跨超节点通信，只支持超节点内

## 调用示例
- 单算子模式调用
  ```python
  import torch
  import torch_npu
  
  # 初始化输入
  x1 = torch.randint(-1, 2, (16, 32), dtype=torch.float16).npu()
  x2 = torch.randint(-1, 2, (32, 32), dtype=torch.float16).npu()
  bias = torch.randint(-1, 2, (32,), dtype=torch.float16).npu()
  # 其他参数
  hcom = "fake group info"
  world_size = 2
  # 调用MatmulAlltoAll算子
  res = torch_npu.npu_all_to_all_matmul(x1, x2, hcom, world_size, bias=bias, all2all_axes=[-2,-1])
  # 输出res的形状类似于
  tensor([[-1.0321e+00,  6.2289e-01,  1.4179e-01,  1.1582e+00, -1.8747e-01,
            2.3165e+00, -3.4849e-01, -4.4494e-01,  1.1038e+00, -1.5604e+00,
            6.9800e-01,  1.6987e+00, -8.8242e-01, -1.0305e+00, -1.6537e+00,
            2.7214e+00, -5.1618e-01, -1.8200e-01,  1.8188e+00,  7.0063e-01,
           -8.0380e-01, -9.7834e-01,  7.0777e-01, -4.7582e-02, -2.5839e-01,
            1.3785e-01,  1.7807e+00,  7.9090e-02, -8.9851e-01, -4.9518e-01,
           -1.1189e+00, -8.6247e-01],
          [-1.3052e+00,  1.1661e-01, -1.3945e+00,  7.4178e-01, -9.5440e-01,
            5.0529e-02, -2.0327e+00, -1.8708e-01, -1.2583e+00, -1.9664e-01,
            4.3981e-01,  1.2337e-01, -4.5290e-01,  1.7786e+00, -7.9361e-01,
           -7.8097e-01, -1.3808e+00,  1.6942e+00, -1.1622e+00,  6.3309e-01,
            7.2297e-01,  8.0443e-01, -3.1231e-01, -2.6240e-02, -1.9015e+00,
            2.4308e-01, -3.7747e-02,  4.2124e-01,  7.4770e-01, -1.7556e+00,
            9.7620e-01,  3.2943e-01],
          [-6.9184e-01,  3.0648e-01,  2.6679e-01, -2.7126e-01, -2.3982e+00,
           -3.7244e-01, -3.3400e-01, -1.1101e-01,  9.1360e-01,  2.2194e+00,
           -7.2461e-02,  9.4988e-02,  5.3438e-01, -7.5926e-01, -6.2325e-01,
            1.2285e+00,  1.1827e+00,  6.7400e-01, -6.4264e-01, -1.4517e+00,
            3.2716e-01, -1.0468e+00,  6.2546e-01, -3.1762e+00,  9.7306e-01,
            1.8830e-01, -1.0312e+00, -2.2427e-01,  1.1527e+00, -6.1190e-01,
            8.3790e-01,  4.8933e-01],
          [ 1.6508e-02,  2.3683e+00,  6.4964e-01, -1.0908e+00, -4.6451e-01,
            8.3218e-01,  3.2308e-01, -4.6733e-01, -2.2168e+00,  2.0731e+00,
            7.9789e-01, -6.0773e-01,  4.0600e-01,  3.6759e-01,  6.1818e-01,
           -4.3798e-01, -2.8660e+00,  3.7701e-02,  2.8182e-01, -8.5863e-02,
           -1.1137e-02,  4.8926e-01,  1.0670e+00, -9.0587e-01,  1.6926e+00,
           -1.2680e+00,  6.7200e-01,  3.2650e-01,  1.6260e+00, -4.0430e-01,
           -1.3591e-01, -2.5671e-02],
          [ 1.1498e+00,  1.4875e-01,  3.7689e-01, -7.4506e-01,  9.6234e-01,
            2.0656e-01,  3.9347e-01, -5.7313e-01,  2.1519e-01,  4.8783e-01,
           -2.9576e-03,  1.5790e+00,  8.5187e-01,  1.4114e+00,  6.7917e-01,
            1.9349e+00, -9.8073e-01, -1.7973e-01, -7.6273e-01,  3.8086e-01,
           -6.9003e-01,  2.8973e-01, -2.0242e+00, -8.0495e-02, -6.5029e-01,
            1.5070e-01, -1.2915e+00, -7.5784e-01,  1.0086e+00,  5.1528e-01,
           -1.4057e+00,  2.3254e+00],
          [-9.7377e-01, -2.1214e-01,  1.2309e+00, -5.2911e-01, -4.7966e-01,
           -7.3101e-02, -9.3732e-01,  1.0623e+00, -1.0705e-01,  3.9100e-01,
           -9.5188e-02,  1.0662e+00, -4.1645e-01, -3.1083e-01, -1.4358e-01,
           -8.4679e-01,  1.1195e+00, -1.3657e+00,  9.5275e-01, -1.1579e+00,
            4.7903e-01,  1.0085e+00,  1.6431e+00,  1.1976e+00,  3.8353e-01,
            6.8370e-02, -8.3445e-01,  2.4800e+00, -9.7962e-01,  6.3613e-01,
            1.5859e-01, -7.1362e-01],
          [ 4.4330e-01,  2.9458e+00,  1.4342e+00,  8.7591e-03,  7.1487e-01,
            2.6754e-02,  7.6820e-01, -1.0667e+00,  1.1269e+00, -6.4609e-01,
           -7.7547e-01,  7.7234e-01,  2.5073e-01,  5.6560e-01, -5.9784e-01,
           -3.7445e-01, -4.1680e-01,  1.9751e+00, -4.9059e-01, -2.3908e-01,
           -8.5928e-01,  1.7069e+00, -1.1442e+00,  2.2217e+00,  3.5738e-01,
            6.8612e-01,  2.3320e+00, -1.9791e+00, -2.9414e-01, -1.0079e-01,
           -5.0897e-01, -5.1150e-01],
          [-8.3146e-02,  4.2937e-01,  7.8852e-01,  7.1765e-01, -6.6637e-02,
           -6.1625e-02, -1.3684e+00,  2.3891e-02,  2.2617e-02, -1.0367e+00,
           -2.5522e-02,  1.2772e-02,  5.5996e-01,  6.6306e-01, -1.9994e+00,
           -7.5787e-01,  5.2938e-01,  5.5434e-01, -9.0300e-01,  6.7917e-01,
            4.3156e-01,  7.3735e-01, -8.1547e-02,  1.5977e+00,  5.8575e-01,
            6.3603e-01, -3.4108e-01, -1.5309e+00, -2.4153e-01, -1.3761e+00,
           -9.1386e-01,  2.1079e-01]])
  ```

