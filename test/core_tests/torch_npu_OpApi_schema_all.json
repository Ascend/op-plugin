{  "func: repeat_interleave_backward_tensor(Tensor grad, Tensor self, Tensor repeats, int? dim=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: repeat_interleave_backward_int(Tensor grad, Tensor self, SymInt repeats, int? dim=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_view_copy(Tensor(a!) self, Tensor other, bool non_blocking) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_transpose(Tensor self, int[] perm, bool require_contiguous=True) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_transpose.out(Tensor self, int[] perm, bool require_contiguous=True, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_transpose_trans_contiguous.out(Tensor self, int[] perm, bool require_contiguous=True, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["v2.1"]
  },
  "func: npu_broadcast(Tensor self, int[] size) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_broadcast.out(Tensor self, int[] size, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_dtype_cast_(Tensor(a!) self, Tensor src) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_alloc_float_status(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_get_float_status(Tensor self, int mode=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_clear_float_status(Tensor self, int mode=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: one_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_fast_gelu(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_fast_gelu_backward(Tensor grad, Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _amp_foreach_non_finite_check(Tensor[] scaled_grads) -> bool": {
    "version": ["all_version"]
  },
  "func: npu_sign_bits_pack(Tensor self, int size) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_bert_apply_adam(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0) -> (Tensor var, Tensor m, Tensor v)": {
    "version": ["all_version"]
  },
  "func: npu_bert_apply_adam.out(Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Scalar max_grad_norm, Scalar global_grad_norm, Scalar weight_decay, Scalar? step_size=None, int adam_mode=0, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))": {
    "version": ["all_version"]
  },
  "func: npu_conv_transpose2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_conv_transpose3d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_convolution_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_conv_transpose2d(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_conv2d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_conv2d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_conv2d_backward(Tensor input, Tensor grad_output, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_conv3d(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_conv3d.out(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_conv3d_backward(Tensor input, Tensor grad, Tensor weight, int[] stride, int[] padding, int[] dilation, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_stride_add(Tensor self, Tensor other, Scalar offset1, Scalar offset2, Scalar c1_len) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_slice(Tensor self, int[] offsets, int[] size) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_slice.out(Tensor self, int[] offsets, int[] size, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_indexing(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_indexing.out(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_indexing_trans_contiguous.out(Tensor self, int[] begin, int[] end, int[] strides, int begin_mask=0, int end_mask=0, int ellipsis_mask=0, int new_axis_mask=0, int shrink_axis_mask=0, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["v2.1"]
  },
  "func: npu_softmax_cross_entropy_with_logits_backward(Tensor grad, Tensor self, Tensor labels) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_stride_copy(Tensor self, int[] shape, int[] stride, Scalar storage_offset) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_stride_copy.out(Tensor self, int[] shape, int[] stride, Scalar storage_offset, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_roi_align(Tensor self, Tensor rois, float spatial_scale, int pooled_height, int pooled_width, int sample_num, int roi_end_mode) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_group_norm_silu(Tensor input, Tensor? weight, Tensor? bias, int group, float eps=0.00001) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_apply_rotary_pos_emb(Tensor query, Tensor key, Tensor cos, Tensor sin, str layout='BSH') -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_roi_alignbk(Tensor self, Tensor rois, int[] xdiff_shape, int pooled_width, int pooled_height, float spatial_scale, int sample_num, int? roi_end_mode=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_sort_v2.out(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_sort_v2(Tensor self, int dim=-1, bool descending=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_one_hot(Tensor self, int num_classes=-1, int depth=1, Scalar on_value=1, Scalar off_value=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_linear_backward(Tensor grad, Tensor input, Tensor weight) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_anchor_response_flags(Tensor self, int[2] featmap_size, int[2] stride, int num_base_anchors) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_dropout_backward(Tensor grad_output, Tensor mask, float p) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_nms_rotated(Tensor self, Tensor scores, float iou_threshold, float scores_threshold=0, int max_output_size=-1, int mode=0) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_masked_fill_range(Tensor self, Tensor start, Tensor end, Tensor value, int axis=-1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_sub_sample(Tensor self, int per_images, float positive_fraction) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_yolo_boxes_encode(Tensor self, Tensor gt_bboxes, Tensor stride, bool performance_mode=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_scatter(Tensor self, Tensor indices, Tensor updates, int dim) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_layer_norm_eval(Tensor input, int[] normalized_shape, Tensor? weight=None, Tensor? bias=None, float eps=1e-05) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_rotated_box_encode(Tensor self, Tensor gt_bboxes, Tensor weight) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_rotated_box_decode(Tensor self, Tensor deltas, Tensor weight) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_rotated_overlaps(Tensor self, Tensor query_boxes, bool trans=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_silu_backward(Tensor grad_output, Tensor x0, Tensor x1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_rotated_iou(Tensor self, Tensor query_boxes, bool trans=False, int mode=0, bool is_cross=True, float v_threshold=0.0, float e_threshold=0.0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_nms_with_mask(Tensor input, Scalar iou_threshold) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_gru_backward(Tensor? grady, Tensor? gradh, Tensor input, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, Tensor hx, Tensor y_output, Tensor h_output, Tensor output_updata, Tensor output_reset, Tensor output_new, Tensor hidden_new) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_mish_backward(Tensor grad, Tensor input) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_reshape(Tensor self, int[] shape, bool can_refresh=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_reshape.out(Tensor self, int[] shape, bool can_refresh=False, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_batch_nms(Tensor self, Tensor scores, float score_threshold, float iou_threshold, int max_size_per_class, int max_total_size, bool change_coordinate_frame=False, bool transpose_box=False) -> (Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_bounding_box_encode(Tensor anchor_box, Tensor ground_truth_box, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_bounding_box_decode(Tensor rois, Tensor deltas, float means0, float means1, float means2, float means3, float stds0, float stds1, float stds2, float stds3, int[1] max_shape, float wh_ratio_clip) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_apply_adam(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_apply_adam.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, bool? use_locking, bool? use_nesterov, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))": {
    "version": ["all_version"]
  },
  "func: npu_apply_adam_w(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_apply_adam_w.out(Scalar beta1_power, Scalar beta2_power, Scalar lr, Scalar weight_decay, Scalar beta1, Scalar beta2, Scalar epsilon, Tensor grad, Tensor? max_grad_norm, bool? amsgrad, bool? maximize, *, Tensor(a!) var, Tensor(b!) m, Tensor(c!) v) -> (Tensor(a!), Tensor(b!), Tensor(c!))": {
    "version": ["all_version"]
  },
  "func: npu_deformable_conv2dbk(Tensor input, Tensor grad_output, Tensor offset_out, Tensor weight, Tensor offset, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_giou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_diou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_iou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_nms_v4(Tensor self, Tensor scores, Scalar max_output_size, Tensor iou_threshold, Tensor scores_threshold, bool pad_to_max_output_size=False) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_pad(Tensor input, int[] paddings) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_random_choice_with_mask(Tensor x, int count=256, int seed=0, int seed2=0) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_normalize_batch(Tensor self, Tensor seq_len, int normalize_type=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_ptiou(Tensor bboxes, Tensor gtboxes, int mode=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_geglu(Tensor self, int dim=-1, int approximate=1, bool activate_left=False) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_geglu_grad(Tensor grad_output, Tensor self, Tensor gelu, int dim=-1, int approximate=1, bool activate_left=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_dynamic_quant(Tensor input, *, Tensor? smooth_scales=None) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_lstm_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor weight, Tensor bias, Tensor hx, Tensor cx, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: _dropout_with_byte_mask_backward(Tensor grad_output, Tensor mask, float p) -> Tensor": {
    "version": ["all_version"]
  },
  "func: dropout_with_byte_mask(Tensor self, float p, bool train) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_dropout_with_add_softmax_backward(Tensor grad, Tensor mask, Tensor softmax_out, Scalar alpha, float prob, int dim) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_multi_head_attention_backward(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor query_res, Tensor key_res, Tensor value_res, Tensor attn_scores, Tensor attn_res, Tensor context, Tensor y_grad, Tensor dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_add_layer_norm(Tensor x1, Tensor x2, Tensor gamma, Tensor beta, float epsilon=1e-05, bool additional_output=False) -> (Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_add_rms_norm(Tensor x1, Tensor x2, Tensor gamma, float epsilon=1e-06) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_add_layer_norm_backward(Tensor? dy_opt, Tensor x1, Tensor x2, Tensor rstd, Tensor mean, Tensor gamma, Tensor? dsum_opt) -> (Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_rms_norm(Tensor self, Tensor gamma, float epsilon=1e-06) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_rms_norm_backward(Tensor dy, Tensor self, Tensor gamma, Tensor rstd) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_swiglu(Tensor self, int dim=-1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_swiglu_backward(Tensor grad_output, Tensor self, int dim=-1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_deep_norm(Tensor x, Tensor gx, Tensor beta, Tensor gamma, float alpha=0.3, float epsilon=1e-06) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_deep_norm_backward(Tensor dy, Tensor x, Tensor gx, Tensor gamma, Tensor mean, Tensor rstd, float alpha=0.3) -> (Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_dropout_gen_mask(int[] size, float p, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_ciou_backward(Tensor grad, Tensor bboxes, Tensor gtboxes, Tensor? atan_sub, bool trans=False, bool is_cross=True, int mode=0) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_sign_bits_unpack(Tensor input, int size, ScalarType dtype) -> Tensor": {
    "version": ["all_version"]
  },
  "func: decode_jpeg(Tensor self, int[] image_shape, int channels=3, bool try_recover_truncated=False) -> Tensor": {
    "version": ["v2.1"]
  },
  "func: crop_and_resize(Tensor self, float[]? boxes, int[] box_index, int[] crop_size, float extrapolation_value=0, str method=\"bilinear\") -> Tensor": {
    "version": ["v2.1"]
  },
  "func: reverse(Tensor self, int[] axis) -> Tensor": {
    "version": ["v2.1"]
  },
  "func: image_normalize(Tensor self, float[]? mean, float[]? variance, int dtype=0) -> Tensor": {
    "version": ["v2.1"]
  },
  "func: image_normalize_(Tensor(a!) self, float[]? mean, float[]? variance, int dtype=0) -> Tensor(a!)": {
    "version": ["v2.1"]
  },
  "func: img_to_tensor(Tensor self) -> Tensor": {
    "version": ["v2.1"]
  },
  "func: _conv_depthwise2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[2] output_mask) -> (Tensor grad_input, Tensor grad_weight)": {
    "version": ["all_version"]
  },
  "func: slow_conv_dilated2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)": {
    "version": ["all_version"]
  },
  "func: slow_conv_transpose2d_backward(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, int[2] output_padding, int[2] dilation, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)": {
    "version": ["all_version"]
  },
  "func: npu_lstm_cell_backward(Tensor? grady, Tensor? gradh, Tensor? gradc, Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor y_output, Tensor h_output, Tensor c_output, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: batch_norm_reduce(Tensor input, float eps) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: batch_norm_gather_stats_update(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_fused_attention_score_backward(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_fused_attention_score_fwd(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_fused_attention_score_grad(Tensor grad_output, Tensor softmax_output, Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool value_transpose=False, bool dx_transpose=False) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_fused_attention_qkv_grad(Tensor grad_output_query, Tensor grad_output_key, Tensor grad_output_value, Tensor query_kernel, Tensor key_kernel, Tensor value_kernel, Tensor hidden_states, Tensor grad_output_ln) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: npu_fused_attention_layernorm_qkv_fwd(Tensor x, Tensor kernel_query, Tensor kernel_key, Tensor kernel_value, Tensor gamma, Tensor beta, Tensor? bias_query=None, Tensor? bias_key=None, Tensor? bias_value=None, int seq_len=128, int num_heads=12, float eps=1e-05) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: npu_layernorm_grad(Tensor grad_out, Tensor input, int[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_ifmr(Tensor data, Tensor data_min, Tensor data_max, Tensor cumsum, float min_percentile, float max_percentile, float search_start, float search_end, float search_step, bool with_offset) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_grid_assign_positive(Tensor self, Tensor overlaps, Tensor box_responsible_flags, Tensor max_overlaps, Tensor argmax_overlaps, Tensor gt_max_overlaps, Tensor gt_argmax_overlaps, int num_gts, float pos_iou_thr, float min_pos_iou, bool gt_max_assign_all) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_rotary_mul_backward(Tensor grad, Tensor self, Tensor r1, Tensor r2) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, int groups) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_convolution_transpose(Tensor input, Tensor weight, Tensor? bias, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups) -> Tensor": {
    "version": ["all_version"]
  },
  "func: fast_gelu(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_confusion_transpose(Tensor self, int[] perm, int[] shape, bool transpose_first) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_ps_roi_pooling(Tensor self, Tensor rois, float spatial_scale, int group_size, int output_dim) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_linear(Tensor input, Tensor weight, Tensor? bias=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _npu_dropout(Tensor self, float p) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_softmax_cross_entropy_with_logits(Tensor self, Tensor labels) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: npu_max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: npu_bmmV2(Tensor self, Tensor mat2, int[] output_sizes) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_dtype_cast(Tensor self, ScalarType dtype) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_silu(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_silu_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_gru(Tensor input, Tensor hx, Tensor weight_input, Tensor weight_hidden, Tensor bias_input, Tensor bias_hidden, Tensor seq_length, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_mish(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: npu_min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: npu_deformable_conv2d(Tensor input, Tensor weight, Tensor offset, Tensor? bias, int[2] kernel_size, int[] stride, int[] padding, int[] dilation=[1,1,1,1], int groups=1, int deformable_groups=1, bool modulated=True) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_giou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_diou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=False, int mode=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_lstm(Tensor input, Tensor weight, Tensor bias, Tensor seq_mask, Tensor h, Tensor c, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, bool flag_seq, bool direction) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_lstm_data(Tensor input, Tensor batch_sizes, Tensor weight, Tensor bias, Tensor seq_mask, Tensor h, Tensor c, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first, bool flag_seq, bool direction) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: _dropout_with_byte_mask(Tensor self, float p) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_dropout_with_add_softmax(Tensor self, Tensor x1, Scalar alpha, float prob, int dim) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_scaled_masked_softmax(Tensor x, Tensor mask, Scalar scale=1, bool fixed_triu_mask=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_multi_head_attention(Tensor query, Tensor key, Tensor value, Tensor query_weight, Tensor key_weight, Tensor value_weight, Tensor attn_mask, Tensor out_proj_weight, Tensor? query_bias, Tensor? key_bias, Tensor? value_bias, Tensor? out_proj_bias, Tensor? dropout_mask, int attn_head_num, int attn_dim_per_head, int src_len, int tgt_len, float dropout_prob, bool softmax_use_float) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_fusion_attention(Tensor query, Tensor key, Tensor value, int head_num, str input_layout, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, float scale=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, int inner_precise=0, int[]? prefix=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None, int sparse_mode=0, bool gen_mask_parallel=True, bool sync=False) -> (Tensor, Tensor, Tensor, Tensor, int, int, int)": {
    "version": ["all_version"]
  },
  "func: npu_fusion_attention_grad(Tensor query, Tensor key, Tensor value, Tensor dy, int head_num, str input_layout, *, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? softmax_max=None, Tensor? softmax_sum=None, Tensor? softmax_in=None, Tensor? attention_in=None, float scale_value=1., float keep_prob=1., int pre_tockens=2147483647, int next_tockens=2147483647, int inner_precise=0, int seed=0, int offset=0, int numels=0, int[]? prefix=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None, int sparse_mode=0, bool gen_mask_parallel=True, bool sync=False) -> (Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_fusion_attention_v2(Tensor query, Tensor key, Tensor value, int head_num, str input_layout, *, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? query_rope=None, Tensor? key_rope=None, float scale=1., float keep_prob=1., int pre_tokens=2147483647, int next_tokens=2147483647, int inner_precise=0, int[]? prefix=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None, int sparse_mode=0, bool gen_mask_parallel=True, bool sync=False, int pse_type=1, int[]? q_start_idx=None, int[]? kv_start_idx=None) -> (Tensor, Tensor, Tensor, Tensor, int, int, int)": {
    "version": ["all_version"]
  },
  "func: npu_fusion_attention_grad_v2(Tensor query, Tensor key, Tensor value, Tensor dy, int head_num, str input_layout, *, Tensor? pse=None, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? softmax_max=None, Tensor? softmax_sum=None, Tensor? softmax_in=None, Tensor? attention_in=None, Tensor? query_rope=None, Tensor? key_rope=None, float scale_value=1., float keep_prob=1., int pre_tokens=2147483647, int next_tokens=2147483647, int inner_precise=0, int seed=0, int offset=0, int numels=0, int[]? prefix=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None, int sparse_mode=0, bool gen_mask_parallel=True, bool sync=False, int pse_type=1, int[]? q_start_idx=None, int[]? kv_start_idx=None) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_moe_compute_expert_tokens(Tensor sorted_expert_for_source_row, int num_expert) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_dropout_do_mask(Tensor self, Tensor mask, float p) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_ciou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_lstm_cell(Tensor input, Tensor w_ih, Tensor w_hh, Tensor h, Tensor c, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_fused_attention_score(Tensor query_layer, Tensor key_layer, Tensor value_layer, Tensor attention_mask, Scalar scale, float keep_prob, bool query_transpose=False, bool key_transpose=False, bool bmm_score_transpose_a=False, bool bmm_score_transpose_b=False, bool value_transpose=False, bool dx_transpose=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_rotary_mul(Tensor self, Tensor r1, Tensor r2) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _npu_ciou(Tensor self, Tensor gtboxes, bool trans=False, bool is_cross=True, int mode=0, bool atan_sub_flag=False) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_prompt_flash_attention(Tensor query, Tensor key, Tensor value, *, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? pse_shift=None, int[]? actual_seq_lengths=None, Tensor? deq_scale1=None, Tensor? quant_scale1=None, Tensor? deq_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, int num_heads=1, float scale_value=1.0, int pre_tokens=2147473647, int next_tokens=0, str input_layout=\"BSH\", int num_key_value_heads=0, int[]? actual_seq_lengths_kv=None, int sparse_mode=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_fused_infer_attention_score(Tensor query, Tensor key, Tensor value, *, Tensor? pse_shift=None, Tensor? atten_mask=None, SymInt[]? actual_seq_lengths=None, SymInt[]? actual_seq_lengths_kv=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? key_antiquant_scale=None, Tensor? key_antiquant_offset=None, Tensor? value_antiquant_scale=None, Tensor? value_antiquant_offset=None, Tensor? block_table=None, Tensor? query_padding_size=None, Tensor? kv_padding_size=None, Tensor? key_shared_prefix=None, Tensor? value_shared_prefix=None, SymInt[]? actual_shared_prefix_len=None, int num_heads=1, float scale=1.0, int pre_tokens=2147483647, int next_tokens=2147483647, str input_layout=\"BSH\", int num_key_value_heads=0, int sparse_mode=0, int inner_precise=0, int block_size=0, int antiquant_mode=0, int key_antiquant_mode=0, int value_antiquant_mode=0, bool softmax_lse_flag=False) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_fused_infer_attention_score.out(Tensor query, Tensor key, Tensor value, *, Tensor? pse_shift=None, Tensor? atten_mask=None, SymInt[]? actual_seq_lengths=None, SymInt[]? actual_seq_lengths_kv=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? key_antiquant_scale=None, Tensor? key_antiquant_offset=None, Tensor? value_antiquant_scale=None, Tensor? value_antiquant_offset=None, Tensor? block_table=None, Tensor? query_padding_size=None, Tensor? kv_padding_size=None, Tensor? key_shared_prefix=None, Tensor? value_shared_prefix=None, SymInt[]? actual_shared_prefix_len=None, Tensor? query_rope=None, Tensor? key_rope=None, Tensor? key_rope_antiquant_scale=None, int num_heads=1, float scale=1.0, int pre_tokens=2147483647, int next_tokens=2147483647, str input_layout=\"BSH\", int num_key_value_heads=0, int sparse_mode=0, int inner_precise=0, int block_size=0, int antiquant_mode=0, int key_antiquant_mode=0, int value_antiquant_mode=0, bool softmax_lse_flag=False, Tensor? workspace=None, Tensor(a!) attention_out, Tensor(b!) softmax_lse) -> (Tensor(a!), Tensor(b!))": {
    "version": ["all_version"]
  },
  "func: _npu_fused_infer_attention_score_get_max_workspace(Tensor query, Tensor key, Tensor value, *, Tensor? pse_shift=None, Tensor? atten_mask=None, SymInt[]? actual_seq_lengths=None, SymInt[]? actual_seq_lengths_kv=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? key_antiquant_scale=None, Tensor? key_antiquant_offset=None, Tensor? value_antiquant_scale=None, Tensor? value_antiquant_offset=None, Tensor? block_table=None, Tensor? query_padding_size=None, Tensor? kv_padding_size=None, Tensor? key_shared_prefix=None, Tensor? value_shared_prefix=None, SymInt[]? actual_shared_prefix_len=None, Tensor? query_rope=None, Tensor? key_rope=None, Tensor? key_rope_antiquant_scale=None, int num_heads=1, float scale=1.0, int pre_tokens=2147483647, int next_tokens=2147483647, str input_layout=\"BSH\", int num_key_value_heads=0, int sparse_mode=0, int inner_precise=0, int block_size=0, int antiquant_mode=0, int key_antiquant_mode=0, int value_antiquant_mode=0, bool softmax_lse_flag=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_incre_flash_attention(Tensor query, Tensor key, Tensor value, *, Tensor? padding_mask=None, Tensor? atten_mask=None, Tensor? pse_shift=None, SymInt[]? actual_seq_lengths=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? block_table=None, Tensor? dequant_scale1=None, Tensor? quant_scale1=None, Tensor? dequant_scale2=None, Tensor? quant_scale2=None, Tensor? quant_offset2=None, Tensor? kv_padding_size=None, int num_heads=1, float scale_value=1.0, str input_layout=\"BSH\", int num_key_value_heads=0, int block_size=0, int inner_precise=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_mla_prolog(Tensor token_x, Tensor weight_dq, Tensor weight_uq_qr, Tensor weight_uk, Tensor weight_dkv_kr, Tensor rmsnorm_gamma_cq, Tensor rmsnorm_gamma_ckv, Tensor rope_sin, Tensor rope_cos, Tensor cache_index, Tensor kv_cache, Tensor kr_cache, *, Tensor? dequant_scale_x=None, Tensor? dequant_scale_w_dq=None, Tensor? dequant_scale_w_uq_qr=None, Tensor? dequant_scale_w_dkv_kr=None, Tensor? quant_scale_ckv=None, Tensor? quant_scale_ckr=None, Tensor? smooth_scales_cq=None, float rmsnorm_epsilon_cq=1e-05, float rmsnorm_epsilon_ckv=1e-05, str cache_mode=\"PA_BSND\") -> (Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_mla_prolog_v2(Tensor token_x, Tensor weight_dq, Tensor weight_uq_qr, Tensor weight_uk, Tensor weight_dkv_kr, Tensor rmsnorm_gamma_cq, Tensor rmsnorm_gamma_ckv, Tensor rope_sin, Tensor rope_cos, Tensor cache_index, Tensor kv_cache, Tensor kr_cache, *, Tensor? dequant_scale_x=None, Tensor? dequant_scale_w_dq=None, Tensor? dequant_scale_w_uq_qr=None, Tensor? dequant_scale_w_dkv_kr=None, Tensor? quant_scale_ckv=None, Tensor? quant_scale_ckr=None, Tensor? smooth_scales_cq=None, float rmsnorm_epsilon_cq=1e-05, float rmsnorm_epsilon_ckv=1e-05, str cache_mode=\"PA_BSND\") -> (Tensor, Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_convolution_transpose_backward(Tensor input, Tensor grad, Tensor weight, int[] padding, int[] output_padding, int[] stride, int[] dilation, int groups, bool[3] grad_input_mask) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_scaled_masked_softmax_backward(Tensor y_grad, Tensor y, Tensor mask, Scalar scale, bool fixed_triu_mask) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_dtype_cast_backward(Tensor grad, ScalarType dtype) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_binary_cross_entropy_with_logits_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight_opt, Tensor? pos_weight_opt, int reduction) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_confusion_transpose_backward(Tensor grad, int[] perm, SymInt[] shape, bool transpose_first) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_max_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_min_backward(Tensor grad, int dim, Tensor indices, SymInt[] sizes, bool keepdim) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_ps_roi_pooling_backward(Tensor output_grad, Tensor rois, float spatial_scale, int group_size, int output_dim, SymInt[] input_size) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_bmm_v2_mat1_backward(Tensor grad, Tensor mat1, Tensor mat2, SymInt[] size) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_bmm_v2_mat2_backward(Tensor grad, Tensor mat1, Tensor mat2, SymInt[] size) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_lstm_data_backward(Tensor? grady_opt, Tensor? gradh_opt, Tensor? gradc_opt, Tensor input, Tensor batch_sizes, Tensor weight, Tensor bias, Tensor init_h, Tensor init_c, Tensor y, Tensor h, Tensor c, Tensor i, Tensor j, Tensor f, Tensor o, Tensor tanhc, bool flag_direction) -> (Tensor, Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor": {
    "version": ["all_version"]
  },
  "func: kl_div_backward(Tensor grad_output, Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_mm_reduce_scatter_base(Tensor self, Tensor x2, str hcom, int world_size, *, str reduce_op='sum', Tensor? bias=None, int comm_turn=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_all_gather_base_mm(Tensor self, Tensor x2, str hcom, int world_size, *, Tensor? bias=None, int gather_index=0, bool gather_output=True, int comm_turn=0) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_mm_all_reduce_base(Tensor x1, Tensor x2, str hcom, *, str reduce_op='sum', Tensor? bias=None, Tensor? antiquant_scale=None, Tensor? antiquant_offset=None, Tensor? x3=None, Tensor? dequant_scale=None, Tensor? pertoken_scale=None, Tensor? comm_quant_scale_1=None, Tensor? comm_quant_scale_2=None, int antiquant_group_size=0, int comm_turn=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_scatter_list(Tensor[] self, Tensor indices, Tensor updates, Tensor? mask=None, str reduce='update', int axis=-2) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: npu_scatter_list_(Tensor(a!)[] self, Tensor indices, Tensor updates, Tensor? mask=None, str reduce='update', int axis=-2) -> ()": {
    "version": ["all_version"]
  },
  "func: npu_scatter_nd_update(Tensor self, Tensor indices, Tensor updates) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_scatter_nd_update_(Tensor(a!) self, Tensor indices, Tensor updates) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: scatter_update(Tensor self, Tensor indices, Tensor updates, int axis) -> Tensor": {
    "version": ["all_version"]
  },
  "func: scatter_update_(Tensor(a!) self, Tensor indices, Tensor updates, int axis) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_quant_scatter(Tensor self, Tensor indices, Tensor updates, Tensor quant_scales, Tensor? quant_zero_points=None, int axis=0, int quant_axis=1, str reduce='update') -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_quant_scatter_(Tensor(a!) self, Tensor indices, Tensor updates, Tensor quant_scales, Tensor? quant_zero_points=None, int axis=0, int quant_axis=1, str reduce='update') -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_multi_head_attention_v2(Tensor query, Tensor key, Tensor value, Tensor? atten_mask=None, Tensor? alibi_mask=None, float scale=1.0, int head_num=1, str input_layout=\"BNSD\", float keep_prob=1., int pre_tokens=2147483647, int next_tokens=1, bool gen_mask_parallel=True, bool sync=False) -> (Tensor, Tensor, int, int, int)": {
    "version": ["all_version"]
  },
  "func: npu_multi_head_attention_v2_grad(Tensor attention_score_grad, Tensor query, Tensor key, Tensor value, Tensor softmax_log_max_sum, Tensor attention_score, Tensor? atten_mask=None, Tensor? alibi_mask=None, float scale=1.0, int head_num=1, str input_layout=\"BNSD\", float keep_prob=1., int pre_tokens=2147483647, int next_tokens=1, int seed=0, int offset=0, int numels=0, bool gen_mask_parallel=True, bool sync=False) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_masked_softmax_with_rel_pos_bias(Tensor x, Tensor? atten_mask, Tensor relative_pos_bias, float scale_value=1.0, int inner_precision_mode=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_moe_init_routing(Tensor x, Tensor row_idx, Tensor expert_idx, int active_num) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_moe_gating_top_k_softmax(Tensor x, Tensor? finished=None, int k=1) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_ffn(Tensor x, Tensor weight1, Tensor weight2, str activation, *, int[]? expert_tokens=None, int[]? expert_tokens_index=None, Tensor? bias1=None, Tensor? bias2=None, Tensor? scale=None, Tensor? offset=None, Tensor? deq_scale1=None, Tensor? deq_scale2=None, Tensor? antiquant_scale1=None, Tensor? antiquant_scale2=None, Tensor? antiquant_offset1=None, Tensor? antiquant_offset2=None, int? inner_precise=None, ScalarType? output_dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_grouped_matmul(Tensor[] x, Tensor[] weight, *, Tensor[]? bias=None, Tensor[]? scale=None, Tensor[]? offset=None, Tensor[]? antiquant_scale=None, Tensor[]? antiquant_offset=None, Tensor[]? per_token_scale=None, Tensor? group_list=None, Tensor[]? activation_input=None, Tensor[]? activation_quant_scale=None, Tensor[]? activation_quant_offset=None, int? split_item=0, int? group_type=None, int? group_list_type=0, int? act_type=0, int[]? tuning_config=None, ScalarType? output_dtype=None) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: npu_grouped_matmul.List(Tensor[] x, Tensor[] weight, *, Tensor[]? bias=None, Tensor[]? scale=None, Tensor[]? offset=None, Tensor[]? antiquant_scale=None, Tensor[]? antiquant_offset=None, Tensor[]? per_token_scale=None, int[]? group_list=None, Tensor[]? activation_input=None, Tensor[]? activation_quant_scale=None, Tensor[]? activation_quant_offset=None, int? split_item=0, int? group_type=None, int? group_list_type=0, int? act_type=0, ScalarType? output_dtype=None) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: npu_grouped_matmul_finalize_routing(Tensor x, Tensor w, Tensor group_list, *, Tensor? scale=None, Tensor? bias=None, Tensor? offset=None, Tensor? pertoken_scale=None, Tensor? shared_input=None, Tensor? logit=None, Tensor? row_index=None, ScalarType? dtype=None, float? shared_input_weight=1.0, int? shared_input_offset=0, int? output_bs=0, int? group_list_type=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_weight_quant_batchmatmul(Tensor x, Tensor weight, Tensor antiquant_scale, Tensor? antiquant_offset=None, Tensor? quant_scale=None, Tensor? quant_offset=None, Tensor? bias=None, int antiquant_group_size=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_convert_weight_to_int4pack(Tensor weight, int inner_k_tiles=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_transpose_batchmatmul(Tensor input, Tensor weight, *, Tensor? bias=None, Tensor? scale=None, int[]? perm_x1=None, int[]? perm_x2=None, int[]? perm_y=None, int? batch_split_factor=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_trans_quant_param(Tensor scale, Tensor? offset=None, int? round_mode=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_quant_matmul(Tensor x1, Tensor x2, Tensor scale, *, Tensor? offset=None, Tensor? pertoken_scale=None, Tensor? bias=None, ScalarType? output_dtype=None, SymInt[]? group_sizes=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _npu_dropout_gen_mask.Tensor(Tensor self, int[] size, float p, int seed, int offset, *, bool? parallel=True, bool? sync=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_quantize(Tensor self, Tensor scales, Tensor? zero_points, ScalarType dtype, int axis=1, bool div_mode=True) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_anti_quant(Tensor x, Tensor scale, *, Tensor? offset=None, ScalarType? dst_dtype=None, ScalarType? src_dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _npu_silent_check(Tensor(a!) input_grad, Tensor val, Tensor(b!) pre_val, Tensor(c!) min_val, Tensor(d!) max_val, Tensor val_counter, int c_min_steps, float c_thresh_l1, float c_coeff_l1, float c_thresh_l2, float c_coeff_l2) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_quant_conv2d(Tensor input, Tensor weight, Tensor scale, int[2] strides=1, int[2] pads=0, int[2] dilations=1, int groups=1, int offset_x=0, str round_mode='rint', ScalarType? output_dtype=None, Tensor? bias=None, Tensor? offset=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_moe_finalize_routing(Tensor expanded_permuted_rows, Tensor? skip1, Tensor? skip2, Tensor? bias, Tensor? scales, Tensor expanded_src_to_dst_row, Tensor? export_for_source_row, int? drop_pad_mode=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: matmul_double_backward(Tensor? grad_self, Tensor? grad_other, Tensor grad_out, Tensor self, Tensor other, bool[3] mask) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: __ilshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: __ilshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: __ior__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: __ior__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: __irshift__.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: __irshift__.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: __lshift__.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: __lshift__.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: __rshift__.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: __rshift__.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: __xor__.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: __xor__.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _adaptive_avg_pool2d_backward(Tensor grad_output, Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _adaptive_avg_pool3d(Tensor self, SymInt[3] output_size) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _adaptive_avg_pool3d_backward(Tensor grad_output, Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _add_relu.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _add_relu.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _add_relu_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _aminmax(Tensor self) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: _aminmax.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: signbit.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: aminmax(Tensor self, *, int? dim=None, bool keepdim=False) -> (Tensor min, Tensor max)": {
    "version": ["all_version"]
  },
  "func: aminmax.out(Tensor self, *, int? dim=None, bool keepdim=False, Tensor(a!) min, Tensor(b!) max) -> (Tensor(a!) min, Tensor(b!) max)": {
    "version": ["all_version"]
  },
  "func: _amp_foreach_non_finite_check_and_unscale_(Tensor(a!)[] self, Tensor(b!) found_inf, Tensor inv_scale) -> ()": {
    "version": ["all_version"]
  },
  "func: _batch_norm_impl_index(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> (Tensor, Tensor, Tensor, Tensor, int)": {
    "version": ["all_version"]
  },
  "func: _batch_norm_impl_index_backward(int impl_index, Tensor input, Tensor grad_output, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_var_transform, bool train, float eps, bool[3] output_mask, Tensor reservedSpace) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: _cdist_backward(Tensor grad, Tensor x1, Tensor x2, float p, Tensor cdist) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _cdist_forward(Tensor x1, Tensor x2, float p, int? compute_mode) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _conv_depthwise2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, SymInt[2] padding, int[2] dilation) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _conv_depthwise2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, SymInt[2] padding, int[2] dilation, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool benchmark, bool deterministic, bool cudnn_enabled, bool allow_tf32) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _ctc_loss(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, bool zero_infinity=False) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: _ctc_loss_backward(Tensor grad, Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, Tensor neg_log_likelihood, Tensor log_alpha, int blank, bool zero_infinity=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _cummax_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()": {
    "version": ["all_version"]
  },
  "func: _cummin_helper(Tensor self, Tensor(a!) values, Tensor(b!) indices, int dim) -> ()": {
    "version": ["all_version"]
  },
  "func: _dim_arange(Tensor like, int dim) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _embedding_bag(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: _embedding_bag_forward_only(Tensor weight, Tensor indices, Tensor offsets, bool scale_grad_by_freq=False, int mode=0, bool sparse=False, Tensor? per_sample_weights=None, bool include_last_offset=False, int padding_idx=-1) -> (Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: _embedding_bag_per_sample_weights_backward(Tensor grad, Tensor weight, Tensor indices, Tensor offsets, Tensor offset2bag, int mode, int padding_idx=-1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _index_put_impl_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False, bool unsafe=False) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _unsafe_index.Tensor(Tensor self, Tensor?[] indices) -> Tensor": {
    "version": ["v2.1"]
  },
  "func: _linalg_svd(Tensor A, bool full_matrices=False, bool compute_uv=True, *, str? driver=None) -> (Tensor U, Tensor S, Tensor Vh)": {
    "version": ["all_version"]
  },
  "func: _linalg_svd.U(Tensor A, bool full_matrices=False, bool compute_uv=True, *, str? driver=None, Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh) -> (Tensor(a!) U, Tensor(b!) S, Tensor(c!) Vh)": {
    "version": ["all_version"]
  },
  "func: _log_softmax(Tensor self, int dim, bool half_to_float) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _log_softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _log_softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _log_softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _native_batch_norm_legit(Tensor input, Tensor? weight, Tensor? bias, Tensor(a!) running_mean, Tensor(b!) running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: _nnpack_spatial_convolution(Tensor input, Tensor weight, Tensor? bias, SymInt[2] padding, int[2] stride=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _pack_padded_sequence(Tensor input, Tensor lengths, bool batch_first) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: _pad_packed_sequence(Tensor data, Tensor batch_sizes, bool batch_first, Scalar padding_value, int total_length) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: _pdist_forward(Tensor self, float p=2) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _slow_conv2d_backward.output_mask(Tensor grad_output, Tensor self, Tensor weight, int[2] kernel_size, int[2] stride, int[2] padding, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)": {
    "version": ["all_version"]
  },
  "func: _slow_conv2d_forward(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _slow_conv2d_forward.output(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias, int[2] stride, int[2] padding, *, Tensor(a!) output) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _softmax(Tensor self, int dim, bool half_to_float) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _softmax.out(Tensor self, int dim, bool half_to_float, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _softmax_backward_data(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _softmax_backward_data.out(Tensor grad_output, Tensor output, int dim, ScalarType input_dtype, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _unique2(Tensor self, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: abs(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: abs.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: abs_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: acos(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: acos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: acos_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: acosh(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: acosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: acosh_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: adaptive_avg_pool1d(Tensor self, int[1] output_size) -> Tensor": {
    "version": ["all_version"]
  },
  "func: adaptive_avg_pool2d(Tensor self, SymInt[2] output_size) -> Tensor": {
    "version": ["all_version"]
  },
  "func: adaptive_avg_pool2d.out(Tensor self, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: adaptive_avg_pool3d.out(Tensor self, SymInt[3] output_size, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: adaptive_avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: adaptive_max_pool2d(Tensor self, int[2] output_size) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: adaptive_max_pool2d.out(Tensor self, int[2] output_size, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))": {
    "version": ["all_version"]
  },
  "func: adaptive_max_pool2d_backward(Tensor grad_output, Tensor self, Tensor indices) -> Tensor": {
    "version": ["all_version"]
  },
  "func: adaptive_max_pool2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: add.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: add.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: add.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: add_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: add_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: addbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: addbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: addbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: addcdiv(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: addcdiv.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: addcdiv_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: addcmul(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: addcmul.out(Tensor self, Tensor tensor1, Tensor tensor2, *, Scalar value=1, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: addcmul_(Tensor(a!) self, Tensor tensor1, Tensor tensor2, *, Scalar value=1) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: addmm(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: addmm.out(Tensor self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: addmm_(Tensor(a!) self, Tensor mat1, Tensor mat2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: addmv(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: addmv.out(Tensor self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: addmv_(Tensor(a!) self, Tensor mat, Tensor vec, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: addr(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: addr.out(Tensor self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: addr_(Tensor(a!) self, Tensor vec1, Tensor vec2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: affine_grid_generator(Tensor theta, SymInt[] size, bool align_corners) -> Tensor": {
    "version": ["all_version"]
  },
  "func: affine_grid_generator_backward(Tensor grad, SymInt[] size, bool align_corners) -> Tensor": {
    "version": ["all_version"]
  },
  "func: all(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: all.dim(Tensor self, int dim, bool keepdim=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: all.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: all.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: amax(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: amax.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: amin(Tensor self, int[1] dim=[], bool keepdim=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: amin.out(Tensor self, int[1] dim=[], bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: angle(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: angle.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: any(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: any.dim(Tensor self, int dim, bool keepdim=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: any.out(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: any.all_out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: arange(Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: arange.out(Scalar end, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: arange.start(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: arange.start_out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: arange.start_step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: argmax.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: argmin(Tensor self, int? dim=None, bool keepdim=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: argmin.out(Tensor self, int? dim=None, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: argsort(Tensor self, int dim=-1, bool descending=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: argsort.dimname(Tensor self, Dimname dim, bool descending=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: asin(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: asin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: asin_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: asinh(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: asinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: asinh_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: atan(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: atan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: atan2(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: atan2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: atan2_(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: atan_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: atanh(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: atanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: atanh_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: avg_pool2d(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: avg_pool2d.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: avg_pool2d_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor": {
    "version": ["all_version"]
  },
  "func: avg_pool2d_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: avg_pool3d(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: avg_pool3d.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, bool ceil_mode=False, bool count_include_pad=True, int? divisor_override=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: avg_pool3d_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override) -> Tensor": {
    "version": ["all_version"]
  },
  "func: avg_pool3d_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, bool ceil_mode, bool count_include_pad, int? divisor_override, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: baddbmm(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: baddbmm.out(Tensor self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: baddbmm_(Tensor(a!) self, Tensor batch1, Tensor batch2, *, Scalar beta=1, Scalar alpha=1) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, bool cudnn_enabled) -> Tensor": {
    "version": ["all_version"]
  },
  "func: batch_norm_backward_elemt(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, Tensor sum_dy, Tensor sum_dy_xmu, Tensor count) -> Tensor": {
    "version": ["all_version"]
  },
  "func: batch_norm_backward_reduce(Tensor grad_out, Tensor input, Tensor mean, Tensor invstd, Tensor? weight, bool input_g, bool weight_g, bool bias_g) -> (Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: batch_norm_elemt(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps) -> Tensor": {
    "version": ["all_version"]
  },
  "func: batch_norm_elemt.out(Tensor input, Tensor? weight, Tensor? bias, Tensor mean, Tensor invstd, float eps, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: batch_norm_gather_stats_with_counts(Tensor input, Tensor mean, Tensor invstd, Tensor? running_mean, Tensor? running_var, float momentum, float eps, Tensor counts) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: batch_norm_stats(Tensor input, float eps) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: bernoulli(Tensor self, *, Generator? generator=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: bernoulli.out(Tensor self, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bernoulli.p(Tensor self, float p, *, Generator? generator=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: bernoulli_.Tensor(Tensor(a!) self, Tensor p, *, Generator? generator=None) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bernoulli_.float(Tensor(a!) self, float p=0.5, *, Generator? generator=None) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: binary_cross_entropy(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor": {
    "version": ["all_version"]
  },
  "func: binary_cross_entropy.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: binary_cross_entropy_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean) -> Tensor": {
    "version": ["all_version"]
  },
  "func: binary_cross_entropy_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bincount(Tensor self, Tensor? weights=None, int minlength=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: bitwise_and.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: bitwise_and.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bitwise_and.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: bitwise_and.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bitwise_and_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bitwise_and_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bitwise_not(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: bitwise_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bitwise_not_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bitwise_or.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: bitwise_or.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bitwise_or.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: bitwise_or.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bitwise_xor.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: bitwise_xor.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bitwise_xor.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: bitwise_xor.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bitwise_xor_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bitwise_xor_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bmm(Tensor self, Tensor mat2) -> Tensor": {
    "version": ["all_version"]
  },
  "func: bmm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: bucketize.Scalar(Scalar self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: bucketize.Tensor(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: bucketize.Tensor_out(Tensor self, Tensor boundaries, *, bool out_int32=False, bool right=False, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: cat(Tensor[] tensors, int dim=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: cat.names(Tensor[] tensors, Dimname dim) -> Tensor": {
    "version": ["all_version"]
  },
  "func: cat.names_out(Tensor[] tensors, Dimname dim, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: cat.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: cdist(Tensor x1, Tensor x2, float p=2, int? compute_mode=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: ceil(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: ceil.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: ceil_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _coalesce(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: complex(Tensor real, Tensor imag) -> Tensor": {
    "version": ["all_version"]
  },
  "func: complex.out(Tensor real, Tensor imag, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: channel_shuffle(Tensor self, int groups) -> Tensor": {
    "version": ["all_version"]
  },
  "func: clamp(Tensor self, Scalar? min=None, Scalar? max=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: clamp.out(Tensor self, Scalar? min=None, Scalar? max=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: clamp_(Tensor(a!) self, Scalar? min=None, Scalar? max=None) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: clamp_max(Tensor self, Scalar max) -> Tensor": {
    "version": ["all_version"]
  },
  "func: clamp_max.out(Tensor self, Scalar max, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: clamp_max_(Tensor(a!) self, Scalar max) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: clamp_min(Tensor self, Scalar min) -> Tensor": {
    "version": ["all_version"]
  },
  "func: clamp_min.out(Tensor self, Scalar min, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: clamp_min_(Tensor(a!) self, Scalar min) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: clamp.Tensor(Tensor self, Tensor? min=None, Tensor? max=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: clamp.Tensor_out(Tensor self, Tensor? min=None, Tensor? max=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: clamp_.Tensor(Tensor(a!) self, Tensor? min=None, Tensor? max=None) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: clamp_max.Tensor(Tensor self, Tensor max) -> Tensor": {
    "version": ["all_version"]
  },
  "func: clamp_max.Tensor_out(Tensor self, Tensor max, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: clamp_max_.Tensor(Tensor(a!) self, Tensor max) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: clamp_min.Tensor(Tensor self, Tensor min) -> Tensor": {
    "version": ["all_version"]
  },
  "func: clamp_min.Tensor_out(Tensor self, Tensor min, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: clamp_min_.Tensor(Tensor(a!) self, Tensor min) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: col2im(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor": {
    "version": ["all_version"]
  },
  "func: col2im.out(Tensor self, SymInt[2] output_size, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: constant_pad_nd(Tensor self, SymInt[] pad, Scalar value=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: conv_tbc(Tensor self, Tensor weight, Tensor bias, int pad=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: conv_tbc_backward(Tensor self, Tensor input, Tensor weight, Tensor bias, int pad) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: conv_transpose2d.input(Tensor input, Tensor weight, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int groups=1, int[2] dilation=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: conv_transpose3d.input(Tensor input, Tensor weight, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, SymInt[3] output_padding=0, int groups=1, int[3] dilation=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: convolution(Tensor input, Tensor weight, Tensor? bias, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups) -> Tensor": {
    "version": ["all_version"]
  },
  "func: convolution_backward(Tensor grad_output, Tensor input, Tensor weight, SymInt[]? bias_sizes, int[] stride, SymInt[] padding, int[] dilation, bool transposed, SymInt[] output_padding, int groups, bool[3] output_mask) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: convolution_backward_overrideable(Tensor grad_output, Tensor input, Tensor weight, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups, bool[3] output_mask) -> (Tensor grad_input, Tensor grad_weight, Tensor grad_bias)": {
    "version": ["all_version"]
  },
  "func: convolution_overrideable(Tensor input, Tensor weight, Tensor? bias, int[] stride, int[] padding, int[] dilation, bool transposed, int[] output_padding, int groups) -> Tensor": {
    "version": ["all_version"]
  },
  "func: count_nonzero(Tensor self, int? dim=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: count_nonzero.dim_IntList(Tensor self, int[] dim) -> Tensor": {
    "version": ["all_version"]
  },
  "func: cos(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: cos.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: cos_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: cosh(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: cosh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: cosh_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: ctc_loss.IntList(Tensor log_probs, Tensor targets, int[] input_lengths, int[] target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: ctc_loss.Tensor(Tensor log_probs, Tensor targets, Tensor input_lengths, Tensor target_lengths, int blank=0, int reduction=Mean, bool zero_infinity=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: cumprod.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: cumprod.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: cumprod_.dimname(Tensor(a!) self, Dimname dim, *, ScalarType? dtype=None) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: cumsum(Tensor self, int dim, *, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: cumsum.dimname_out(Tensor self, Dimname dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: cumsum.out(Tensor self, int dim, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: div.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: div.Scalar_mode(Tensor self, Scalar other, *, str? rounding_mode) -> Tensor": {
    "version": ["all_version"]
  },
  "func: div.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: div.Tensor_mode(Tensor self, Tensor other, *, str? rounding_mode) -> Tensor": {
    "version": ["all_version"]
  },
  "func: div.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: div.out_mode(Tensor self, Tensor other, *, str? rounding_mode, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: div_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: div_.Scalar_mode(Tensor(a!) self, Scalar other, *, str? rounding_mode) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: div_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: div_.Tensor_mode(Tensor(a!) self, Tensor other, *, str? rounding_mode) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: dot(Tensor self, Tensor tensor) -> Tensor": {
    "version": ["all_version"]
  },
  "func: dot.out(Tensor self, Tensor tensor, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: vdot(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: vdot.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: dropout(Tensor input, float p, bool train) -> Tensor": {
    "version": ["all_version"]
  },
  "func: embedding(Tensor weight, Tensor indices, SymInt padding_idx=-1, bool scale_grad_by_freq=False, bool sparse=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: embedding_dense_backward(Tensor grad_output, Tensor indices, SymInt num_weights, SymInt padding_idx, bool scale_grad_by_freq) -> Tensor": {
    "version": ["all_version"]
  },
  "func: embedding_renorm_(Tensor(a!) self, Tensor indices, float max_norm, float norm_type) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: eq.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: eq.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: eq.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: eq.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: eq_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: eq_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: equal(Tensor self, Tensor other) -> bool": {
    "version": ["all_version"]
  },
  "func: erf(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: erf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: erf_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: erfc(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: erfc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: erfc_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: erfinv(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: erfinv.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: erfinv_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: exp(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: exp.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: exp2(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: exp2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: exp2_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: exp_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: expm1(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: expm1.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: expm1_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: exponential_(Tensor(a!) self, float lambd=1, *, Generator? generator=None) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: eye(SymInt n, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: eye.m(SymInt n, SymInt m, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: eye.m_out(SymInt n, SymInt m, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: eye.out(SymInt n, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: fake_quantize_per_channel_affine_cachemask(Tensor self, Tensor scale, Tensor zero_point, int axis, int quant_min, int quant_max) -> (Tensor output, Tensor mask)": {
    "version": ["v2.1"]
  },
  "func: _fake_quantize_per_tensor_affine_cachemask_tensor_qparams(Tensor self, Tensor scale, Tensor zero_point, Tensor fake_quant_enabled, int quant_min, int quant_max) -> (Tensor output, Tensor mask)": {
    "version": ["v2.1"]
  },
  "func: _fused_moving_avg_obs_fq_helper(Tensor self, Tensor observer_on, Tensor fake_quant_on, Tensor(a!) running_min, Tensor(b!) running_max, Tensor(c!) scale, Tensor(d!) zero_point, float averaging_const, int quant_min, int quant_max, int ch_axis, bool per_row_fake_quant=False, bool symmetric_quant=False) -> (Tensor output, Tensor mask)": {
    "version": ["v2.1"]
  },
  "func: fill_.Scalar(Tensor(a!) self, Scalar value) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: fill_.Tensor(Tensor(a!) self, Tensor value) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: fill_diagonal_(Tensor(a!) self, Scalar fill_value, bool wrap=False) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: flip(Tensor self, int[] dims) -> Tensor": {
    "version": ["all_version"]
  },
  "func: floor(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: floor.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: floor_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: floor_divide(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: floor_divide.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: floor_divide.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: floor_divide_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: floor_divide_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: fmod.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: fmod.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: fmod.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: fmod.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: fmod_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: fmod_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: frac(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: frac.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: frac_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: gather(Tensor self, int dim, Tensor index, *, bool sparse_grad=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: gather.dimname(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: gather.dimname_out(Tensor self, Dimname dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: gather.out(Tensor self, int dim, Tensor index, *, bool sparse_grad=False, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: gcd.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: ge.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: ge.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: ge.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: ge.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: ge_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: ge_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: gelu(Tensor self, *, str approximate='none') -> Tensor": {
    "version": ["all_version"]
  },
  "func: gelu_backward(Tensor grad_output, Tensor self, *, str approximate='none') -> Tensor": {
    "version": ["all_version"]
  },
  "func: glu(Tensor self, int dim=-1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: glu.out(Tensor self, int dim=-1, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: glu_backward(Tensor grad_output, Tensor self, int dim) -> Tensor": {
    "version": ["all_version"]
  },
  "func: glu_backward.grad_input(Tensor grad_output, Tensor self, int dim, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: grid_sampler_2d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor": {
    "version": ["all_version"]
  },
  "func: grid_sampler_2d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: grid_sampler_3d(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> Tensor": {
    "version": ["all_version"]
  },
  "func: grid_sampler_3d_backward(Tensor grad_output, Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners, bool[2] output_mask) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: gru.input(Tensor input, Tensor hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: gt.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: gt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: gt.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: gt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: gt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: gt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: hardshrink(Tensor self, Scalar lambd=0.5) -> Tensor": {
    "version": ["all_version"]
  },
  "func: hardshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: hardshrink_backward(Tensor grad_out, Tensor self, Scalar lambd) -> Tensor": {
    "version": ["all_version"]
  },
  "func: hardshrink_backward.grad_input(Tensor grad_out, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: hardsigmoid(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: hardsigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: hardsigmoid_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: hardsigmoid_backward(Tensor grad_output, Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: hardswish(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: hardswish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: hardswish_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: hardswish_backward(Tensor grad_output, Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: hardtanh(Tensor self, Scalar min_val=-1, Scalar max_val=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: hardtanh.out(Tensor self, Scalar min_val=-1, Scalar max_val=1, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: hardtanh_(Tensor(a!) self, Scalar min_val=-1, Scalar max_val=1) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: hardtanh_backward(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val) -> Tensor": {
    "version": ["all_version"]
  },
  "func: hardtanh_backward.grad_input(Tensor grad_output, Tensor self, Scalar min_val, Scalar max_val, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: im2col(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride) -> Tensor": {
    "version": ["all_version"]
  },
  "func: im2col.out(Tensor self, int[2] kernel_size, int[2] dilation, int[2] padding, int[2] stride, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: index.Tensor(Tensor self, Tensor?[] indices) -> Tensor": {
    "version": ["all_version"]
  },
  "func: index_add(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: index_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor source, *, Scalar alpha=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: index_add.out(Tensor self, int dim, Tensor index, Tensor source, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: index_fill.int_Scalar(Tensor self, int dim, Tensor index, Scalar value) -> Tensor": {
    "version": ["all_version"]
  },
  "func: index_fill.int_Tensor(Tensor self, int dim, Tensor index, Tensor value) -> Tensor": {
    "version": ["all_version"]
  },
  "func: index_fill_.int_Scalar(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: index_fill_.int_Tensor(Tensor(a!) self, int dim, Tensor index, Tensor value) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: index_put(Tensor self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: index_put_(Tensor(a!) self, Tensor?[] indices, Tensor values, bool accumulate=False) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: index_select(Tensor self, int dim, Tensor index) -> Tensor": {
    "version": ["all_version"]
  },
  "func: index_select.dimname(Tensor self, Dimname dim, Tensor index) -> Tensor": {
    "version": ["all_version"]
  },
  "func: index_select.dimname_out(Tensor self, Dimname dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: index_select.out(Tensor self, int dim, Tensor index, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: inverse(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: inverse.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: isin.Tensor_Scalar(Tensor element, Scalar test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: isin.Tensor_Scalar_out(Tensor element, Scalar test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: isclose(Tensor self, Tensor other, float rtol=1e-05, float atol=1e-08, bool equal_nan=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: isfinite(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: isin.Scalar_Tensor_out(Scalar element, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: isposinf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: isneginf.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: kthvalue(Tensor self, int k, int dim=-1, bool keepdim=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: kthvalue.dimname(Tensor self, int k, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: kthvalue.dimname_out(Tensor self, int k, Dimname dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)": {
    "version": ["all_version"]
  },
  "func: kthvalue.values(Tensor self, int k, int dim=-1, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)": {
    "version": ["all_version"]
  },
  "func: le.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: le.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: le.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: le.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: le_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: le_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: leaky_relu(Tensor self, Scalar negative_slope=0.01) -> Tensor": {
    "version": ["all_version"]
  },
  "func: leaky_relu.out(Tensor self, Scalar negative_slope=0.01, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: leaky_relu_(Tensor(a!) self, Scalar negative_slope=0.01) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: leaky_relu_backward(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result) -> Tensor": {
    "version": ["all_version"]
  },
  "func: leaky_relu_backward.grad_input(Tensor grad_output, Tensor self, Scalar negative_slope, bool self_is_result, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: lerp.Scalar(Tensor self, Tensor end, Scalar weight) -> Tensor": {
    "version": ["all_version"]
  },
  "func: lerp.Scalar_out(Tensor self, Tensor end, Scalar weight, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: lerp.Tensor(Tensor self, Tensor end, Tensor weight) -> Tensor": {
    "version": ["all_version"]
  },
  "func: lerp.Tensor_out(Tensor self, Tensor end, Tensor weight, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: lerp_.Scalar(Tensor(a!) self, Tensor end, Scalar weight) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: lerp_.Tensor(Tensor(a!) self, Tensor end, Tensor weight) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: linalg_cross(Tensor self, Tensor other, *, int dim=-1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: linalg_cross.out(Tensor self, Tensor other, *, int dim=-1, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: linalg_qr(Tensor self, str mode='reduced') -> (Tensor Q, Tensor R)": {
    "version": ["all_version"]
  },
  "func: linalg_qr.out(Tensor self, str mode='reduced', *, Tensor(a!) Q, Tensor(b!) R) -> (Tensor(a!) Q, Tensor(b!) R)": {
    "version": ["all_version"]
  },
  "func: linalg_solve_triangular(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: linalg_solve_triangular.out(Tensor self, Tensor B, *, bool upper, bool left=True, bool unitriangular=False, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: linspace(Scalar start, Scalar end, int steps, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: linspace.out(Scalar start, Scalar end, int steps, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: log(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: log.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: log10(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: log10.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: log10_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: log1p(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: log1p.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: log1p_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: log2(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: log2.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: log2_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: log_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: log_sigmoid(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: log_sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: log_sigmoid_backward(Tensor grad_output, Tensor self, Tensor buffer) -> Tensor": {
    "version": ["all_version"]
  },
  "func: log_sigmoid_backward.grad_input(Tensor grad_output, Tensor self, Tensor buffer, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: log_sigmoid_forward(Tensor self) -> (Tensor output, Tensor buffer)": {
    "version": ["all_version"]
  },
  "func: log_sigmoid_forward.output(Tensor self, *, Tensor(a!) output, Tensor(b!) buffer) -> (Tensor(a!), Tensor(b!))": {
    "version": ["all_version"]
  },
  "func: log_softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: log_softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: logaddexp(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: logaddexp.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: logaddexp2(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: logaddexp2.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: logical_and(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: logical_and.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: logical_and_(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: logical_not(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: logical_not.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: logical_not_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: logical_or(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: logical_or.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: logical_or_(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: logical_xor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: logical_xor.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: logspace(Scalar start, Scalar end, int steps, float base=10.0, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: logspace.out(Scalar start, Scalar end, int steps, float base=10.0, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: logsumexp(Tensor self, int[1] dim, bool keepdim=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: logsumexp.names(Tensor self, Dimname[1] dim, bool keepdim=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: logsumexp.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: logsumexp.out(Tensor self, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: lstm.data(Tensor data, Tensor batch_sizes, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: lstm.input(Tensor input, Tensor[] hx, Tensor[] params, bool has_biases, int num_layers, float dropout, bool train, bool bidirectional, bool batch_first) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: lstm_cell(Tensor input, Tensor[] hx, Tensor w_ih, Tensor w_hh, Tensor? b_ih=None, Tensor? b_hh=None) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: lt.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: lt.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: lt.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: lt.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: lt_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: lt_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: masked_fill_.Scalar(Tensor(a!) self, Tensor mask, Scalar value) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: masked_fill_.Tensor(Tensor(a!) self, Tensor mask, Tensor value) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: masked_scatter_(Tensor(a!) self, Tensor mask, Tensor source) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: masked_select(Tensor self, Tensor mask) -> Tensor": {
    "version": ["all_version"]
  },
  "func: masked_select.out(Tensor self, Tensor mask, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: matmul(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: matmul_backward(Tensor grad, Tensor self, Tensor other, bool[2] mask) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: max(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: max.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: max.dim_max(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)": {
    "version": ["all_version"]
  },
  "func: max.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: max.names_dim_max(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) max, Tensor(b!) max_values) -> (Tensor(a!) values, Tensor(b!) indices)": {
    "version": ["all_version"]
  },
  "func: max.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: max_pool2d_with_indices(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: max_pool2d_with_indices.out(Tensor self, int[2] kernel_size, int[2] stride=[], int[2] padding=0, int[2] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))": {
    "version": ["all_version"]
  },
  "func: max_pool2d_with_indices_backward(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices) -> Tensor": {
    "version": ["all_version"]
  },
  "func: max_pool2d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[2] kernel_size, int[2] stride, int[2] padding, int[2] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: max_pool3d_with_indices(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: max_pool3d_with_indices.out(Tensor self, int[3] kernel_size, int[3] stride=[], int[3] padding=0, int[3] dilation=1, bool ceil_mode=False, *, Tensor(a!) out, Tensor(b!) indices) -> (Tensor(a!), Tensor(b!))": {
    "version": ["all_version"]
  },
  "func: max_pool3d_with_indices_backward(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices) -> Tensor": {
    "version": ["all_version"]
  },
  "func: max_pool3d_with_indices_backward.grad_input(Tensor grad_output, Tensor self, int[3] kernel_size, int[3] stride, int[3] padding, int[3] dilation, bool ceil_mode, Tensor indices, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: max_unpool2d(Tensor self, Tensor indices, SymInt[2] output_size) -> Tensor": {
    "version": ["all_version"]
  },
  "func: max_unpool2d.out(Tensor self, Tensor indices, SymInt[2] output_size, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: max_unpool3d(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: max_unpool3d.out(Tensor self, Tensor indices, SymInt[3] output_size, int[3] stride, int[3] padding, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: maximum(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: maximum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: mean(Tensor self, *, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: mean.dim(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: mean.names_dim(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: mean.names_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: mean.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: median(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: median.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: median.dim_values(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)": {
    "version": ["all_version"]
  },
  "func: min(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: min.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: min.dim_min(Tensor self, int dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)": {
    "version": ["all_version"]
  },
  "func: min.names_dim(Tensor self, Dimname dim, bool keepdim=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: min.names_dim_min(Tensor self, Dimname dim, bool keepdim=False, *, Tensor(a!) min, Tensor(b!) min_indices) -> (Tensor(a!) values, Tensor(b!) indices)": {
    "version": ["all_version"]
  },
  "func: min.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: minimum(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: minimum.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: mish(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: mish.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: mish_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: mish_backward(Tensor grad_output, Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: mm(Tensor self, Tensor mat2) -> Tensor": {
    "version": ["all_version"]
  },
  "func: mm.out(Tensor self, Tensor mat2, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: mse_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor": {
    "version": ["all_version"]
  },
  "func: mse_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: mse_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor": {
    "version": ["all_version"]
  },
  "func: mse_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: mul.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: mul.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: mul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: mul_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: mul_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: multilabel_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor": {
    "version": ["all_version"]
  },
  "func: multilabel_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: multilabel_margin_loss_forward(Tensor self, Tensor target, int reduction) -> (Tensor output, Tensor is_target)": {
    "version": ["all_version"]
  },
  "func: multilabel_margin_loss_forward.output(Tensor self, Tensor target, int reduction, *, Tensor(a!) output, Tensor(b!) is_target) -> (Tensor(a!), Tensor(b!))": {
    "version": ["all_version"]
  },
  "func: multinomial(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: multinomial.out(Tensor self, int num_samples, bool replacement=False, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: mv(Tensor self, Tensor vec) -> Tensor": {
    "version": ["all_version"]
  },
  "func: mv.out(Tensor self, Tensor vec, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: nanmedian(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: nanmedian.dim(Tensor self, int dim, bool keepdim=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: nan_to_num(Tensor self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: nan_to_num_(Tensor(a!) self, float? nan=None, float? posinf=None, float? neginf=None) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: nan_to_num.out(Tensor self, float? nan=None, float? posinf=None, float? neginf=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: linalg_vector_norm(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: linalg_vector_norm.out(Tensor self, Scalar ord=2, int[1]? dim=None, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: native_batch_norm_backward(Tensor grad_out, Tensor input, Tensor? weight, Tensor? running_mean, Tensor? running_var, Tensor? save_mean, Tensor? save_invstd, bool train, float eps, bool[3] output_mask) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: native_batch_norm.out(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps, *, Tensor(a!) out, Tensor(b!) save_mean, Tensor(c!) save_invstd) -> (Tensor(a!), Tensor(b!), Tensor(c!))": {
    "version": ["all_version"]
  },
  "func: native_dropout(Tensor input, float p, bool? train) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: native_dropout_backward(Tensor grad_output, Tensor mask, float scale) -> Tensor": {
    "version": ["all_version"]
  },
  "func: native_layer_norm(Tensor input, SymInt[] normalized_shape, Tensor? weight, Tensor? bias, float eps) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: native_layer_norm_backward(Tensor grad_out, Tensor input, SymInt[] normalized_shape, Tensor mean, Tensor rstd, Tensor? weight, Tensor? bias, bool[3] output_mask) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: ne.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: ne.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: ne.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: ne.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: ne_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: ne_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: neg(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: neg.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: neg_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: nll_loss(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor": {
    "version": ["all_version"]
  },
  "func: nll_loss.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: nll_loss2d(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100) -> Tensor": {
    "version": ["all_version"]
  },
  "func: nll_loss2d.out(Tensor self, Tensor target, Tensor? weight=None, int reduction=Mean, SymInt ignore_index=-100, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: nll_loss2d_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor": {
    "version": ["all_version"]
  },
  "func: nll_loss2d_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: nll_loss2d_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)": {
    "version": ["all_version"]
  },
  "func: nll_loss2d_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))": {
    "version": ["all_version"]
  },
  "func: nll_loss_backward(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight) -> Tensor": {
    "version": ["all_version"]
  },
  "func: nll_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, Tensor total_weight, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: nll_loss_forward(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index) -> (Tensor output, Tensor total_weight)": {
    "version": ["all_version"]
  },
  "func: nll_loss_forward.output(Tensor self, Tensor target, Tensor? weight, int reduction, SymInt ignore_index, *, Tensor(a!) output, Tensor(b!) total_weight) -> (Tensor(a!), Tensor(b!))": {
    "version": ["all_version"]
  },
  "func: nonzero(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: nonzero.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: norm.Scalar(Tensor self, Scalar p=2) -> Tensor": {
    "version": ["all_version"]
  },
  "func: norm.ScalarOpt_dim(Tensor self, Scalar? p, int[1] dim, bool keepdim=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: norm.ScalarOpt_dim_dtype(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype) -> Tensor": {
    "version": ["all_version"]
  },
  "func: norm.ScalarOpt_dtype(Tensor self, Scalar? p, *, ScalarType dtype) -> Tensor": {
    "version": ["all_version"]
  },
  "func: norm.dtype_out(Tensor self, Scalar? p, int[1] dim, bool keepdim, *, ScalarType dtype, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: norm.out(Tensor self, Scalar? p, int[1] dim, bool keepdim=False, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: normal.Tensor_Tensor(Tensor mean, Tensor std, *, Generator? generator=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: normal.Tensor_Tensor_out(Tensor mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: normal.Tensor_float(Tensor mean, float std=1, *, Generator? generator=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: normal.Tensor_float_out(Tensor mean, float std=1, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: normal.float_Tensor(float mean, Tensor std, *, Generator? generator=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: normal.float_Tensor_out(float mean, Tensor std, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: normal.float_float(float mean, float std, SymInt[] size, *, Generator? generator=None, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: normal.float_float_out(float mean, float std, SymInt[] size, *, Generator? generator=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: normal_(Tensor(a!) self, float mean=0, float std=1, *, Generator? generator=None) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: one_hot(Tensor self, int num_classes=-1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: ones(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: ones.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: ones.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: ones_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: pdist(Tensor self, float p=2) -> Tensor": {
    "version": ["all_version"]
  },
  "func: pow.Scalar(Scalar self, Tensor exponent) -> Tensor": {
    "version": ["all_version"]
  },
  "func: pow.Scalar_out(Scalar self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: pow.Tensor_Scalar(Tensor self, Scalar exponent) -> Tensor": {
    "version": ["all_version"]
  },
  "func: pow.Tensor_Scalar_out(Tensor self, Scalar exponent, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: pow.Tensor_Tensor(Tensor self, Tensor exponent) -> Tensor": {
    "version": ["all_version"]
  },
  "func: pow.Tensor_Tensor_out(Tensor self, Tensor exponent, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: pow_.Scalar(Tensor(a!) self, Scalar exponent) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: pow_.Tensor(Tensor(a!) self, Tensor exponent) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _prelu_kernel(Tensor self, Tensor weight) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _prelu_kernel_backward(Tensor grad_output, Tensor self, Tensor weight) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: prod(Tensor self, *, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: prod.dim_int(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: prod.int_out(Tensor self, int dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: put_(Tensor(a!) self, Tensor index, Tensor source, bool accumulate=False) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: quantize_per_channel(Tensor self, Tensor scales, Tensor zero_points, int axis, ScalarType dtype) -> Tensor": {
    "version": ["all_version"]
  },
  "func: quantize_per_tensor(Tensor self, float scale, int zero_point, ScalarType dtype) -> Tensor": {
    "version": ["all_version"]
  },
  "func: random_(Tensor(a!) self, *, Generator? generator=None) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: random_.from(Tensor(a!) self, int from, int? to, *, Generator? generator=None) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: random_.to(Tensor(a!) self, int to, *, Generator? generator=None) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: randperm(SymInt n, *, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: randperm.generator(SymInt n, *, Generator? generator, ScalarType? dtype=long, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: randperm.generator_out(SymInt n, *, Generator? generator, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: randperm.out(SymInt n, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: range(Scalar start, Scalar end, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: range.out(Scalar start, Scalar end, Scalar step=1, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: range.step(Scalar start, Scalar end, Scalar step=1, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: reciprocal(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: reciprocal.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: reciprocal_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: reflection_pad1d(Tensor self, SymInt[2] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: reflection_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: reflection_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: reflection_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: reflection_pad2d(Tensor self, SymInt[4] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: reflection_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: reflection_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: reflection_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: reflection_pad3d(Tensor self, SymInt[6] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: reflection_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: reflection_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: reflection_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: relu(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: relu_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: remainder.Scalar(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: remainder.Scalar_out(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: remainder.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: remainder.Tensor_out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: remainder_.Scalar(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: remainder_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: remainder.Scalar_Tensor(Scalar self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: renorm(Tensor self, Scalar p, int dim, Scalar maxnorm) -> Tensor": {
    "version": ["all_version"]
  },
  "func: renorm.out(Tensor self, Scalar p, int dim, Scalar maxnorm, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: renorm_(Tensor(a!) self, Scalar p, int dim, Scalar maxnorm) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: repeat(Tensor self, SymInt[] ": {
    "version": ["all_version"]
  },
  "func: repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, int? output_size=None) -> Tensor": {
    "version": ["v2.1"]
  },
  "func: repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, int? output_size=None) -> Tensor": {
    "version": ["v2.1"]
  },
  "func: replication_pad1d(Tensor self, SymInt[2] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: replication_pad1d.out(Tensor self, SymInt[2] padding, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: replication_pad1d_backward(Tensor grad_output, Tensor self, SymInt[2] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: replication_pad1d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[2] padding, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: replication_pad2d(Tensor self, SymInt[4] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: replication_pad2d.out(Tensor self, SymInt[4] padding, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: replication_pad2d_backward(Tensor grad_output, Tensor self, SymInt[4] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: replication_pad2d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[4] padding, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: replication_pad3d(Tensor self, SymInt[6] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: replication_pad3d.out(Tensor self, SymInt[6] padding, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: replication_pad3d_backward(Tensor grad_output, Tensor self, SymInt[6] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: replication_pad3d_backward.grad_input(Tensor grad_output, Tensor self, SymInt[6] padding, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: roll(Tensor self, SymInt[1] shifts, int[1] dims=[]) -> Tensor": {
    "version": ["all_version"]
  },
  "func: round(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: round.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: round_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: round.decimals(Tensor self, *, int decimals) -> Tensor": {
    "version": ["all_version"]
  },
  "func: round_.decimals(Tensor(a!) self, *, int decimals) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: round.decimals_out(Tensor self, *, int decimals, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: rrelu_with_noise(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: rrelu_with_noise.out(Tensor self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: rrelu_with_noise_(Tensor(a!) self, Tensor noise, Scalar lower=0.125, Scalar upper=0.3333333333333333, bool training=False, Generator? generator=None) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: rsqrt(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: rsqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: rsqrt_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: rsub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: rsub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: scaled_dot_product_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None) -> Tensor": {
    "version": ["v2.1", "v2.3", "v2.4"]
  },
  "func: scatter_.src(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: scatter_.value(Tensor(a!) self, int dim, Tensor index, Scalar value) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: scatter.src_out(Tensor self, int dim, Tensor index, Tensor src, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: scatter.value_out(Tensor self, int dim, Tensor index, Scalar value, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: scatter_add(Tensor self, int dim, Tensor index, Tensor src) -> Tensor": {
    "version": ["all_version"]
  },
  "func: scatter_add.dimname(Tensor self, Dimname dim, Tensor index, Tensor src) -> Tensor": {
    "version": ["all_version"]
  },
  "func: scatter_add_(Tensor(a!) self, int dim, Tensor index, Tensor src) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: searchsorted.Scalar(Tensor sorted_sequence, Scalar self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: searchsorted.Tensor(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: searchsorted.Tensor_out(Tensor sorted_sequence, Tensor self, *, bool out_int32=False, bool right=False, str? side=None, Tensor? sorter=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sgn(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: sgn.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sigmoid(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: sigmoid.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sigmoid_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sigmoid_backward(Tensor grad_output, Tensor output) -> Tensor": {
    "version": ["all_version"]
  },
  "func: sigmoid_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sign(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: sign.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sign_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sin(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: sin.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sin_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sinc(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: sinc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sinc_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sinh(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: sinh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sinh_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: slogdet(Tensor self) -> (Tensor sign, Tensor logabsdet)": {
    "version": ["all_version"]
  },
  "func: slow_conv3d(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: slow_conv3d.out(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias=None, int[3] stride=1, SymInt[3] padding=0, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: slow_conv3d_forward(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding) -> Tensor": {
    "version": ["all_version"]
  },
  "func: slow_conv3d_forward.output(Tensor self, Tensor weight, int[3] kernel_size, Tensor? bias, int[3] stride, SymInt[3] padding, *, Tensor(a!) output) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: slow_conv_dilated2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, int[2] dilation=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: slow_conv_transpose2d(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: slow_conv_transpose2d.out(Tensor self, Tensor weight, int[2] kernel_size, Tensor? bias=None, int[2] stride=1, SymInt[2] padding=0, SymInt[2] output_padding=0, int[2] dilation=1, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: smooth_l1_loss(Tensor self, Tensor target, int reduction=Mean, float beta=1.0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: smooth_l1_loss.out(Tensor self, Tensor target, int reduction=Mean, float beta=1.0, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: smooth_l1_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta) -> Tensor": {
    "version": ["all_version"]
  },
  "func: smooth_l1_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, float beta, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: soft_margin_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor": {
    "version": ["all_version"]
  },
  "func: soft_margin_loss.out(Tensor self, Tensor target, int reduction=Mean, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: soft_margin_loss_backward(Tensor grad_output, Tensor self, Tensor target, int reduction) -> Tensor": {
    "version": ["all_version"]
  },
  "func: soft_margin_loss_backward.grad_input(Tensor grad_output, Tensor self, Tensor target, int reduction, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: softmax.Dimname(Tensor self, Dimname dim, *, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: softmax.int(Tensor self, int dim, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: softplus(Tensor self, Scalar beta=1, Scalar threshold=20) -> Tensor": {
    "version": ["all_version"]
  },
  "func: softplus.out(Tensor self, Scalar beta=1, Scalar threshold=20, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: softplus_backward.grad_input(Tensor grad_output, Tensor self, Scalar beta, Scalar threshold, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: softshrink(Tensor self, Scalar lambd=0.5) -> Tensor": {
    "version": ["all_version"]
  },
  "func: softshrink.out(Tensor self, Scalar lambd=0.5, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: softshrink_backward(Tensor grad_output, Tensor self, Scalar lambd) -> Tensor": {
    "version": ["all_version"]
  },
  "func: softshrink_backward.grad_input(Tensor grad_output, Tensor self, Scalar lambd, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sort(Tensor self, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: sort.dimname(Tensor self, Dimname dim, bool descending=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: sort.dimname_values(Tensor self, Dimname dim, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)": {
    "version": ["all_version"]
  },
  "func: sort.values(Tensor self, int dim=-1, bool descending=False, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)": {
    "version": ["all_version"]
  },
  "func: sort.stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: sort.values_stable(Tensor self, *, bool? stable, int dim=-1, bool descending=False, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)": {
    "version": ["all_version"]
  },
  "func: sqrt(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: sqrt.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sqrt_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: stack(Tensor[] tensors, int dim=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: stack.out(Tensor[] tensors, int dim=0, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: std.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: std.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: std_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor": {
    "version": ["v2.1", "v2.6"]
  },
  "func: stft(Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None, bool? align_to_window=None) -> Tensor": {
    "version": ["v2.7", "newest"]
  },
  "func: sub.Scalar(Tensor self, Scalar other, Scalar alpha=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: sub.Tensor(Tensor self, Tensor other, *, Scalar alpha=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: sub.out(Tensor self, Tensor other, *, Scalar alpha=1, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sub_.Scalar(Tensor(a!) self, Scalar other, Scalar alpha=1) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sub_.Tensor(Tensor(a!) self, Tensor other, *, Scalar alpha=1) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sum(Tensor self, *, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: sum.DimnameList_out(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sum.IntList_out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: sum.dim_DimnameList(Tensor self, Dimname[1] dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: sum.dim_IntList(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: nansum(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: nansum.out(Tensor self, int[1]? dim, bool keepdim=False, *, ScalarType? dtype=None, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: take(Tensor self, Tensor index) -> Tensor": {
    "version": ["all_version"]
  },
  "func: take.out(Tensor self, Tensor index, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: tan(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: tan.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: tan_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: tanh(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: tanh.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: tanh_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: tanh_backward(Tensor grad_output, Tensor output) -> Tensor": {
    "version": ["all_version"]
  },
  "func: tanh_backward.grad_input(Tensor grad_output, Tensor output, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: threshold(Tensor self, Scalar ": {
    "version": ["all_version"]
  },
  "func: threshold.out(Tensor self, Scalar threshold, Scalar value, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: threshold_(Tensor(a!) self, Scalar threshold, Scalar value) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: threshold_backward(Tensor grad_output, Tensor self, Scalar threshold) -> Tensor": {
    "version": ["all_version"]
  },
  "func: topk(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True) -> (Tensor values, Tensor indices)": {
    "version": ["all_version"]
  },
  "func: topk.values(Tensor self, SymInt k, int dim=-1, bool largest=True, bool sorted=True, *, Tensor(a!) values, Tensor(b!) indices) -> (Tensor(a!) values, Tensor(b!) indices)": {
    "version": ["all_version"]
  },
  "func: trace(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: triangular_solve.X(Tensor self, Tensor A, bool upper=True, bool transpose=False, bool unitriangular=False, *, Tensor(a!) X, Tensor(b!) M) -> (Tensor(a!) solution, Tensor(b!) cloned_coefficient)": {
    "version": ["all_version"]
  },
  "func: tril(Tensor self, int diagonal=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: tril.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: tril_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: triu(Tensor self, int diagonal=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: triu.out(Tensor self, int diagonal=0, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: triu_(Tensor(a!) self, int diagonal=0) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: trunc(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: trunc.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: trunc_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: uniform_(Tensor(a!) self, float from=0, float to=1, *, Generator? generator=None) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: unique_consecutive(Tensor self, bool return_inverse=False, bool return_counts=False, int? dim=None) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: unique_dim(Tensor self, int dim, bool sorted=True, bool return_inverse=False, bool return_counts=False) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: upsample_bicubic2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: upsample_bicubic2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: upsample_bicubic2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: upsample_bicubic2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: upsample_bilinear2d(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: upsample_bilinear2d.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: upsample_bilinear2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: upsample_bilinear2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: upsample_linear1d(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: upsample_linear1d.out(Tensor self, SymInt[1] output_size, bool align_corners, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: upsample_linear1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, bool align_corners, float? scales=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: upsample_nearest1d(Tensor self, SymInt[1] output_size, float? scales=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: upsample_nearest1d.out(Tensor self, SymInt[1] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: upsample_nearest1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: upsample_nearest1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: upsample_nearest2d(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: upsample_nearest2d.out(Tensor self, SymInt[2] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: upsample_nearest2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: upsample_nearest2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: upsample_nearest3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: upsample_nearest3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: upsample_nearest3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: upsample_nearest3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: upsample_trilinear3d(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: upsample_trilinear3d.out(Tensor self, SymInt[3] output_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: upsample_trilinear3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: upsample_trilinear3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, bool align_corners, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: var.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: var.correction_out(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: var_mean.correction(Tensor self, int[1]? dim=None, *, Scalar? correction=None, bool keepdim=False) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: _weight_norm(Tensor v, Tensor g, int dim=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: where(Tensor condition) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: where.self(Tensor condition, Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: where.self_out(Tensor condition, Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: xlogy.OutScalar_Other(Tensor self, Scalar other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: xlogy.OutScalar_Self(Scalar self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: xlogy.OutTensor(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: xlogy.Scalar_Other(Tensor self, Scalar other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: xlogy.Scalar_Self(Scalar self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: xlogy.Tensor(Tensor self, Tensor other) -> Tensor": {
    "version": ["all_version"]
  },
  "func: xlogy_.Scalar_Other(Tensor(a!) self, Scalar other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: xlogy_.Tensor(Tensor(a!) self, Tensor other) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: zero_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: zeros(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: zeros.names(int[] size, *, Dimname[]? names, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: zeros.out(SymInt[] size, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: zeros_like(Tensor self, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, MemoryFormat? memory_format=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: celu(Tensor self, Scalar alpha=1.0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: celu_(Tensor(a!) self, Scalar alpha=1.0) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: elu.out(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: elu(Tensor self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor": {
    "version": ["all_version"]
  },
  "func: elu_(Tensor(a!) self, Scalar alpha=1, Scalar scale=1, Scalar input_scale=1) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: elu_backward.grad_input(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: elu_backward(Tensor grad_output, Scalar alpha, Scalar scale, Scalar input_scale, bool is_result, Tensor self_or_result) -> Tensor": {
    "version": ["all_version"]
  },
  "func: silu(Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: silu_(Tensor(a!) self) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: silu.out(Tensor self, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: silu_backward.grad_input(Tensor grad_output, Tensor self, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: silu_backward(Tensor grad_output, Tensor self) -> Tensor": {
    "version": ["all_version"]
  },
  "func: binary_cross_entropy_with_logits(Tensor self, Tensor target, Tensor? weight=None, Tensor? pos_weight=None, int reduction=Mean) -> Tensor": {
    "version": ["all_version"]
  },
  "func: l1_loss(Tensor self, Tensor target, int reduction=Mean) -> Tensor": {
    "version": ["all_version"]
  },
  "func: kl_div(Tensor self, Tensor target, int reduction=Mean, *, bool log_target=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: histc(Tensor self, int bins=100, Scalar min=0, Scalar max=0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: histc.out(Tensor self, int bins=100, Scalar min=0, Scalar max=0, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: index_copy_(Tensor(a!) self, int dim, Tensor index, Tensor source) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: index_copy(Tensor self, int dim, Tensor index, Tensor source) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _foreach_maximum.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_maximum_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_maximum.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_maximum_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_minimum.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_minimum_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_minimum.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_minimum_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_sqrt(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_sqrt_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_ceil_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_floor_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_trunc_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_round_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_ceil(Tensor(a!)[] self) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_floor(Tensor(a!)[] self) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_trunc(Tensor(a!)[] self) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_round(Tensor(a!)[] self) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_frac_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_frac(Tensor(a!)[] self) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_mul_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_sub_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_sub.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_div_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_div.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_add.List(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_add_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_sub.List(Tensor[] tensors1, Tensor[] tensors2, *, Scalar alpha=1) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_sub_.List(Tensor(a!)[] self, Tensor[] other, *, Scalar alpha=1) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_mul.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_mul_.List(Tensor(a!)[] self, Tensor[] other) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_div.List(Tensor[] tensors1, Tensor[] tensors2) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_div_.List(Tensor(a!)[] self, Tensor[] other) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_add.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_add_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_sub.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_sub_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_div.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_div_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_mul.ScalarList(Tensor[] tensors, Scalar[] scalars) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_mul_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_mul.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_sigmoid(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_sigmoid_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_add.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_add_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_addcdiv_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_addcdiv.Scalar(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_addcmul_.Scalar(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_addcmul.Scalar(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar value=1) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_cos_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_cos(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_pow.Scalar(Tensor[] tensors, Scalar scalar) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_pow_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_maximum.List(Tensor[] self, Tensor[] other) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_minimum.List(Tensor[] self, Tensor[] other) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_maximum_.List(Tensor(a!)[] self, Tensor[] other) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_minimum_.List(Tensor(a!)[] self, Tensor[] other) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_pow.List(Tensor[] self, Tensor[] exponent) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_pow_.List(Tensor(a!)[] self, Tensor[] exponent) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_pow.ScalarList(Tensor[] self, Scalar[] exponent) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_pow_.ScalarList(Tensor(a!)[] self, Scalar[] exponent) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_addcdiv_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_addcdiv.ScalarList(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_addcmul_.ScalarList(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_addcmul.ScalarList(Tensor[] input, Tensor[] tensor1, Tensor[] tensor2, Scalar[] scalars) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_addcdiv.Tensor(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_addcdiv_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_addcmul.Tensor(Tensor[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_addcmul_.Tensor(Tensor(a!)[] self, Tensor[] tensor1, Tensor[] tensor2, Tensor scalars) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_exp_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_exp(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_neg(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_neg_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: native_group_norm(Tensor input, Tensor? weight, Tensor? bias, int N, int C, int HxW, int group, float eps) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: native_group_norm_backward(Tensor grad_out, Tensor input, Tensor mean, Tensor rstd, Tensor? weight, int N, int C, int HxW, int group, bool[3] output_mask) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: fft_fftshift(Tensor self, int[1]? dim=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: fft_ifftshift(Tensor self, int[1]? dim=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: view_as_real(Tensor(a) self) -> Tensor(a)": {
    "version": ["all_version"]
  },
  "func: view_as_complex(Tensor(a) self) -> Tensor(a)": {
    "version": ["all_version"]
  },
  "func: _fused_adamw_(Tensor(a!)[] self, Tensor(b!)[] grads, Tensor(c!)[] exp_avgs, Tensor(d!)[] exp_avg_sqs, Tensor(e!)[] max_exp_avg_sqs, Tensor[] state_steps, *, float lr, float beta1, float beta2, float weight_decay, float eps, bool amsgrad, bool maximize, Tensor? grad_scale=None, Tensor? found_inf=None) -> ()": {
    "version": ["all_version"]
  },
  "func: _fft_c2c(Tensor self, SymInt[] dim, int normalization, bool forward) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _fft_c2c.out(Tensor self, SymInt[] dim, int normalization, bool forward, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _fft_r2c(Tensor self, int[] dim, int normalization, bool onesided) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _fft_r2c.out(Tensor self, int[] dim, int normalization, bool onesided, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _fft_c2r(Tensor self, int[] dim, int normalization, SymInt last_dim_size) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _fft_c2r.out(Tensor self, int[] dim, int normalization, SymInt last_dim_size, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _amp_update_scale_(Tensor(a!) self, Tensor(b!) growth_tracker, Tensor found_inf, float growth_factor, float backoff_factor, int growth_interval) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: polar(Tensor abs, Tensor angle) -> Tensor": {
    "version": ["all_version"]
  },
  "func: polar.out(Tensor abs, Tensor angle, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _npu_silent_check_v2(Tensor val, Tensor(a!) input_grad, Tensor(b!) sfda, Tensor(c!) step, int c_min_steps, float c_thresh_l1, float c_coeff_l1, float c_thresh_l2, float c_coeff_l2, int npu_asd_detect) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_dynamic_quant_asymmetric(Tensor input, *, Tensor? smooth_scales=None, Tensor? group_index=None, ScalarType? dst_type=None) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_prefetch(Tensor self, Tensor? dependency, int max_size, int offset=0) -> ()": {
    "version": ["all_version"]
  },
  "func: npu_group_quant(Tensor x, Tensor scale, Tensor group_index, *, Tensor? offset=None, ScalarType? dst_dtype=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: stft_backward(Tensor grad_output, Tensor self, int n_fft, int? hop_length=None, int? win_length=None, Tensor? window=None, bool normalized=False, bool? onesided=None, bool? return_complex=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: fft_r2c_backward(Tensor grad, int[] dim, int normalization, bool onesided, int last_dim_size) -> Tensor": {
    "version": ["all_version"]
  },
  "func: fft_c2r_backward(Tensor grad, int[] dim, int normalization) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _foreach_norm.Scalar(Tensor[] tensors, Scalar ord=2) -> Tensor[]": {
    "version": ["v2.1", "v2.3"]
  },
  "func: _foreach_abs_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_abs(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_copy_(Tensor(a!)[] self, Tensor[] src, bool non_blocking=False) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_sign(Tensor[] self) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_sign_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_pow.ScalarAndTensor(Scalar self, Tensor[] exponent) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_reciprocal_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_reciprocal(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_expm1_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_expm1(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_erf(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_erf_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_erfc(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_erfc_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_asin(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_asin_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_sin_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_sin(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_sinh_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_sinh(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_acos_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_acos(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_cosh_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_cosh(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_atan_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_atan(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_tan_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_tan(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_tanh(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_tanh_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_clamp_max.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_clamp_max_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_clamp_max.List(Tensor[] self, Tensor[] other) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_clamp_max_.List(Tensor(a!)[] self, Tensor[] other) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_clamp_max.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_clamp_max_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_clamp_min.ScalarList(Tensor[] self, Scalar[] scalars) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_clamp_min_.ScalarList(Tensor(a!)[] self, Scalar[] scalars) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_clamp_min.List(Tensor[] self, Tensor[] other) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_clamp_min_.List(Tensor(a!)[] self, Tensor[] other) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_clamp_min.Scalar(Tensor[] self, Scalar scalar) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_clamp_min_.Scalar(Tensor(a!)[] self, Scalar scalar) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_lerp_.Scalar(Tensor(a!)[] self, Tensor[] tensors1, Scalar weight) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_lerp.Scalar(Tensor[] self, Tensor[] tensors1, Scalar weight) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_lerp_.List(Tensor(a!)[] self, Tensor[] tensors1, Tensor[] weights) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_lerp.List(Tensor[] self, Tensor[] tensors1, Tensor[] weights) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_log1p_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_log1p(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_log_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_log(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_log10_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_log10(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_log2_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _foreach_log2(Tensor[] tensors) -> Tensor[]": {
    "version": ["all_version"]
  },
  "func: _foreach_zero_(Tensor(a!)[] self) -> ()": {
    "version": ["all_version"]
  },
  "func: _unique(Tensor self, bool sorted=True, bool return_inverse=False) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: argsort.stable(Tensor self, bool stable=False, int dim=-1, bool descending=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: dropout_(Tensor(a!) self, float p, bool train) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _upsample_bicubic2d_aa_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _upsample_bicubic2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _upsample_bicubic2d_aa(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _upsample_bicubic2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _upsample_bilinear2d_aa(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _upsample_bilinear2d_aa.out(Tensor self, SymInt[2] output_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _upsample_bilinear2d_aa_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _upsample_bilinear2d_aa_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, bool align_corners, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: isin.Tensor_Tensor(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False) -> Tensor": {
    "version": ["all_version"]
  },
  "func: isin.Tensor_Tensor_out(Tensor elements, Tensor test_elements, *, bool assume_unique=False, bool invert=False, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: int_repr(Tensor self) -> Tensor": {
    "version": ["v2.3", "v2.4", "v2.5", "v2.6", "v2.7", "v2.8"]
  },
  "func: dequantize.self(Tensor self) -> Tensor": {
    "version": ["v2.3", "v2.4", "v2.5", "v2.6", "v2.7", "v2.8"]
  },
  "func: _empty_affine_quantized(SymInt[] size, *, ScalarType? dtype=None, Layout? layout=None, Device? device=None, bool? pin_memory=None, float scale=1, int zero_point=0, MemoryFormat? memory_format=contiguous_format) -> Tensor": {
    "version": ["v2.3", "v2.4", "v2.5", "v2.6", "v2.7", "v2.8"]
  },
  "func: repeat_interleave.self_Tensor(Tensor self, Tensor repeats, int? dim=None, *, SymInt? output_size=None) -> Tensor": {
    "version": ["v2.3", "v2.4", "v2.5", "v2.6", "v2.7", "v2.8"]
  },
  "func: repeat_interleave.self_int(Tensor self, SymInt repeats, int? dim=None, *, SymInt? output_size=None) -> Tensor": {
    "version": ["v2.3", "v2.4", "v2.5", "v2.6", "v2.7", "v2.8"]
  },
  "func: _foreach_norm.Scalar(Tensor[] tensors, Scalar ord=2, ScalarType? dtype=None) -> Tensor[]": {
    "version": ["v2.4", "v2.5", "v2.6", "v2.7", "v2.8"]
  },
  "func: scaled_dot_product_attention(Tensor query, Tensor key, Tensor value, Tensor? attn_mask=None, float dropout_p=0.0, bool is_causal=False, *, float? scale=None, bool enable_gqa=False) -> Tensor": {
    "version": ["v2.5", "v2.6", "v2.7", "v2.8"]
  },
  "func: _npu_silent_check_v3(Tensor val, Tensor(a!) input_grad, Tensor(b!) step, Tensor(c!) max, Tensor(d!) avg, float c_thresh_l1, float c_thresh_l2, float betal, int npu_asd_detect) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _upsample_nearest_exact1d.out(Tensor self, SymInt[3] output_size, float? scales=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _upsample_nearest_exact1d_backward.grad_input(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _upsample_nearest_exact1d_backward(Tensor grad_output, SymInt[1] output_size, SymInt[3] input_size, float? scales=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _upsample_nearest_exact1d(Tensor self, SymInt[3] output_size, float? scales=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _upsample_nearest_exact2d.out(Tensor self, SymInt[3] output_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _upsample_nearest_exact2d_backward.grad_input(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _upsample_nearest_exact2d_backward(Tensor grad_output, SymInt[2] output_size, SymInt[4] input_size, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _upsample_nearest_exact2d(Tensor self, SymInt[3] output_size, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _upsample_nearest_exact3d.out(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) out) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _upsample_nearest_exact3d_backward.grad_input(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None, *, Tensor(a!) grad_input) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: _upsample_nearest_exact3d_backward(Tensor grad_output, SymInt[3] output_size, SymInt[5] input_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: _upsample_nearest_exact3d(Tensor self, SymInt[3] output_size, float? scales_d=None, float? scales_h=None, float? scales_w=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_gelu_backward(Tensor grad_output, Tensor self, *, str approximate='none') -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_gelu(Tensor self, *, str approximate='none') -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_group_norm_swish(Tensor input, int num_groups, Tensor weight, Tensor bias, float? eps=1e-5, float? swish_scale=1.0) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_group_norm_swish_grad(Tensor grad, Tensor input, int num_groups, Tensor weight, Tensor bias, Tensor mean, Tensor rstd, bool[3] grad_input_mask, float? swish_scale=1.0) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_cross_entropy_loss(Tensor input, Tensor target, Tensor? weight=None, str reduction='mean', int ignore_index=-100, float label_smoothing=0.0, float lse_square_scale_for_zloss=0.0, bool return_zloss=False) -> (Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_cross_entropy_loss_backward(Tensor grad_loss, Tensor log_prob, Tensor target, Tensor? weight=None, Tensor? grad_zloss=None, Tensor? lse_for_zloss=None, str reduction='mean', int ignore_index=-100, float label_smoothing=0.0, float lse_square_scale_for_zloss=0.0) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_advance_step_flashattn(Tensor(a!) input_tokens, Tensor sampled_token_ids, Tensor(b!) input_positions, Tensor(c!) seq_lens, Tensor(d!) slot_mapping, Tensor block_tables, int num_seqs, int num_queries, int block_size) -> ()": {
    "version": ["all_version"]
  },
  "func: npu_mrope(Tensor positions, Tensor query, Tensor key, Tensor cos_sin_cache, int head_size, *, int[]? mrope_section=None, str? rotary_mode='half') -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_gather_sparse_index(Tensor input, Tensor index) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_nsa_compress(Tensor input, Tensor weight, int compress_block_size, int compress_stride, *, int[]? actual_seq_len=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_nsa_compress_grad(Tensor grad, Tensor input, Tensor weight, int compress_block_size, int compress_stride, *, int[]? actual_seq_len=None) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_nsa_compress_infer.cache(Tensor input, Tensor weight, Tensor slot_mapping, int compress_block_size, int compress_stride, int page_block_size, *, Tensor? block_table=None, int[]? actual_seq_len=None, Tensor(a!) cache) -> Tensor(a!)": {
    "version": ["all_version"]
  },
  "func: npu_nsa_compress_attention(Tensor query, Tensor key, Tensor value, float scale_value, int head_num, int compress_block_size, int compress_stride, int select_block_size, int select_block_count, *, Tensor? topk_mask=None, Tensor? atten_mask=None, int[]? actual_seq_qlen=None, int[]? actual_cmp_seq_kvlen=None, int[]? actual_sel_seq_kvlen=None) -> (Tensor, Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_nsa_compress_attention_infer(Tensor query, Tensor key, Tensor value, float scale_value, int head_num, int key_value_head_num, int select_block_size, int select_block_count, int page_block_size, int compress_block_size, int compress_stride, *, Tensor? atten_mask=None, Tensor? block_table=None, Tensor? topk_mask=None, int[]? actual_seq_qlen=None, int[]? actual_cmp_seq_kvlen=None, int[]? actual_sel_seq_kvlen=None) -> (Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_nsa_select_attention(Tensor query, Tensor key, Tensor value, Tensor topk_indices, float scale_value, int head_num, int select_block_size, int select_block_count, *, Tensor? atten_mask=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_nsa_select_attention_grad(Tensor grad, Tensor query, Tensor key, Tensor value, Tensor attention_out, Tensor softmax_max, Tensor softmax_sum, Tensor topk_indices, float scale_value, int head_num, int select_block_size, int select_block_count, *, Tensor? atten_mask=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  },
  "func: npu_nsa_select_attention_infer(Tensor query, Tensor key, Tensor value, Tensor topk_indices, float scale_value, int head_num, int key_value_head_num, int select_block_size, int select_block_count, int page_block_size, *, str layout='BSND', Tensor? atten_mask=None, Tensor? block_table=None, int[]? actual_seq_qlen=None, int[]? actual_seq_kvlen=None) -> Tensor": {
    "version": ["all_version"]
  },
  "func: npu_grouped_matmul_swiglu_quant(Tensor x, Tensor weight, Tensor group_list, Tensor weight_scale, Tensor x_scale, *, Tensor? bias=None, Tensor? offset=None) -> (Tensor, Tensor, Tensor)": {
    "version": ["all_version"]
  }
}
